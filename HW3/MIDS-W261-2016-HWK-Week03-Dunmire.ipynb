{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.1 Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin: \n",
    "    line = line.strip()\n",
    "    line = line.rstrip()\n",
    "    line = line.split(',')\n",
    "    if line[1] == \"Debt collection\":\n",
    "        sys.stderr.write('reporter:counter:Debt-Counter,Total,1\\n')\n",
    "    elif line[1] == 'Mortgage':\n",
    "        sys.stderr.write('reporter:counter:Mortgage-Counter,Total,1\\n')\n",
    "    else:\n",
    "        sys.stderr.write('reporter:counter:Other-Counter,Total,1\\n')\n",
    "    sys.stderr.write(\"reporter:counter:Tokens,Total,1\\n\")\n",
    "    print line[1] + '\\t' + '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "debt_counter = 0\n",
    "mortgage_counter = 0\n",
    "other_counter = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip().split('\\t')\n",
    "    if line[0] == \"Debt collection\":\n",
    "        debt_counter +=1 \n",
    "    elif line[0] == 'Mortgage':\n",
    "        mortgage_counter += 1\n",
    "    else:\n",
    "        other_counter += 1\n",
    "print \"Debt collection: \" + str(debt_counter)\n",
    "print \"Mortgage: \" + str(mortgage_counter)\n",
    "print \"Other: \" + str(other_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat Consumer_Complaints.csv | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/29 21:04:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/29 21:05:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 21:05:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 21:05:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/29 21:05:34 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/29 21:05:34 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/29 21:05:34 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/29 21:05:35 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/29 21:05:35 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/29 21:05:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1206054309_0001\n",
      "16/01/29 21:05:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/29 21:05:35 INFO mapreduce.Job: Running job: job_local1206054309_0001\n",
      "16/01/29 21:05:35 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/29 21:05:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1206054309_0001_m_000000_0\n",
      "16/01/29 21:05:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/29 21:05:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/29 21:05:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/29 21:05:36 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/29 21:05:36 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: Records R/W=2032/1\n",
      "16/01/29 21:05:36 INFO streaming.PipeMapRed: R/W/S=10000/7896/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:36 INFO mapreduce.Job: Job job_local1206054309_0001 running in uber mode : false\n",
      "16/01/29 21:05:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/29 21:05:37 INFO streaming.PipeMapRed: R/W/S=100000/98646/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:37 INFO streaming.PipeMapRed: R/W/S=200000/198558/0 in:200000=200000/1 [rec/s] out:198558=198558/1 [rec/s]\n",
      "16/01/29 21:05:38 INFO streaming.PipeMapRed: R/W/S=300000/298306/0 in:150000=300000/2 [rec/s] out:149153=298306/2 [rec/s]\n",
      "16/01/29 21:05:38 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/29 21:05:38 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: \n",
      "16/01/29 21:05:38 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: Spilling map output\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: bufstart = 0; bufend = 4878322; bufvoid = 104857600\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 24962748(99850992); length = 1251649/6553600\n",
      "16/01/29 21:05:38 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/29 21:05:38 INFO mapred.Task: Task:attempt_local1206054309_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Records R/W=2032/1\n",
      "16/01/29 21:05:38 INFO mapred.Task: Task 'attempt_local1206054309_0001_m_000000_0' done.\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1206054309_0001_m_000000_0\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: Starting task: attempt_local1206054309_0001_r_000000_0\n",
      "16/01/29 21:05:38 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/29 21:05:38 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/29 21:05:38 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/29 21:05:38 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2102fea2\n",
      "16/01/29 21:05:38 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/29 21:05:38 INFO reduce.EventFetcher: attempt_local1206054309_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/29 21:05:38 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1206054309_0001_m_000000_0 decomp: 5504150 len: 5504154 to MEMORY\n",
      "16/01/29 21:05:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/29 21:05:38 INFO reduce.InMemoryMapOutput: Read 5504150 bytes from map-output for attempt_local1206054309_0001_m_000000_0\n",
      "16/01/29 21:05:38 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5504150, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5504150\n",
      "16/01/29 21:05:38 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/29 21:05:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/29 21:05:38 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/29 21:05:38 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/29 21:05:38 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5504124 bytes\n",
      "16/01/29 21:05:39 INFO reduce.MergeManagerImpl: Merged 1 segments, 5504150 bytes to disk to satisfy reduce memory limit\n",
      "16/01/29 21:05:39 INFO reduce.MergeManagerImpl: Merging 1 files, 5504154 bytes from disk\n",
      "16/01/29 21:05:39 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/29 21:05:39 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/29 21:05:39 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5504124 bytes\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/01/29 21:05:39 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/29 21:05:39 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: Records R/W=312913/1\n",
      "16/01/29 21:05:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/29 21:05:39 INFO mapred.Task: Task:attempt_local1206054309_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/29 21:05:39 INFO mapred.Task: Task attempt_local1206054309_0001_r_000000_0 is allowed to commit now\n",
      "16/01/29 21:05:39 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1206054309_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/consumer_counters/_temporary/0/task_local1206054309_0001_r_000000\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: Records R/W=312913/1 > reduce\n",
      "16/01/29 21:05:39 INFO mapred.Task: Task 'attempt_local1206054309_0001_r_000000_0' done.\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: Finishing task: attempt_local1206054309_0001_r_000000_0\n",
      "16/01/29 21:05:39 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/29 21:05:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/29 21:05:39 INFO mapreduce.Job: Job job_local1206054309_0001 completed successfully\n",
      "16/01/29 21:05:39 INFO mapreduce.Job: Counters: 39\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=11220438\n",
      "\t\tFILE: Number of bytes written=17314316\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=57\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=4878322\n",
      "\t\tMap output materialized bytes=5504154\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=5504154\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=3\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=671088640\n",
      "\tDebt-Counter\n",
      "\t\tTotal=44372\n",
      "\tMortgage-Counter\n",
      "\t\tTotal=125752\n",
      "\tOther-Counter\n",
      "\t\tTotal=142789\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tTokens\n",
      "\t\tTotal=312913\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=57\n",
      "16/01/29 21:05:39 INFO streaming.StreamJob: Output directory: consumer_counters\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output consumer_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 21:06:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Debt collection: 44372\t\n",
      "Mortgage: 125752\t\n",
      "Other: 142789\t\n"
     ]
    }
   ],
   "source": [
    "#show results\n",
    "!hdfs dfs -cat /user/dunmireg/consumer_counters/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting historyserver, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/mapred-dunmireg-historyserver-Glenns-Air.home.out\r\n"
     ]
    }
   ],
   "source": [
    "#Start history server to check counter\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop start historyserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping historyserver\r\n"
     ]
    }
   ],
   "source": [
    "#Stop history server\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/mr-jobhistory-daemon.sh --config /usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop stop historyserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this link to view job tracker\n",
    "\n",
    "http://192.168.0.10:19888/jobhistory/app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/29 18:22:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/29 18:22:50 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/29 18:22:50 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory\n",
    "#!hadoop fs -copyToLocal /user/dunmireg/consumer_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/29 21:09:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/29 21:09:45 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/Consumer_Complaints.csv\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/29 21:09:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/29 21:09:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/consumer_counters\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "16/01/29 21:09:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/29 21:10:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/Consumer_Complaints.csv\n",
    "!hadoop fs -rmr /user/dunmireg/consumer_counters\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part 1__\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('input_text.txt', 'w') as myfile:\n",
    "    myfile.write('foo foo quux labs foo bar quux')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.rstrip()\n",
    "    sys.stderr.write('reporter:counter:Map-Count,Total,1\\n')\n",
    "    line = line.split()\n",
    "    for word in line:\n",
    "        print word + '\\t' + '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = None\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.rstrip()\n",
    "    word, count = line.split('\\t')\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += int(count)\n",
    "    else:\n",
    "        if current_word:\n",
    "            sys.stderr.write('reporter:counter:Reduce-Counter,Total,1\\n')\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = int(count)\n",
    "\n",
    "if current_word == word:\n",
    "    sys.stderr.write('reporter:counter:Reduce-Counter,Total,1\\n')\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat input.txt | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/30 11:33:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/30 11:33:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 11:34:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put input_text.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 11:34:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 11:34:41 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 11:34:41 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 11:34:41 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 11:34:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 11:34:42 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 11:34:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local589956672_0001\n",
      "16/01/30 11:34:42 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 11:34:42 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 11:34:42 INFO mapreduce.Job: Running job: job_local589956672_0001\n",
      "16/01/30 11:34:42 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 11:34:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 11:34:42 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 11:34:42 INFO mapred.LocalJobRunner: Starting task: attempt_local589956672_0001_m_000000_0\n",
      "16/01/30 11:34:42 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 11:34:42 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 11:34:42 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 11:34:42 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/input_text.txt:0+30\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: \n",
      "16/01/30 11:34:43 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: bufstart = 0; bufend = 45; bufvoid = 104857600\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600\n",
      "16/01/30 11:34:43 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 11:34:43 INFO mapred.Task: Task:attempt_local589956672_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "16/01/30 11:34:43 INFO mapred.Task: Task 'attempt_local589956672_0001_m_000000_0' done.\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local589956672_0001_m_000000_0\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: Starting task: attempt_local589956672_0001_r_000000_0\n",
      "16/01/30 11:34:43 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 11:34:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 11:34:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 11:34:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@72e75028\n",
      "16/01/30 11:34:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 11:34:43 INFO reduce.EventFetcher: attempt_local589956672_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 11:34:43 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local589956672_0001_m_000000_0 decomp: 61 len: 65 to MEMORY\n",
      "16/01/30 11:34:43 INFO reduce.InMemoryMapOutput: Read 61 bytes from map-output for attempt_local589956672_0001_m_000000_0\n",
      "16/01/30 11:34:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 61, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->61\n",
      "16/01/30 11:34:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 11:34:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 11:34:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 11:34:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55 bytes\n",
      "16/01/30 11:34:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 61 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 11:34:43 INFO reduce.MergeManagerImpl: Merging 1 files, 65 bytes from disk\n",
      "16/01/30 11:34:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 11:34:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 11:34:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 55 bytes\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 11:34:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 11:34:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 11:34:43 INFO mapred.Task: Task:attempt_local589956672_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 11:34:43 INFO mapred.Task: Task attempt_local589956672_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 11:34:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local589956672_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local589956672_0001_r_000000\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: Records R/W=7/1 > reduce\n",
      "16/01/30 11:34:43 INFO mapred.Task: Task 'attempt_local589956672_0001_r_000000_0' done.\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local589956672_0001_r_000000_0\n",
      "16/01/30 11:34:43 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 11:34:43 INFO mapreduce.Job: Job job_local589956672_0001 running in uber mode : false\n",
      "16/01/30 11:34:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 11:34:43 INFO mapreduce.Job: Job job_local589956672_0001 completed successfully\n",
      "16/01/30 11:34:43 INFO mapreduce.Job: Counters: 37\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=212234\n",
      "\t\tFILE: Number of bytes written=798951\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=60\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=65\n",
      "\t\tInput split bytes=102\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=65\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=620756992\n",
      "\tMap-Count\n",
      "\t\tTotal=1\n",
      "\tReduce-Counter\n",
      "\t\tTotal=3\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=30\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/01/30 11:34:43 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input input_text.txt \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/30 11:35:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 11:35:33 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/input_text.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/30 11:35:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 11:35:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/word_count\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/30 11:35:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/30 11:36:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/input_text.txt\n",
    "!hadoop fs -rmr /user/dunmireg/word_count\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part 2__\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "#Structure of complaints\n",
    "#Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,\n",
    "#Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "for line in reader(sys.stdin):\n",
    "    issue = line[3]\n",
    "    if issue == '':\n",
    "        issue = 'Blank'\n",
    "    words = re.findall(WORD_RE, issue)\n",
    "    for word in words:\n",
    "        print word + '\\t' + str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = None\n",
    "word = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    #issue, count = line.split('\\t')\n",
    "    line = line.split('\\t')\n",
    "    word = line[0]\n",
    "    count = int(line[1])\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            #sys.stderr.write('reporter:counter:Reduce-Counter,Total,1\\n')\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "\n",
    "if current_word == word:\n",
    "    #sys.stderr.write('reporter:counter:Reduce-Counter,Total,1\\n')\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat Consumer_Complaints.csv | python mapper.py | sort | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/30 15:50:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/30 15:51:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 15:51:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 15:51:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 15:51:34 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 15:51:34 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 15:51:34 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 15:51:34 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 15:51:34 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 15:51:35 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1512164377_0001\n",
      "16/01/30 15:51:35 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 15:51:35 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 15:51:35 INFO mapreduce.Job: Running job: job_local1512164377_0001\n",
      "16/01/30 15:51:35 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 15:51:35 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 15:51:35 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 15:51:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1512164377_0001_m_000000_0\n",
      "16/01/30 15:51:35 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 15:51:35 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 15:51:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 15:51:35 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/01/30 15:51:35 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 15:51:35 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 15:51:35 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 15:51:35 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 15:51:35 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 15:51:35 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 15:51:35 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 15:51:35 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 15:51:35 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 15:51:36 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:36 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:36 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:36 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:36 INFO streaming.PipeMapRed: Records R/W=1222/1\n",
      "16/01/30 15:51:36 INFO streaming.PipeMapRed: R/W/S=10000/41101/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:36 INFO mapreduce.Job: Job job_local1512164377_0001 running in uber mode : false\n",
      "16/01/30 15:51:36 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 15:51:37 INFO streaming.PipeMapRed: R/W/S=100000/448120/0 in:100000=100000/1 [rec/s] out:448120=448120/1 [rec/s]\n",
      "16/01/30 15:51:38 INFO streaming.PipeMapRed: R/W/S=200000/884534/0 in:100000=200000/2 [rec/s] out:442267=884534/2 [rec/s]\n",
      "16/01/30 15:51:39 INFO streaming.PipeMapRed: R/W/S=300000/1296452/0 in:100000=300000/3 [rec/s] out:432150=1296452/3 [rec/s]\n",
      "16/01/30 15:51:39 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 15:51:39 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 15:51:39 INFO mapred.LocalJobRunner: \n",
      "16/01/30 15:51:39 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 15:51:39 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 15:51:39 INFO mapred.MapTask: bufstart = 0; bufend = 13424747; bufvoid = 104857600\n",
      "16/01/30 15:51:39 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 20821148(83284592); length = 5393249/6553600\n",
      "16/01/30 15:51:40 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 15:51:40 INFO mapred.Task: Task:attempt_local1512164377_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 15:51:40 INFO mapred.LocalJobRunner: Records R/W=1222/1\n",
      "16/01/30 15:51:40 INFO mapred.Task: Task 'attempt_local1512164377_0001_m_000000_0' done.\n",
      "16/01/30 15:51:40 INFO mapred.LocalJobRunner: Finishing task: attempt_local1512164377_0001_m_000000_0\n",
      "16/01/30 15:51:40 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 15:51:40 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 15:51:40 INFO mapred.LocalJobRunner: Starting task: attempt_local1512164377_0001_r_000000_0\n",
      "16/01/30 15:51:40 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 15:51:40 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 15:51:40 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 15:51:40 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2f812a23\n",
      "16/01/30 15:51:40 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 15:51:40 INFO reduce.EventFetcher: attempt_local1512164377_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 15:51:40 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1512164377_0001_m_000000_0 decomp: 16121375 len: 16121379 to MEMORY\n",
      "16/01/30 15:51:40 INFO reduce.InMemoryMapOutput: Read 16121375 bytes from map-output for attempt_local1512164377_0001_m_000000_0\n",
      "16/01/30 15:51:40 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 16121375, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->16121375\n",
      "16/01/30 15:51:40 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 15:51:40 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 15:51:40 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 15:51:40 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 15:51:40 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16121369 bytes\n",
      "16/01/30 15:51:41 INFO reduce.MergeManagerImpl: Merged 1 segments, 16121375 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 15:51:41 INFO reduce.MergeManagerImpl: Merging 1 files, 16121379 bytes from disk\n",
      "16/01/30 15:51:41 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 15:51:41 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 15:51:41 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16121369 bytes\n",
      "16/01/30 15:51:41 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 15:51:41 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/01/30 15:51:41 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 15:51:41 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 15:51:41 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:41 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:41 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:41 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:41 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:41 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/30 15:51:41 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:41 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:41 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:42 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:42 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:42 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 15:51:42 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:700000=700000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 15:51:42 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 15:51:42 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 15:51:42 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 15:51:42 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:1100000=1100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 15:51:43 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:1200000=1200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 15:51:43 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:1300000=1300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 15:51:43 INFO streaming.PipeMapRed: Records R/W=1348313/1\n",
      "16/01/30 15:51:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 15:51:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 15:51:43 INFO mapred.Task: Task:attempt_local1512164377_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 15:51:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 15:51:43 INFO mapred.Task: Task attempt_local1512164377_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 15:51:43 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1512164377_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local1512164377_0001_r_000000\n",
      "16/01/30 15:51:43 INFO mapred.LocalJobRunner: Records R/W=1348313/1 > reduce\n",
      "16/01/30 15:51:43 INFO mapred.Task: Task 'attempt_local1512164377_0001_r_000000_0' done.\n",
      "16/01/30 15:51:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local1512164377_0001_r_000000_0\n",
      "16/01/30 15:51:43 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 15:51:43 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 15:51:44 INFO mapreduce.Job: Job job_local1512164377_0001 completed successfully\n",
      "16/01/30 15:51:44 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=32454888\n",
      "\t\tFILE: Number of bytes written=49165963\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=2350\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348313\n",
      "\t\tMap output bytes=13424747\n",
      "\t\tMap output materialized bytes=16121379\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=189\n",
      "\t\tReduce shuffle bytes=16121379\n",
      "\t\tReduce input records=1348313\n",
      "\t\tReduce output records=189\n",
      "\t\tSpilled Records=2696626\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=619708416\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2350\n",
      "16/01/30 15:51:44 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/word_count/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/30 15:52:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 15:52:10 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/Consumer_Complaints.csv\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/30 15:52:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 15:52:12 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/word_count\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "16/01/30 15:52:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/30 15:52:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/Consumer_Complaints.csv\n",
    "!hadoop fs -rmr /user/dunmireg/word_count\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Part 3__\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/30 16:39:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/30 16:39:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 16:39:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put Consumer_Complaints.csv /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/30 16:40:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 16:40:11 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/30 16:40:11 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/30 16:40:11 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/30 16:40:11 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/30 16:40:11 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/30 16:40:12 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local353958300_0001\n",
      "16/01/30 16:40:12 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/30 16:40:12 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/30 16:40:12 INFO mapreduce.Job: Running job: job_local353958300_0001\n",
      "16/01/30 16:40:12 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/30 16:40:12 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 16:40:12 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/30 16:40:12 INFO mapred.LocalJobRunner: Starting task: attempt_local353958300_0001_m_000000_0\n",
      "16/01/30 16:40:12 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 16:40:12 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 16:40:12 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 16:40:12 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/Consumer_Complaints.csv:0+50906486\n",
      "16/01/30 16:40:12 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/30 16:40:12 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/30 16:40:12 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/30 16:40:12 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/30 16:40:12 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/30 16:40:12 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/30 16:40:12 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/30 16:40:12 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./mapper.py]\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/30 16:40:12 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/30 16:40:12 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:12 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:12 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:12 INFO streaming.PipeMapRed: Records R/W=789/1\n",
      "16/01/30 16:40:12 INFO streaming.PipeMapRed: R/W/S=1000/979/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:12 INFO streaming.PipeMapRed: R/W/S=10000/41101/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:13 INFO mapreduce.Job: Job job_local353958300_0001 running in uber mode : false\n",
      "16/01/30 16:40:13 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/30 16:40:13 INFO streaming.PipeMapRed: R/W/S=100000/448120/0 in:100000=100000/1 [rec/s] out:448120=448120/1 [rec/s]\n",
      "16/01/30 16:40:14 INFO streaming.PipeMapRed: R/W/S=200000/884534/0 in:100000=200000/2 [rec/s] out:442267=884534/2 [rec/s]\n",
      "16/01/30 16:40:15 INFO streaming.PipeMapRed: R/W/S=300000/1296452/0 in:100000=300000/3 [rec/s] out:432150=1296452/3 [rec/s]\n",
      "16/01/30 16:40:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 16:40:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 16:40:15 INFO mapred.LocalJobRunner: \n",
      "16/01/30 16:40:15 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/30 16:40:15 INFO mapred.MapTask: Spilling map output\n",
      "16/01/30 16:40:15 INFO mapred.MapTask: bufstart = 0; bufend = 13424747; bufvoid = 104857600\n",
      "16/01/30 16:40:15 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 20821148(83284592); length = 5393249/6553600\n",
      "16/01/30 16:40:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/01/30 16:40:16 INFO Configuration.deprecation: mapred.skip.map.auto.incr.proc.count is deprecated. Instead, use mapreduce.map.skip.proc-count.auto-incr\n",
      "16/01/30 16:40:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:16 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:16 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:16 INFO streaming.PipeMapRed: R/W/S=100000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:16 INFO streaming.PipeMapRed: R/W/S=200000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:17 INFO streaming.PipeMapRed: R/W/S=300000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:17 INFO streaming.PipeMapRed: R/W/S=400000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:17 INFO streaming.PipeMapRed: R/W/S=500000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:17 INFO streaming.PipeMapRed: R/W/S=600000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:17 INFO streaming.PipeMapRed: R/W/S=700000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:17 INFO streaming.PipeMapRed: R/W/S=800000/0/0 in:800000=800000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 16:40:17 INFO streaming.PipeMapRed: R/W/S=900000/0/0 in:900000=900000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 16:40:17 INFO streaming.PipeMapRed: R/W/S=1000000/0/0 in:1000000=1000000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: R/W/S=1100000/0/0 in:1100000=1100000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: R/W/S=1200000/0/0 in:1200000=1200000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: R/W/S=1300000/0/0 in:1300000=1300000/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: Records R/W=789/1 > sort\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: Records R/W=1348313/1\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 16:40:18 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/30 16:40:18 INFO mapred.Task: Task:attempt_local353958300_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: Records R/W=1348313/1\n",
      "16/01/30 16:40:18 INFO mapred.Task: Task 'attempt_local353958300_0001_m_000000_0' done.\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local353958300_0001_m_000000_0\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: Starting task: attempt_local353958300_0001_r_000000_0\n",
      "16/01/30 16:40:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/30 16:40:18 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/30 16:40:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/30 16:40:18 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@13793748\n",
      "16/01/30 16:40:18 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/30 16:40:18 INFO reduce.EventFetcher: attempt_local353958300_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/30 16:40:18 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local353958300_0001_m_000000_0 decomp: 2730 len: 2734 to MEMORY\n",
      "16/01/30 16:40:18 INFO reduce.InMemoryMapOutput: Read 2730 bytes from map-output for attempt_local353958300_0001_m_000000_0\n",
      "16/01/30 16:40:18 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 2730, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2730\n",
      "16/01/30 16:40:18 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 16:40:18 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/30 16:40:18 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 16:40:18 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2724 bytes\n",
      "16/01/30 16:40:18 INFO reduce.MergeManagerImpl: Merged 1 segments, 2730 bytes to disk to satisfy reduce memory limit\n",
      "16/01/30 16:40:18 INFO reduce.MergeManagerImpl: Merging 1 files, 2734 bytes from disk\n",
      "16/01/30 16:40:18 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/30 16:40:18 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/30 16:40:18 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 2724 bytes\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW3/./reducer.py]\n",
      "16/01/30 16:40:18 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/30 16:40:18 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: Records R/W=189/1\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/30 16:40:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/30 16:40:18 INFO mapred.Task: Task:attempt_local353958300_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/30 16:40:18 INFO mapred.Task: Task attempt_local353958300_0001_r_000000_0 is allowed to commit now\n",
      "16/01/30 16:40:18 INFO output.FileOutputCommitter: Saved output of task 'attempt_local353958300_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/word_count/_temporary/0/task_local353958300_0001_r_000000\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: Records R/W=189/1 > reduce\n",
      "16/01/30 16:40:18 INFO mapred.Task: Task 'attempt_local353958300_0001_r_000000_0' done.\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local353958300_0001_r_000000_0\n",
      "16/01/30 16:40:18 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/30 16:40:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/30 16:40:19 INFO mapreduce.Job: Job job_local353958300_0001 completed successfully\n",
      "16/01/30 16:40:19 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=217598\n",
      "\t\tFILE: Number of bytes written=808612\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=101812972\n",
      "\t\tHDFS: Number of bytes written=2350\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=1348313\n",
      "\t\tMap output bytes=13424747\n",
      "\t\tMap output materialized bytes=2734\n",
      "\t\tInput split bytes=111\n",
      "\t\tCombine input records=1348313\n",
      "\t\tCombine output records=189\n",
      "\t\tReduce input groups=189\n",
      "\t\tReduce shuffle bytes=2734\n",
      "\t\tReduce input records=189\n",
      "\t\tReduce output records=189\n",
      "\t\tSpilled Records=378\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=543162368\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2350\n",
      "16/01/30 16:40:19 INFO streaming.StreamJob: Output directory: word_count\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-combiner reducer.py \\\n",
    "-input Consumer_Complaints.csv \\\n",
    "-output word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/word_count/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/30 16:40:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 16:40:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/Consumer_Complaints.csv\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/30 16:40:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/30 16:40:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/word_count\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/30 16:40:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/30 16:41:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/Consumer_Complaints.csv\n",
    "!hadoop fs -rmr /user/dunmireg/word_count\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
