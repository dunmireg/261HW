{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Homework 261 Spring 2016 - Glenn Dunmire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.0:\n",
    "\n",
    "_What is a race condition in the context of parallel computation? Give an example._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A race condition is a situation where the final output of running a program depends on the sequence of events. That is to say, the final value may be different depending on the order in which steps are executed. \n",
    "\n",
    "A classic example of a race condition is where two threads want to increase a variable. Ideally one thread would increment the variable, then the other thread would. So if the original value was 0, the final output would be 2. However, if the threads access the variable at the same time or without a lock, the result could be 1. This would be because the threads overwrite each other. So A increments 0 -> 1 but then B overwrites the variable with 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_What is MapReduce? How does it differ from Hadoop?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, MapReduce is a programming framework while Hadoop is an implementation of MapReduce. MapReduce is a model for processing large datasets using a parallel, distributed algorithm on a cluster. Hadoop is a specific implementation of MapReduce in Java, which uses a special distributed file system (HDFS) and manages aspects of workflow like distribution and fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is based on the paradigm of functional programming. This paradigm is based on the evaluation of mathematical functions and avoids changing state or mutable data. An important point is that a functional language is the concept of a function that can take other functions as an argument, also known as higher-order functions. Map and Reduce are examples of this higher order function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 12]\n"
     ]
    }
   ],
   "source": [
    "#Example of a functional program: using map to print the lengths of strings in a list\n",
    "\n",
    "states = [\"Maryland\", \"Virginia\", \"Pennsylvania\"]\n",
    "states_length = map(len, states)\n",
    "print states_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here I am providing the map function with another function, len(). Also here I am not changing the values inside the list nor am I relying on anything other than the input list to produce my output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: \n",
    "\n",
    "Given as input: Records of the form '<'integer, “NA”>, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form <'integer, “NA”> in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form <'integer, “NA”>. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write a text file of form <integer, \"NA\">. \n",
    "#use the random package to get random numbers\n",
    "import random\n",
    "\n",
    "N = 10000 #set size of list of numbers\n",
    "\n",
    "#I chose to only include a list from 0 to 10000 to make it easy to check if the numbers were sorted properly. \n",
    "numbers = random.sample(range(0, 10000), N) #list of numbers at random\n",
    "output = [] #store output\n",
    "for number in numbers:\n",
    "    output.append('<' + str(number) + ', ' + 'NA>') #properly format strings\n",
    "\n",
    "with open('integer.txt', 'w') as myfile: #write output to a text file\n",
    "    myfile.write(\"\\n\".join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line[1:] #remove beginning '<' \n",
    "    num = line.split()[0] #split on whitespace and only keep number\n",
    "    num = num[:-1] #remove trailing comma\n",
    "    print '%s\\t%s' % (num, 'NA') #print result to STDOUT for input to reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    num, na = line.split('\\t')\n",
    "    \n",
    "    print '<' + num + ', NA>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test mapper and reducer\n",
    "#!cat integer.txt | python mapper.py | sort -n | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/25 14:47:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/25 14:47:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:24:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#make directory\n",
    "#!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 14:47:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put integer.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 14:47:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 14:47:57 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 14:47:57 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 14:47:57 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 14:47:57 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 14:47:57 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 14:47:57 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/25 14:47:57 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/25 14:47:58 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local264777542_0001\n",
      "16/01/25 14:47:58 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 14:47:58 INFO mapreduce.Job: Running job: job_local264777542_0001\n",
      "16/01/25 14:47:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: Starting task: attempt_local264777542_0001_m_000000_0\n",
      "16/01/25 14:47:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 14:47:58 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 14:47:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/integer.txt:0+108889\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 14:47:58 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 14:47:58 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 14:47:58 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:58 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:58 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:58 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:58 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:58 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/25 14:47:58 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 14:47:58 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: \n",
      "16/01/25 14:47:58 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: bufstart = 0; bufend = 78890; bufvoid = 104857600\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26174400(104697600); length = 39997/6553600\n",
      "16/01/25 14:47:58 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 14:47:58 INFO mapred.Task: Task:attempt_local264777542_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: Records R/W=10000/1\n",
      "16/01/25 14:47:58 INFO mapred.Task: Task 'attempt_local264777542_0001_m_000000_0' done.\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local264777542_0001_m_000000_0\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 14:47:58 INFO mapred.LocalJobRunner: Starting task: attempt_local264777542_0001_r_000000_0\n",
      "16/01/25 14:47:58 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 14:47:58 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 14:47:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 14:47:58 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@46526d0d\n",
      "16/01/25 14:47:58 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 14:47:58 INFO reduce.EventFetcher: attempt_local264777542_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 14:47:58 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local264777542_0001_m_000000_0 decomp: 98892 len: 98896 to MEMORY\n",
      "16/01/25 14:47:59 INFO reduce.InMemoryMapOutput: Read 98892 bytes from map-output for attempt_local264777542_0001_m_000000_0\n",
      "16/01/25 14:47:59 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98892, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98892\n",
      "16/01/25 14:47:59 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 14:47:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 14:47:59 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 14:47:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 14:47:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 98888 bytes\n",
      "16/01/25 14:47:59 INFO reduce.MergeManagerImpl: Merged 1 segments, 98892 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 14:47:59 INFO reduce.MergeManagerImpl: Merging 1 files, 98896 bytes from disk\n",
      "16/01/25 14:47:59 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 14:47:59 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 14:47:59 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 98888 bytes\n",
      "16/01/25 14:47:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 14:47:59 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/25 14:47:59 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 14:47:59 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 14:47:59 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:59 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:59 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:59 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:59 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 14:47:59 INFO streaming.PipeMapRed: Records R/W=10000/1\n",
      "16/01/25 14:47:59 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 14:47:59 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 14:47:59 INFO mapreduce.Job: Job job_local264777542_0001 running in uber mode : false\n",
      "16/01/25 14:47:59 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/25 14:47:59 INFO mapred.Task: Task:attempt_local264777542_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 14:47:59 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 14:47:59 INFO mapred.Task: Task attempt_local264777542_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 14:47:59 INFO output.FileOutputCommitter: Saved output of task 'attempt_local264777542_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/integerOutput/_temporary/0/task_local264777542_0001_r_000000\n",
      "16/01/25 14:47:59 INFO mapred.LocalJobRunner: Records R/W=10000/1 > reduce\n",
      "16/01/25 14:47:59 INFO mapred.Task: Task 'attempt_local264777542_0001_r_000000_0' done.\n",
      "16/01/25 14:47:59 INFO mapred.LocalJobRunner: Finishing task: attempt_local264777542_0001_r_000000_0\n",
      "16/01/25 14:47:59 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 14:48:00 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 14:48:00 INFO mapreduce.Job: Job job_local264777542_0001 completed successfully\n",
      "16/01/25 14:48:00 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=409896\n",
      "\t\tFILE: Number of bytes written=1097568\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=217778\n",
      "\t\tHDFS: Number of bytes written=118890\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=10000\n",
      "\t\tMap output records=10000\n",
      "\t\tMap output bytes=78890\n",
      "\t\tMap output materialized bytes=98896\n",
      "\t\tInput split bytes=99\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10000\n",
      "\t\tReduce shuffle bytes=98896\n",
      "\t\tReduce input records=10000\n",
      "\t\tReduce output records=10000\n",
      "\t\tSpilled Records=20000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=546308096\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=108889\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=118890\n",
      "16/01/25 14:48:00 INFO streaming.StreamJob: Output directory: integerOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-n \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input integer.txt \\\n",
    "-output integerOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/integerOutput/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 14:49:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 14:49:02 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/25 14:49:02 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory\n",
    "!hadoop fs -copyToLocal /user/dunmireg/integerOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 14:49:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 14:49:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/integer.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 14:49:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 14:49:07 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/integerOutput\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 14:49:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 14:49:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/integer.txt #check\n",
    "!hadoop fs -rmr /user/dunmireg/integerOutput\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest 10:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Largest 10\n",
      "9990\n",
      "9991\n",
      "9992\n",
      "9993\n",
      "9994\n",
      "9995\n",
      "9996\n",
      "9997\n",
      "9998\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "#display output\n",
    "import os\n",
    "\n",
    "with open(os.path.join('./integerOutput', 'part-00000'), 'r') as myfile: #get appropriate output\n",
    "    lines = myfile.readlines() #read in lines\n",
    "    print \"Smallest 10:\"\n",
    "    for i in range(10): #get smallest 10, the first 10 numbers\n",
    "        line = lines[i] #get right line\n",
    "        line = line[1:] #remove '<'\n",
    "        num = line.split()[0] #split on whitespace, keeping number\n",
    "        num = num[:-1] #remove comma\n",
    "        print num\n",
    "        \n",
    "    print \"Largest 10\"\n",
    "    for i in range(9990, 10000): #repeat above with different range\n",
    "        line = lines[i]\n",
    "        line = line[1:]\n",
    "        num = line.split()[0]\n",
    "        num = num[:-1]\n",
    "        print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 2.2\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "\n",
    " \n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "       8    \n",
    "       #NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #regex for string matching\n",
    "\n",
    "for line in sys.stdin: #for each line\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip() #get text of subject and content\n",
    "    words = re.findall(WORD_RE, text) #match all words\n",
    "    for word in words:\n",
    "        print word + '\\t' + '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#credit to Professor Shanahan for the structure of this reducer\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "#lines come from standard input\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    line = line.split('\\t')\n",
    "    word = line[0]\n",
    "    count = int(line[1])\n",
    "    \n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "#print last line\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!cat enronemail_1h.txt | python mapper.py | sort | python reducer.py\n",
    "#confirm assistance = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/25 18:17:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/25 18:17:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 18:17:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 18:17:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 18:17:50 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 18:17:50 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 18:17:50 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 18:17:50 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 18:17:51 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 18:17:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1364191356_0001\n",
      "16/01/25 18:17:51 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 18:17:51 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 18:17:51 INFO mapreduce.Job: Running job: job_local1364191356_0001\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: Starting task: attempt_local1364191356_0001_m_000000_0\n",
      "16/01/25 18:17:51 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 18:17:51 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 18:17:51 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 18:17:51 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 18:17:51 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 18:17:51 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 18:17:51 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 18:17:51 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/25 18:17:51 INFO streaming.PipeMapRed: R/W/S=100/13450/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 18:17:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 18:17:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: \n",
      "16/01/25 18:17:51 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: bufstart = 0; bufend = 252211; bufvoid = 104857600\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26082748(104330992); length = 131649/6553600\n",
      "16/01/25 18:17:51 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 18:17:51 INFO mapred.Task: Task:attempt_local1364191356_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/25 18:17:51 INFO mapred.Task: Task 'attempt_local1364191356_0001_m_000000_0' done.\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: Finishing task: attempt_local1364191356_0001_m_000000_0\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 18:17:51 INFO mapred.LocalJobRunner: Starting task: attempt_local1364191356_0001_r_000000_0\n",
      "16/01/25 18:17:51 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 18:17:51 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 18:17:51 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 18:17:52 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@76742bea\n",
      "16/01/25 18:17:52 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 18:17:52 INFO reduce.EventFetcher: attempt_local1364191356_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 18:17:52 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1364191356_0001_m_000000_0 decomp: 318039 len: 318043 to MEMORY\n",
      "16/01/25 18:17:52 INFO reduce.InMemoryMapOutput: Read 318039 bytes from map-output for attempt_local1364191356_0001_m_000000_0\n",
      "16/01/25 18:17:52 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 318039, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->318039\n",
      "16/01/25 18:17:52 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 18:17:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 18:17:52 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 18:17:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 18:17:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318035 bytes\n",
      "16/01/25 18:17:52 INFO reduce.MergeManagerImpl: Merged 1 segments, 318039 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 18:17:52 INFO reduce.MergeManagerImpl: Merging 1 files, 318043 bytes from disk\n",
      "16/01/25 18:17:52 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 18:17:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 18:17:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318035 bytes\n",
      "16/01/25 18:17:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 18:17:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/25 18:17:52 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 18:17:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 18:17:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 18:17:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 18:17:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 18:17:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 18:17:52 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 18:17:52 INFO streaming.PipeMapRed: Records R/W=21196/1\n",
      "16/01/25 18:17:52 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 18:17:52 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 18:17:52 INFO mapreduce.Job: Job job_local1364191356_0001 running in uber mode : false\n",
      "16/01/25 18:17:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/25 18:17:52 INFO mapred.Task: Task:attempt_local1364191356_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 18:17:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 18:17:52 INFO mapred.Task: Task attempt_local1364191356_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 18:17:52 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1364191356_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneWordCount/_temporary/0/task_local1364191356_0001_r_000000\n",
      "16/01/25 18:17:52 INFO mapred.LocalJobRunner: Records R/W=21196/1 > reduce\n",
      "16/01/25 18:17:52 INFO mapred.Task: Task 'attempt_local1364191356_0001_r_000000_0' done.\n",
      "16/01/25 18:17:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local1364191356_0001_r_000000_0\n",
      "16/01/25 18:17:52 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 18:17:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 18:17:53 INFO mapreduce.Job: Job job_local1364191356_0001 completed successfully\n",
      "16/01/25 18:17:53 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=848202\n",
      "\t\tFILE: Number of bytes written=1755937\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=53488\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32913\n",
      "\t\tMap output bytes=252211\n",
      "\t\tMap output materialized bytes=318043\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5491\n",
      "\t\tReduce shuffle bytes=318043\n",
      "\t\tReduce input records=32913\n",
      "\t\tReduce output records=5491\n",
      "\t\tSpilled Records=65826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=510656512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53488\n",
      "16/01/25 18:17:53 INFO streaming.StreamJob: Output directory: enroneWordCount\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/enroneWordCount/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 18:18:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 18:18:13 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory\n",
    "#bin/hadoop fs -copyToLocal /hdfs/source/path /localfs/destination/path\n",
    "!hadoop fs -copyToLocal /user/dunmireg/enroneWordCount/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 18:18:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 18:18:22 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 18:18:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 18:18:23 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneWordCount\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 18:18:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 18:18:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt \n",
    "!hadoop fs -rmr /user/dunmireg/enroneWordCount\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times assistance occurs: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rename output file for convenience and print results of assistance\n",
    "#import os\n",
    "#os.rename('part-00000', 'wordCount') #only needs to run once\n",
    "\n",
    "#open file and read contents\n",
    "with open('wordCount', 'r') as myfile:\n",
    "    lines = myfile.readlines()\n",
    "    for line in lines:\n",
    "        components = line.split('\\t') #parse input\n",
    "        if components[0] == 'assistance': #check if found the right word\n",
    "            print \"Number of times assistance occurs: \" + components[1] #print results\n",
    "            break  #break loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1\n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    #reverse input, so instead of word, count it now becomes count, word with count serving as key\n",
    "    #note convert number to an int to remove new line character, then turn to string\n",
    "    print components[1].rstrip() + '\\t' + components[0] #print result to STDOUT for input to reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    line = line.split('\\t')\n",
    "    count = line[0]\n",
    "    word = line[1].rstrip()\n",
    "    \n",
    "    #reverse order, relying on hadoop shuffling to get into proper order\n",
    "    print word + '\\t' + count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/25 19:11:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/25 19:11:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:11:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put wordCount /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:11:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:11:47 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 19:11:47 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 19:11:47 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 19:11:47 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 19:11:47 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/25 19:11:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local327801418_0001\n",
      "16/01/25 19:11:48 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 19:11:48 INFO mapreduce.Job: Running job: job_local327801418_0001\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 19:11:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: Starting task: attempt_local327801418_0001_m_000000_0\n",
      "16/01/25 19:11:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 19:11:48 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 19:11:48 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/wordCount:0+53488\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: Records R/W=5491/1\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: \n",
      "16/01/25 19:11:48 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: bufstart = 0; bufend = 53488; bufvoid = 104857600\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26192436(104769744); length = 21961/6553600\n",
      "16/01/25 19:11:48 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 19:11:48 INFO mapred.Task: Task:attempt_local327801418_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: Records R/W=5491/1\n",
      "16/01/25 19:11:48 INFO mapred.Task: Task 'attempt_local327801418_0001_m_000000_0' done.\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: Finishing task: attempt_local327801418_0001_m_000000_0\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: Starting task: attempt_local327801418_0001_r_000000_0\n",
      "16/01/25 19:11:48 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 19:11:48 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 19:11:48 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 19:11:48 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@68c5d7a0\n",
      "16/01/25 19:11:48 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 19:11:48 INFO reduce.EventFetcher: attempt_local327801418_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 19:11:48 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local327801418_0001_m_000000_0 decomp: 64472 len: 64476 to MEMORY\n",
      "16/01/25 19:11:48 INFO reduce.InMemoryMapOutput: Read 64472 bytes from map-output for attempt_local327801418_0001_m_000000_0\n",
      "16/01/25 19:11:48 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 64472, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->64472\n",
      "16/01/25 19:11:48 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 19:11:48 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 19:11:48 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 19:11:48 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 64468 bytes\n",
      "16/01/25 19:11:48 INFO reduce.MergeManagerImpl: Merged 1 segments, 64472 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 19:11:48 INFO reduce.MergeManagerImpl: Merging 1 files, 64476 bytes from disk\n",
      "16/01/25 19:11:48 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 19:11:48 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 19:11:48 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 64468 bytes\n",
      "16/01/25 19:11:48 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 19:11:48 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: Records R/W=5491/1\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 19:11:48 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 19:11:49 INFO mapred.Task: Task:attempt_local327801418_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 19:11:49 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 19:11:49 INFO mapred.Task: Task attempt_local327801418_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 19:11:49 INFO output.FileOutputCommitter: Saved output of task 'attempt_local327801418_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/sortedWordCount/_temporary/0/task_local327801418_0001_r_000000\n",
      "16/01/25 19:11:49 INFO mapred.LocalJobRunner: Records R/W=5491/1 > reduce\n",
      "16/01/25 19:11:49 INFO mapred.Task: Task 'attempt_local327801418_0001_r_000000_0' done.\n",
      "16/01/25 19:11:49 INFO mapred.LocalJobRunner: Finishing task: attempt_local327801418_0001_r_000000_0\n",
      "16/01/25 19:11:49 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 19:11:49 INFO mapreduce.Job: Job job_local327801418_0001 running in uber mode : false\n",
      "16/01/25 19:11:49 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 19:11:49 INFO mapreduce.Job: Job job_local327801418_0001 completed successfully\n",
      "16/01/25 19:11:49 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=341050\n",
      "\t\tFILE: Number of bytes written=994302\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=106976\n",
      "\t\tHDFS: Number of bytes written=53488\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5491\n",
      "\t\tMap output records=5491\n",
      "\t\tMap output bytes=53488\n",
      "\t\tMap output materialized bytes=64476\n",
      "\t\tInput split bytes=97\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=107\n",
      "\t\tReduce shuffle bytes=64476\n",
      "\t\tReduce input records=5491\n",
      "\t\tReduce output records=5491\n",
      "\t\tSpilled Records=10982\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=6\n",
      "\t\tTotal committed heap usage (bytes)=542113792\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=53488\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53488\n",
      "16/01/25 19:11:49 INFO streaming.StreamJob: Output directory: sortedWordCount\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-n \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input wordCount \\\n",
    "-output sortedWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/sortedWordCount/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 19:11:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:11:57 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/25 19:11:57 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory\n",
    "!hadoop fs -copyToLocal /user/dunmireg/sortedWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 19:11:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:12:00 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/wordCount\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 19:12:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 19:12:01 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/sortedWordCount\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 19:12:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 19:12:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/wordCount\n",
    "!hadoop fs -rmr /user/dunmireg/sortedWordCount\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for\t373\n",
      "\n",
      "ect\t382\n",
      "\n",
      "your\t394\n",
      "\n",
      "in\t417\n",
      "\n",
      "you\t432\n",
      "\n",
      "a\t542\n",
      "\n",
      "of\t566\n",
      "\n",
      "and\t668\n",
      "\n",
      "to\t963\n",
      "\n",
      "the\t1247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display outputs from results file\n",
    "import os\n",
    "\n",
    "with open(os.path.join('./sortedWordCount', 'part-00000'), 'r') as myfile:\n",
    "    lines = myfile.readlines() #read file\n",
    "    lines = lines[-10:] #get last 10 lines\n",
    "    for line in lines:\n",
    "        print line #print results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see. \n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #regex for string matching\n",
    "\n",
    "for line in sys.stdin: #for each line\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip() #get text of subject and content\n",
    "    words = re.findall(WORD_RE, text) #match all words\n",
    "    for word in words:\n",
    "        print components[0] + '\\t' + word + '\\t' + components[1] #print email ID + word + spam flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "emails = set() #hold email IDs\n",
    "words = {} #hold words and associated counts\n",
    "spam_emails = 0 #how many emails are marked as spam\n",
    "spam_word_count = 0 #how many words appear in spam\n",
    "ham_word_count = 0 #how many words appear in ham\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t') #split input\n",
    "    \n",
    "    ID = components[0] #put input variables into fields to make easier\n",
    "    word = components[1]\n",
    "    spam = int(components[2])\n",
    "    \n",
    "    if word not in words.keys(): #if a word is not in the words dictionary, add it and initialize counts to 0\n",
    "        words[word] = {'spam_count': 0, 'ham_count': 0}\n",
    "    if ID not in emails: #add email to set to store unique IDs\n",
    "        emails.add(ID) \n",
    "        if spam == 1: #increment spam counter\n",
    "            spam_emails += 1\n",
    "        \n",
    "\n",
    "    if spam == 1: #if the flag if spam, increment the word spam_count value by 1, else do the same for ham\n",
    "        words[word]['spam_count'] += 1\n",
    "        spam_word_count += 1\n",
    "    else:\n",
    "        words[word]['ham_count'] += 1\n",
    "        ham_word_count += 1\n",
    "\n",
    "\n",
    "prior_spam = float(spam_emails)/len(emails) #get prior probabilities\n",
    "prior_ham = 1-prior_spam\n",
    "\n",
    "for i, word in words.iteritems(): #calculate conditional probabilities: number of times word appears in class/number of words in class\n",
    "    word['spam_like'] = float(word['spam_count'])/(spam_word_count)\n",
    "    word['ham_like'] = float(word['ham_count'])/(ham_word_count)\n",
    "    \n",
    "\n",
    "print prior_spam #print priors\n",
    "print prior_ham\n",
    "for word in words.keys():\n",
    "    #Word \"\\t\" spam likelihood '\\t' ham likelihood written to file\n",
    "    print word + '\\t' + str(words[word]['spam_like']) + '\\t' + str(words[word]['ham_like']) #print each word along with spam and ham conditional probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Examine output\n",
    "#!cat enronemail_1h.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/25 15:10:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/25 15:10:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 00:14:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#make directory\n",
    "#!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 15:10:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 15:10:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 15:10:50 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 15:10:50 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 15:10:50 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 15:10:51 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 15:10:51 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 15:10:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1213745686_0001\n",
      "16/01/25 15:10:51 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 15:10:51 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 15:10:51 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 15:10:51 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 15:10:51 INFO mapreduce.Job: Running job: job_local1213745686_0001\n",
      "16/01/25 15:10:51 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 15:10:51 INFO mapred.LocalJobRunner: Starting task: attempt_local1213745686_0001_m_000000_0\n",
      "16/01/25 15:10:51 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 15:10:51 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 15:10:51 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 15:10:51 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 15:10:51 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 15:10:51 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:10:51 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:10:51 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/25 15:10:51 INFO streaming.PipeMapRed: R/W/S=100/7458/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:10:51 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 15:10:51 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 15:10:51 INFO mapred.LocalJobRunner: \n",
      "16/01/25 15:10:51 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: bufstart = 0; bufend = 1032108; bufvoid = 104857600\n",
      "16/01/25 15:10:51 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26082748(104330992); length = 131649/6553600\n",
      "16/01/25 15:10:52 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 15:10:52 INFO mapred.Task: Task:attempt_local1213745686_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 15:10:52 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/25 15:10:52 INFO mapred.Task: Task 'attempt_local1213745686_0001_m_000000_0' done.\n",
      "16/01/25 15:10:52 INFO mapred.LocalJobRunner: Finishing task: attempt_local1213745686_0001_m_000000_0\n",
      "16/01/25 15:10:52 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 15:10:52 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 15:10:52 INFO mapred.LocalJobRunner: Starting task: attempt_local1213745686_0001_r_000000_0\n",
      "16/01/25 15:10:52 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 15:10:52 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 15:10:52 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 15:10:52 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@558293ca\n",
      "16/01/25 15:10:52 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 15:10:52 INFO reduce.EventFetcher: attempt_local1213745686_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 15:10:52 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1213745686_0001_m_000000_0 decomp: 1097936 len: 1097940 to MEMORY\n",
      "16/01/25 15:10:52 INFO reduce.InMemoryMapOutput: Read 1097936 bytes from map-output for attempt_local1213745686_0001_m_000000_0\n",
      "16/01/25 15:10:52 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1097936, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1097936\n",
      "16/01/25 15:10:52 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 15:10:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 15:10:52 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 15:10:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 15:10:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/25 15:10:52 INFO reduce.MergeManagerImpl: Merged 1 segments, 1097936 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 15:10:52 INFO reduce.MergeManagerImpl: Merging 1 files, 1097940 bytes from disk\n",
      "16/01/25 15:10:52 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 15:10:52 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 15:10:52 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/25 15:10:52 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 15:10:52 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/25 15:10:52 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 15:10:52 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 15:10:52 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:10:52 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:10:52 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:10:52 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:10:52 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:10:52 INFO mapreduce.Job: Job job_local1213745686_0001 running in uber mode : false\n",
      "16/01/25 15:10:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/25 15:10:54 INFO streaming.PipeMapRed: Records R/W=32913/1\n",
      "16/01/25 15:10:54 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 15:10:54 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 15:10:54 INFO mapred.Task: Task:attempt_local1213745686_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 15:10:54 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 15:10:54 INFO mapred.Task: Task attempt_local1213745686_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 15:10:54 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1213745686_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailCondProbs/_temporary/0/task_local1213745686_0001_r_000000\n",
      "16/01/25 15:10:54 INFO mapred.LocalJobRunner: Records R/W=32913/1 > reduce\n",
      "16/01/25 15:10:54 INFO mapred.Task: Task 'attempt_local1213745686_0001_r_000000_0' done.\n",
      "16/01/25 15:10:54 INFO mapred.LocalJobRunner: Finishing task: attempt_local1213745686_0001_r_000000_0\n",
      "16/01/25 15:10:54 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 15:10:55 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 15:10:55 INFO mapreduce.Job: Job job_local1213745686_0001 completed successfully\n",
      "16/01/25 15:10:55 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2407996\n",
      "\t\tFILE: Number of bytes written=4095648\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=172513\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32913\n",
      "\t\tMap output bytes=1032108\n",
      "\t\tMap output materialized bytes=1097940\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=1097940\n",
      "\t\tReduce input records=32913\n",
      "\t\tReduce output records=5493\n",
      "\t\tSpilled Records=65826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=511180800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=172513\n",
      "16/01/25 15:10:55 INFO streaming.StreamJob: Output directory: enroneEmailCondProbs\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailCondProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/enroneEmailCondProbs/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 15:11:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 15:11:01 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/25 15:11:01 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory\n",
    "#bin/hadoop fs -copyToLocal /hdfs/source/path /localfs/destination/path\n",
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailCondProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 15:11:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 15:11:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 15:11:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 15:11:06 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailCondProbs\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 15:11:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 15:11:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt #check\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailCondProbs\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#The bulk of the classification work happens in the mapper. This will read in the file of conditional probabilities\n",
    "#and then compute the conditional probability of each word and performs the classification. The classification output\n",
    "#is then sent to the reducer\n",
    "import sys\n",
    "import re\n",
    "import os \n",
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "priorSpam = 0 #prior probabilities from file\n",
    "priorHam = 0\n",
    "words = {} #dictionary to hold word conditional probabilities\n",
    "\n",
    "with open(os.path.join('./enroneEmailCondProbs', 'part-00000'), 'r') as myfile: #read file\n",
    "    lines = myfile.readlines()\n",
    "    priorSpam = float(lines[0]) #grab prior probabilities\n",
    "    priorHam = float(lines[1])\n",
    "    for line in lines[2:]: #parse lines for word with conditional probabilities\n",
    "        components = line.split('\\t')\n",
    "        words[components[0]] = {'spam_like': float(components[1]), 'ham_like': float(components[2])}\n",
    "        \n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "spamSkip = 0 #how many times did a skip occur in spam and ham\n",
    "hamSkip = 0\n",
    "\n",
    "\n",
    "#NB: I decided to add a large negative number to the probability of each class if the word did not appear\n",
    "#in that class. If I skipped over the word, the accuracy was 0%. I decided that although this resembles smoothing\n",
    "#it is still appropriate. If I were to skip a word that means that the conditional probability of a word appearing \n",
    "#in the class it did not appear in is 0, which is not true. Instead I set it to a small number. \n",
    "#Other methods have been discussed in class but I believe this is the most appropriate. \n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t') #split line\n",
    "    text = \" \".join(components[-2:]).strip() #combine subject and text\n",
    "    text = re.findall(WORD_RE, text)\n",
    "    \n",
    "    spamScore = log(priorSpam) #take logs\n",
    "    hamScore = log(priorHam)\n",
    "    for word in text:\n",
    "        if word in words.keys():\n",
    "            if float(words[word]['spam_like']) != 0: #this checks if a word has occurred in a class\n",
    "                spamScore += log(float(words[word]['spam_like'])) #increment probability\n",
    "            else:\n",
    "                spamScore += -300\n",
    "                spamSkip += 1 #skipped over a word in spam\n",
    "            if float(words[word]['ham_like']) != 0: #repeat procedure for ham\n",
    "                hamScore += log(float(words[word]['ham_like']))\n",
    "            else:\n",
    "                hamScore += -300\n",
    "                hamSkip += 1\n",
    "        pred = 0 #predicted class\n",
    "    if spamScore > hamScore:\n",
    "        pred = 1\n",
    "    #output is email ID (key), true flag, predicted class, posterior probabilities (exponentiated) and skip counts\n",
    "    #When I tried to print the skip counts by themselves there was an error. I do not know the cause of this error \n",
    "    #and I know it is inefficient and wrong, but this allows me to at least process it\n",
    "    print components[0] + '\\t' + components[1] + '\\t' + str(pred) + '\\t' + str(exp(spamScore)) + '\\t' + str(exp(hamScore)) + '\\t' + str(spamSkip) + '\\t' + str(hamSkip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "misclassified = 0 #number of emails misclassified\n",
    "skipSpam = 0 #number of times skip a word in spam\n",
    "skipHam = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t') #parse intput\n",
    "    if int(components[1]) != int(components[2]): #if the true classification and the predicted do not agree, increment\n",
    "            misclassified += 1\n",
    "    skipSpam = int(components[5])\n",
    "    skipHam = int(components[6])\n",
    "    print line #print results\n",
    "#print output\n",
    "print \"Misclassified: \" + str(misclassified) + \" which means this has an accuracy of \" + str(100-misclassified) + \"%\"\n",
    "print \"Skipped \" + str(skipSpam) + \" words in spam\"\n",
    "print \"Skipped \" + str(skipHam) + \" words in ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check results\n",
    "#!cat enronemail_1h.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/25 15:34:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/25 15:34:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make directory\n",
    "#!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 15:34:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 15:34:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 15:34:31 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 15:34:31 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 15:34:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 15:34:31 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 15:34:31 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 15:34:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local687702657_0001\n",
      "16/01/25 15:34:32 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 15:34:32 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 15:34:32 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 15:34:32 INFO mapreduce.Job: Running job: job_local687702657_0001\n",
      "16/01/25 15:34:32 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 15:34:32 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 15:34:32 INFO mapred.LocalJobRunner: Starting task: attempt_local687702657_0001_m_000000_0\n",
      "16/01/25 15:34:32 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 15:34:32 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 15:34:32 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 15:34:32 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/25 15:34:32 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 15:34:32 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 15:34:32 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 15:34:32 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 15:34:32 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 15:34:32 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 15:34:32 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 15:34:32 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 15:34:32 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 15:34:32 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:34:32 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:34:33 INFO mapreduce.Job: Job job_local687702657_0001 running in uber mode : false\n",
      "16/01/25 15:34:33 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/25 15:34:33 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:100=100/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/25 15:34:36 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 15:34:36 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/25 15:34:36 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 15:34:36 INFO mapred.LocalJobRunner: \n",
      "16/01/25 15:34:36 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 15:34:36 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 15:34:36 INFO mapred.MapTask: bufstart = 0; bufend = 4855; bufvoid = 104857600\n",
      "16/01/25 15:34:36 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "16/01/25 15:34:36 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 15:34:36 INFO mapred.Task: Task:attempt_local687702657_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 15:34:36 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "16/01/25 15:34:36 INFO mapred.Task: Task 'attempt_local687702657_0001_m_000000_0' done.\n",
      "16/01/25 15:34:36 INFO mapred.LocalJobRunner: Finishing task: attempt_local687702657_0001_m_000000_0\n",
      "16/01/25 15:34:36 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 15:34:36 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 15:34:36 INFO mapred.LocalJobRunner: Starting task: attempt_local687702657_0001_r_000000_0\n",
      "16/01/25 15:34:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 15:34:36 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 15:34:36 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 15:34:36 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7551ee53\n",
      "16/01/25 15:34:36 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 15:34:36 INFO reduce.EventFetcher: attempt_local687702657_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 15:34:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local687702657_0001_m_000000_0 decomp: 5057 len: 5061 to MEMORY\n",
      "16/01/25 15:34:37 INFO reduce.InMemoryMapOutput: Read 5057 bytes from map-output for attempt_local687702657_0001_m_000000_0\n",
      "16/01/25 15:34:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 5057, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->5057\n",
      "16/01/25 15:34:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 15:34:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 15:34:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 15:34:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 15:34:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5032 bytes\n",
      "16/01/25 15:34:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 5057 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 15:34:37 INFO reduce.MergeManagerImpl: Merging 1 files, 5061 bytes from disk\n",
      "16/01/25 15:34:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 15:34:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 15:34:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 5032 bytes\n",
      "16/01/25 15:34:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 15:34:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/25 15:34:37 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 15:34:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 15:34:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:34:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:34:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 15:34:37 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/25 15:34:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 15:34:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 15:34:37 INFO mapred.Task: Task:attempt_local687702657_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 15:34:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 15:34:37 INFO mapred.Task: Task attempt_local687702657_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 15:34:37 INFO output.FileOutputCommitter: Saved output of task 'attempt_local687702657_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailClassificationNoSmoothing/_temporary/0/task_local687702657_0001_r_000000\n",
      "16/01/25 15:34:37 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "16/01/25 15:34:37 INFO mapred.Task: Task 'attempt_local687702657_0001_r_000000_0' done.\n",
      "16/01/25 15:34:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local687702657_0001_r_000000_0\n",
      "16/01/25 15:34:37 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 15:34:37 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 15:34:38 INFO mapreduce.Job: Job job_local687702657_0001 completed successfully\n",
      "16/01/25 15:34:38 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=222238\n",
      "\t\tFILE: Number of bytes written=814067\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=5169\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=4855\n",
      "\t\tMap output materialized bytes=5061\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=5061\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=203\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=510132224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=5169\n",
      "16/01/25 15:34:38 INFO streaming.StreamJob: Output directory: enroneEmailClassificationNoSmoothing\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailClassificationNoSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!hdfs dfs -cat /user/dunmireg/enroneEmailClassificationNoSmoothing/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 15:34:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 15:34:43 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/25 15:34:43 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory - makes easier to process for analysis\n",
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailClassificationNoSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 15:34:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 15:34:45 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 15:34:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 15:34:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailClassificationNoSmoothing\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 15:34:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 15:35:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt #check\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailClassificationNoSmoothing\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified: 0 which means this has an accuracy of 100%\t\n",
      "\n",
      "Skipped 4961 words in spam\t\n",
      "\n",
      "Skipped 5694 words in ham\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display outputs from results file\n",
    "import os\n",
    "\n",
    "with open(os.path.join('./enroneEmailClassificationNoSmoothing', 'part-00000'), 'r') as myfile:\n",
    "    lines = myfile.readlines() #read file\n",
    "    lines = lines[-3:] #get last 3 lines\n",
    "    for line in lines:\n",
    "        print line #print results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEXCAYAAABI/TQXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFV5JREFUeJzt3X2UXHV9x/H3kAceJEuIwRAeZGkEAV0rKoJaNFrkSFRA\nRBTFAkK1D0ewFZX0WInPglVBqbbFQqI1KEJF5AAnMWXEIGDQyKMxEo0SgUWSmESbVDDbP76/OXN3\nMrs7OzO7s/PL+3XOPTt37tPvNw+f+7u/e+8sSJIkSZIkSZIkSZIkSZIkSepSC4GPNrnsAuCrw0y/\nH3h5nXmfCWwBSsMsuwXobbJcrejUdjXB7NLpAuzk/gL4AfA7YD2wHHhRR0sUFgL/RwRFZVg5BttZ\nADyZ1r8RuB04psl1DaSh2WWH81zgtjrz/hqYVniuDJxTs+w0YG2T5RrKduDPap5bwOAdVSPbnQs8\n3K5CaWIy5DunB7gRuAzYG9gf+DARrp02AFxMBEVlOHKMtnN1Wv8+xE7uv4eYt5HP6nAt6nYtN9y8\nze5k2qGT2x6K+TIB+CZ0zqHEF/Mb6e82YClwX5p+FtGy/QLR0v8p8KrC8mcDDwKbgTXAOwvT5gLr\ngPcBjwOPACcD84DVxFHDhU2W+83AL4hgBjgBeBR4ehrfDrw7lem3wCUMHYylwrSngK8A+6Z1LQS+\nBNwE/D7V6XCitbyR6EJ5fc36ZgJLiNekTHSnVFxGtLw3AXcTR1EVA8BuwNfTsj8CnleYvpbBr31F\nb6rvJODjwLHA5cSRyefTPMVW967AvwC/Ah5L9dutUPYbU93WE0cOze60arc7D3iAqNs64B+BPYCb\ngf1SeTcTr/2uwKXAb9LwOWBqYb3vJz5P64Bza7azkB3fs9cSR4GbiNf/osK6etPyZ6Vp64G/AY4C\n7iVeiy80/xJInTUNeIL4YryGaM0XnUV0ZZxPhMhpRNhX5psHHJwevxz4A9XW9ty07AfTsuembX0N\neBpwBPC/wEFDlO0qhu/f/q80z9OJIJhXmLYdWAZMBw4EfsaOXRgVC6h2MewKfJpqF8NCor4vSePT\ngIeIndNk4JVEMB1amH8zEd5TiaD6fmFbbyNeu12IkHuUangtAP4InEK8Xu8ldmST0vRfUg35Ypl7\nU30rjaVbgXfU1LEYgp8Dridemz2BG4BPpGmfJAJyUhpextC2A3NqniuWq3a7jxbWtxfVz8kr2LG7\n5iNEF+LMNNyenoP4nD5K7Gx3Jz4HtSFffM92Tdt4ThrvI3ZuJ6Xx3rT8F4n34tXEkey30rb3A/qp\nng+Rus5hRFg+TITyt4FnpGlnEQFadBdwxhDr+hZwXno8lwjxSktwGvFlOqow/91Uv2y1FgJbiZZU\nZbiqMH0vojV6LxFMRduB4wvjfwt8d4jtLCC+1BuJL/N3qQbQwjRUHEsETNFiqi3DhWm84mnE0cH+\nQ2x7AxE6lXL8oDCtRLRWK8E4mpCv3aFVQrBEtG6LfekvIXYmEF1117NjeNeznWgZF9+frcSRUO12\nId6rdxJdhEVz2THkHyLCvOJ4ov4AVxJHLBVz2DHkF45Q9kuBz6bHvWn52YXpTwBvKoxfSzR01CS7\nazprFdHtciBxcm8/4ktQURvyv6L6hTgBuJM4xN1ItKafXph3PdV+2q3pb39h+lYiCOsZIFrVexeG\nswvTNxFfvucCn6mzfDE4fk3UayjfSOufBRxH9QTvANElULEfOwbSrwrrrp3/D0SQV6ZfQHRv/Y54\nvfYiWosVxWUr6xqu3EMZqm98H6KL5EdUg/nmQhk+TQTsEqKr6wMjbOdIBr8/n2Lo7p03Ep+PtUQ3\n1nAnt/cjXteK4vs3m8HvQfE1g6h77Xt0NLHze5x47d/F4M8p7Pi5rB3fc5jyagSG/MTxM2AREZwV\nta3Qg4gW5q7AdUR/9zOIL/lNtNaHOxrPJ0J/MfX7TJ9Z87h2Z1UxQOMnMh8hdobF+Q8qrLuUplfs\nCcxIyx1LnJ94E9FVsjexoyquq7jsLsABadnRGO7k5xNEYB1BNZinU21d/57YEc0BTiS6lOqdBxjK\ncK/j3cQ5mX2Io4VrhinvIwy+9LL4/j3K4Nep+Hgoi9M2DyDq+2+MPncm4knlrmHId86ziS9yJcgP\nBE4H7ijM8wyiC2YKEVCHEWE+NQ1PEIe7JzC4i6RVxROitXYj+mLnE/3P+xNdMkUXUO2TP49orQ+1\nneHKUHQn0QX1fuL1mAu8jjhZWjGP6GKZSpxTuIMIqGlE180TadqH2LHr4oXAG4j+/vcQJ8LvHKZ8\n9fQzdHfLduAK4khtn/Tc/lTft9cCzyLqvRn4UxpaNYU4H7FXWt+Wwnr7iVZ18bW4mjiXU+mT/xDx\nfkPsHM4mPod7AP9cs6167+eexFHLH4EXA29l9KE9Xo2XLBnynbOFOJS9i2jF3UH0cb+3MM9dwCHE\nVSofJQ67N6ZlzyO+dBuIncO3a9Zf+0UazRdrgAjT4nXyj6dpnyQO5/+d+OKeAXyMweH2baJbYiVx\nxciVw2xnqHLVTnuSuJrmBOL1uBx4O3G1UGX+rxF99OuJ7ozK+Ytb0rCa6LLYSnRDFLd1PXHl0AYi\nFE+hfsjWlqv4+DLg1LSOYrdbxQeILpk7iSOJpVRPHB+SxrcQ5wf+FfhenXXUbrORcp1B9KtvIvrm\n35aeX0WE+i9Smfcl3su7ic/ivenxx9L8txBXDd1KvJaVBknlst967+ffESduNxM7hdodfiOfS1vy\nY+hKYm9/X+G5GcSHcTXRfzi9MG0+8HPiw9POluXO6CwGXx3SLerdqKM8HU4cIdlYnMBGenOuYvCZ\ndohL2CotkGVUr7c+gmgJHZGW+WID65fUXd5AnBPam7hh7gZix64u1svglvwq4koIiMO7VenxfAZf\nEXALzd+iLjiT6q303eRP2JLP2c1Uf4bjOqpZoC7Wy+CQ31h4XCqMf4FqXx/Al4k+ZElSh7TanTLS\nj0J5wkSSOmhyE8v0E900jxE3R1SuuvgNg6+bPYA610fPmTNnYM2aNU1sVpJ2amuIy2xHpZmW/A1E\nfzHp7/WF599CXId8MHFJ2A93KOWaNRx//Kkcf/ypnHTS6WzYsIGBgYFshosuuqjjZbB+1m9nrF/O\ndRsYGIDGfvJiByO15K8mfmBoJnG78oeI26evIX6jYy3xw1kQt4xfk/4+RVwfW7e7ZsmSWGT33d/L\nunXr2Hvv2t/mkiS1w0ghf/oQzx83xPOfoPqresOI3x+aMuUjI8wnSWqF17G32dy5cztdhDFl/bpb\nzvXLuW6t6MRvQgxUenF6evpYvnwxfX19IywiSTu3UqkETWS2LXlJypghL0kZM+QlKWOGvCRlzJCX\npIwZ8pKUMUNekjJmyEtSxgx5ScqYIS9JGTPkJSljhrwkZcyQl6SMGfKSlDFDXpIyZshLUsYMeUnK\nmCEvSRkz5CUpY4a8JGXMkJekjBnykpQxQ16SMmbIS1LGDHlJypghL0kZM+QlKWOGvCRlzJCXpIwZ\n8pKUMUNekjJmyEtSxgx5ScqYIS9JGWsl5OcDDwD3AYuBXYEZwFJgNbAEmN5qASVJzWs25HuBvwZe\nAPQBk4C3ABcSIX8osCyNS5I6pNmQ3ww8CewBTE5/HwFOBBaleRYBJ7daQElS85oN+Q3AZ4BfE+H+\nO6IFPwvoT/P0p3FJUodMbnK5OcB7iG6bTcA3gTNq5hlIQx0LANi2rZ8VK1bQ19fXZDEkKU/lcply\nudzyekpNLvdm4NXAuWn87cAxwKuAVwKPAbOBW4HDapYdqGR/T08fy5cvNuQlaQSlUgmayOxmu2tW\nEaG+e9roccCDwHeAM9M8ZwLXN7l+SVIbNNtdcw/wFeBuYDvwY+A/gGnANcA5wFrgtNaLKElqVrMh\nD3BJGoo2EK16SdIE4B2vkpQxQ16SMmbIS1LGDHlJypghL0kZM+QlKWOGvCRlzJCXpIwZ8pKUMUNe\nkjJmyEtSxgx5ScqYIS9JGTPkJSljhrwkZcyQl6SMGfKSlDFDXpIyZshLUsYMeUnKmCEvSRkz5CUp\nY4a8JGXMkJekjBnykpQxQ16SMmbIS1LGDHlJypghL0kZM+QlKWOGvCRlzJCXpIwZ8pKUMUNekjJm\nyEtSxloJ+enAtcBPgQeBo4EZwFJgNbAkzSNJ6pBWQv4y4CbgcOB5wCrgQiLkDwWWpXFJUoc0G/J7\nAccCV6bxp4BNwInAovTcIuDklkonSWpJsyF/MPBb4Crgx8AVwNOAWUB/mqc/jUuSOqTZkJ8MvAD4\nYvr7B3bsmhlIgySpQyY3udy6NKxI49cC84HHgH3T39nA4/UXXwDAtm39rFixgr6+viaLIUl5KpfL\nlMvlltdTamHZ24BziStpFgB7pOfXAxcTLfvp1G3hRwO/p6eP5csXG/KSNIJSqQRNZHazLXmAdwNf\nA6YCa4CzgUnANcA5wFrgtBbWL0lqUSshfw9wVJ3nj2thnZKkNvKOV0nKmCEvSRkz5CUpY4a8JGXM\nkJekjBnykpQxQ16SMmbIS1LGDHlJypghL0kZM+QlKWOGvCRlzJCXpIwZ8pKUMUNekjJmyEtSxgx5\nScqYIS9JGTPkJSljhrwkZcyQl6SMGfKSlDFDXpIyZshLUsYMeUnKmCEvSRkz5CUpY4a8JGXMkJek\njBnykpQxQ16SMmbIS1LGDHlJypghL0kZM+QlKWOGvCRlrNWQnwSsBL6TxmcAS4HVwBJgeovrlyS1\noNWQPx94EBhI4xcSIX8osCyNS5I6pJWQPwCYB3wZKKXnTgQWpceLgJNbWL8kqUWthPzngPcB2wvP\nzQL60+P+NC5J6pBmQ/51wONEf3xpiHkGqHbjSJI6YHKTy72U6JqZB+wG9ABfJVrv+wKPAbOJHUEd\nCwDYtq2fFStW0NfX12QxJClP5XKZcrnc8nqGaoWPxiuAC4DXA5cA64GLiZOu09nx5OtApYHf09PH\n8uWLDXlJGkGpVIImMrtd18lXumU+BbyauITyVWlcktQhzXbXFH0vDQAbgOPasE5JUht4x6skZcyQ\nl6SMGfKSlDFDXpIyZshLUsYMeUnKmCEvSRkz5CUpY4a8JGXMkJekjBnykpQxQ16SMmbIS1LGDHlJ\nypghL0kZM+QlKWOGvCRlzJCXpIwZ8pKUMUNekjJmyEtSxgx5ScqYIS9JGTPkJSljhrwkZcyQl6SM\nGfKSlDFDXpIyZshLUsYMeUnKmCEvSRkz5CUpY4a8JGXMkJekjBnykpQxQ16SMtZsyB8I3Ao8ANwP\nnJeenwEsBVYDS4DprRZQktS8ZkP+SeAfgOcAxwB/DxwOXEiE/KHAsjQuSeqQZkP+MeAn6fHvgZ8C\n+wMnAovS84uAk1sqnSSpJe3ok+8FjgTuAmYB/en5/jQuSeqQyS0uvydwHXA+sKVm2kAa6lgAwLZt\n/axYsYK+vr4WiyFJeSmXy5TL5ZbXU2ph2SnAjcDNwKXpuVXAXKI7ZzZxcvawmuUGKtnf09PH8uWL\nDXlJGkGpVIImMrvZ7poS8J/Ag1QDHuAG4Mz0+Ezg+ibXL0lqg2a7a14GnAHcC6xMz80HPgVcA5wD\nrAVOa7F8kqQWNBvyyxn6KOC4JtcpSWoz73iVpIwZ8pKUMUNekjJmyEtSxgx5ScqYIS9JGTPkJSlj\nhrwkZcyQl6SMGfKSlDFDXpIyZshLUsYMeUnKmCEvSRkz5CUpY4a8JGXMkJekjBnykpQxQ16SMmbI\nS1LGDHlJypghL0kZM+QlKWOGvCRlzJCXpIwZ8pKUMUNekjJmyEtSxgx5ScqYIS9JGTPkJSljhrwk\nZcyQl6SMGfKSlDFDXpIyNhYh/xpgFfBz4ANjsH5JUoPaHfKTgMuJoD8COB04vM3bmNDK5XKnizCm\nrF93y7l+OdetFe0O+RcDDwFrgSeBrwMntXkbE1ruHzTr191yrl/OdWtFu0N+f+Dhwvi69Jwk7fR6\nemZQKpUolUr09MwYl21ObvP6BhqZqafn9QBs3bqWXXbx3K+kncOWLRupxOSWLaVx2Wa7t3IMsIDo\nkweYD2wHLi7M8xAwp83blaTcrQGe1elCTE4F6QWmAj9hJzvxKkm5OwH4GdFin9/hskiSJEkarUZu\nivp8mn4PcOQ4latdRqrf24h63QvcDjxv/IrWFo3e1HYU8BRwyngUqk0aqdtcYCVwP1Ael1K1z0j1\nmwncQnSn3g+cNW4la92VQD9w3zDzdHOujFS/CZMrk4juml5gCvX75ucBN6XHRwN3jlfh2qCR+r0E\n2Cs9fg351a8y3/8ANwJvHK/CtaiRuk0HHgAOSOMzx6twbdBI/RYAn0yPZwLraf+VdmPlWCK4hwrB\nbs4VGLl+o86Vsbp+sZGbok4EFqXHdxFfrFljVJ52a6R+dwCb0uO7qAZGN2j0prZ3A9cCvx23krWu\nkbq9FbiOuM8D4InxKlwbNFK/R4Ge9LiHCPmnxql8rfo+sHGY6d2cKzBy/UadK2MV8o3cFFVvnm4J\nwtHe9HUO1dZFN2j0/TsJ+FIab+geiQmgkbodAswAbgXuBt4+PkVri0bqdwXwHOAR4tD//PEp2rjo\n5lwZrYZyZawO0Rr9wtdep98tQTGacr4SeAfwsjEqy1hopH6XAhemeUu0/56LsdJI3aYALwD+EtiD\naD3dSfTzTnSN1O+fiG6cucQ9K0uBPwe2jF2xxlW35spoNJwrYxXyvwEOLIwfSPXQd6h5DkjPdYNG\n6gdxUuQKou9suEOwiaaR+r2Q6AqA6Nc9gegeuGHMS9eaRur2MNFFszUNtxEh2A0h30j9Xgp8PD1e\nA/wSeDZx1NLtujlXGjUhcqWRm6KKJ0iOobtOkDRSv2cSfaPHjGvJ2mO0N7VdRfdcXdNI3Q4Dvkuc\nxNyDOAl2xPgVsSWN1O+zwEXp8SxiJzA+P6TSHr00duK123Klopeh6zehcqXeTVHvSkPF5Wn6PcTh\ncTcZqX5fJk5orUzDD8e7gC1q5P2r6KaQh8bqdgFxhc19wHnjWrrWjVS/mcB3iO/dfcSJ5m5xNXEu\n4Y/EEdc7yCtXRqpft+eKJEmSJEmSJEmSJEmSctXIj6M16vnAD4gfiLsHOK3OPJ8nnxvUJGnCG+nH\nw0bjEKr/IW82cdlkT2H6i4CvAJvbsC1JUoN6GRzyc4CbibuFbyPuHG7GT6iGfuXXXffFlrwkjate\nBof8Mqr/d/XoND5aLwYeLIyfT/VH41oO+W75DWlJmmj2JH7f/ZuF56amv6cAH66zzDrijuSK2US3\nzF+l8f2AU4kfj+uWH/2TpGz0Um3J9xB96c3qAX7E4J8EmUf83v8v0/AnYHUL25AkjUIvg7trbida\n3hAt70b/Hd9UomtnpN/yt09eksZJ7Y+HnU2E/s3EidMHgA82uK4z0npWFoZ6OwivrpEkSZIkSZIk\nSZIkSZIkSZIkSZK0c/h/uZI0drHriJ8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108671510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEXCAYAAABI/TQXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a65a790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#Make histogram\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spam_probs = [] #list of spam probabilities\n",
    "ham_probs = [] #list of ham probabilities\n",
    "with open(os.path.join('./enroneEmailClassificationNoSmoothing', 'part-00000'), 'r') as myfile: #read file\n",
    "    lines = myfile.readlines() \n",
    "    for line in lines[:-3]: #exclude last 3 lines which have results\n",
    "        components = line.split('\\t')\n",
    "        if len(line) != 2: #I believe I have too many text components and a newline character is getting introduced and some whitespace\n",
    "            #This is an outstanding error I was not able to resolve but I can parse using the logic above\n",
    "            spam_probs.append(float(components[3]))\n",
    "            ham_probs.append(float(components[4]))\n",
    "\n",
    "s = plt.figure(1)\n",
    "plt.hist(spam_probs, bins = 100)\n",
    "plt.xlabel = \"Probability\"\n",
    "plt.ylabel = \"Frequency\"\n",
    "plt.title(\"Spam Exp Probabilities Histogram\")\n",
    "\n",
    "h = plt.figure(2)\n",
    "plt.hist(ham_probs, bins = 100)\n",
    "plt.xlabel = \"Probability\"\n",
    "plt.ylabel = \"Frequency\"\n",
    "plt.title(\"Ham Exp Probabilties Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.4 \n",
    "\n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #regex for string matching\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t') #split input file\n",
    "    text = \" \".join(components[-2:]).strip() #combine to produce subject and content together\n",
    "    words = re.findall(WORD_RE, text)\n",
    "    for word in words:\n",
    "        print components[0] + '\\t' + word + '\\t' + components[1] #print email ID, word, spam flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "emails = set() #hold email IDs\n",
    "words = {} #hold words and associated counts\n",
    "spam_emails = 0 #how many emails are marked as spam\n",
    "spam_word_count = 0 #count of words in spam\n",
    "ham_word_count = 0 #count of words in ham\n",
    "vocab = set() #set of unique words in all text\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    \n",
    "    ID = components[0] #parse components to appropriate variables\n",
    "    word = components[1]\n",
    "    spam = int(components[2])\n",
    "    \n",
    "    if word not in words.keys():\n",
    "        words[word] = {'spam_count': 0, 'ham_count': 0} #add word to dictionary if not there already\n",
    "        vocab.add(word) #add word to vocab\n",
    "    if ID not in emails:\n",
    "        emails.add(ID)\n",
    "        if spam == 1:\n",
    "            spam_emails += 1 #increment spam emails counter\n",
    "        \n",
    "\n",
    "    if spam == 1:\n",
    "        words[word]['spam_count'] += 1 #if email is spam, increment spam counter by 1\n",
    "        spam_word_count += 1\n",
    "    else:\n",
    "        words[word]['ham_count'] += 1 #repeat for ham\n",
    "        ham_word_count += 1\n",
    "\n",
    "\n",
    "prior_spam = float(spam_emails)/len(emails) #get prior probabilities\n",
    "prior_ham = 1-prior_spam\n",
    "\n",
    "for i, word in words.iteritems():\n",
    "    #This calculation uses a laplace smoother, +1 to numerator and + vocab in denominator\n",
    "    #See wikipedia entry\n",
    "    word['spam_like'] = float(word['spam_count'] + 1)/(spam_word_count + len(vocab)) #calculate conditional probs\n",
    "    word['ham_like'] = float(word['ham_count'] + 1)/(ham_word_count + len(vocab))\n",
    "    \n",
    "\n",
    "print prior_spam\n",
    "print prior_ham\n",
    "for word in words.keys():\n",
    "    #Word \"\\t\" spam likelihood '\\t' ham likelihood written to file\n",
    "    print word + '\\t' + str(words[word]['spam_like']) + '\\t' + str(words[word]['ham_like']) #print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/25 16:21:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/25 16:22:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:46:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#make directory\n",
    "#!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:22:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:22:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:22:19 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 16:22:19 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 16:22:19 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 16:22:19 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 16:22:19 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 16:22:20 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1234370154_0001\n",
      "16/01/25 16:22:20 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 16:22:20 INFO mapreduce.Job: Running job: job_local1234370154_0001\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 16:22:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: Starting task: attempt_local1234370154_0001_m_000000_0\n",
      "16/01/25 16:22:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:22:20 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 16:22:20 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 16:22:20 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 16:22:20 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:22:20 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:22:20 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/25 16:22:20 INFO streaming.PipeMapRed: R/W/S=100/19097/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:22:20 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 16:22:20 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: \n",
      "16/01/25 16:22:20 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: bufstart = 0; bufend = 1032108; bufvoid = 104857600\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26082748(104330992); length = 131649/6553600\n",
      "16/01/25 16:22:20 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 16:22:20 INFO mapred.Task: Task:attempt_local1234370154_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/25 16:22:20 INFO mapred.Task: Task 'attempt_local1234370154_0001_m_000000_0' done.\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: Finishing task: attempt_local1234370154_0001_m_000000_0\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: Starting task: attempt_local1234370154_0001_r_000000_0\n",
      "16/01/25 16:22:20 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:22:20 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 16:22:20 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 16:22:20 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@984ee41\n",
      "16/01/25 16:22:20 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 16:22:20 INFO reduce.EventFetcher: attempt_local1234370154_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 16:22:20 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1234370154_0001_m_000000_0 decomp: 1097936 len: 1097940 to MEMORY\n",
      "16/01/25 16:22:20 INFO reduce.InMemoryMapOutput: Read 1097936 bytes from map-output for attempt_local1234370154_0001_m_000000_0\n",
      "16/01/25 16:22:20 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1097936, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1097936\n",
      "16/01/25 16:22:20 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:22:20 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 16:22:20 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 16:22:20 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/25 16:22:20 INFO reduce.MergeManagerImpl: Merged 1 segments, 1097936 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 16:22:20 INFO reduce.MergeManagerImpl: Merging 1 files, 1097940 bytes from disk\n",
      "16/01/25 16:22:20 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 16:22:20 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 16:22:20 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/25 16:22:20 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:22:20 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 16:22:20 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 16:22:21 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:22:21 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:22:21 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:22:21 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:22:21 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:22:21 INFO mapreduce.Job: Job job_local1234370154_0001 running in uber mode : false\n",
      "16/01/25 16:22:21 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/25 16:22:23 INFO streaming.PipeMapRed: Records R/W=32913/1\n",
      "16/01/25 16:22:23 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 16:22:23 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 16:22:23 INFO mapred.Task: Task:attempt_local1234370154_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 16:22:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:22:23 INFO mapred.Task: Task attempt_local1234370154_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 16:22:23 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1234370154_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailCondProbLaplace/_temporary/0/task_local1234370154_0001_r_000000\n",
      "16/01/25 16:22:23 INFO mapred.LocalJobRunner: Records R/W=32913/1 > reduce\n",
      "16/01/25 16:22:23 INFO mapred.Task: Task 'attempt_local1234370154_0001_r_000000_0' done.\n",
      "16/01/25 16:22:23 INFO mapred.LocalJobRunner: Finishing task: attempt_local1234370154_0001_r_000000_0\n",
      "16/01/25 16:22:23 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 16:22:24 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 16:22:24 INFO mapreduce.Job: Job job_local1234370154_0001 completed successfully\n",
      "16/01/25 16:22:24 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2407996\n",
      "\t\tFILE: Number of bytes written=4095672\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=237609\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32913\n",
      "\t\tMap output bytes=1032108\n",
      "\t\tMap output materialized bytes=1097940\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=1097940\n",
      "\t\tReduce input records=32913\n",
      "\t\tReduce output records=5493\n",
      "\t\tSpilled Records=65826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=511180800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=237609\n",
      "16/01/25 16:22:24 INFO streaming.StreamJob: Output directory: enroneEmailCondProbLaplace\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailCondProbLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check output\n",
    "#!hdfs dfs -cat /user/dunmireg/enroneEmailCondProbLaplace/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:22:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:22:28 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/25 16:22:28 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailCondProbLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 16:22:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:22:31 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 16:22:32 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:22:33 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailCondProbLaplace\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 16:22:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 16:23:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailCondProbLaplace\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#mapper for classification. This is essentially the same procedure as in 2.3. \n",
    "import sys\n",
    "import re\n",
    "import os \n",
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "priorSpam = 0 #hold priors\n",
    "priorHam = 0\n",
    "words = {}\n",
    "\n",
    "with open(os.path.join('./enroneEmailCondProbLaplace', 'part-00000'), 'r') as myfile: #read file\n",
    "    lines = myfile.readlines()\n",
    "    priorSpam = float(lines[0]) #parse first lines for priors\n",
    "    priorHam = float(lines[1])\n",
    "    for line in lines[2:]:\n",
    "        components = line.split('\\t')\n",
    "        #add conditional probabilities to words dictionary\n",
    "        words[components[0]] = {'spam_like': float(components[1]), 'ham_like': float(components[2])}\n",
    "        \n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip()\n",
    "    text = re.findall(WORD_RE, text)\n",
    "    \n",
    "    spamScore = log(priorSpam)\n",
    "    hamScore = log(priorHam)\n",
    "    for word in text:\n",
    "        if word in words.keys():\n",
    "            #add conditional probabilities to scores\n",
    "            spamScore += log(float(words[word]['spam_like']))\n",
    "            hamScore += log(float(words[word]['ham_like']))\n",
    "        pred = 0 #assign prediction\n",
    "    if spamScore > hamScore:\n",
    "        pred = 1\n",
    "    #output ID, true classification, prediction, and conditional probabilities\n",
    "    print components[0] + '\\t' + components[1] + '\\t' + str(pred) + '\\t' + str(exp(spamScore)) + '\\t' + str(exp(hamScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "misclassified = 0 #count of how many emails are misclassified\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t') #split components\n",
    "    if int(components[1]) != int(components[2]): #if true classification and prediction don't agree, increment\n",
    "            misclassified += 1\n",
    "    print line\n",
    "#print final output\n",
    "print \"Misclassified: \" + str(misclassified) + \" which means this has an accuracy of \" + str(100-misclassified) + \"%\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check code\n",
    "#!cat enronemail_1h.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/25 16:23:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/25 16:23:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:47:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#make directory\n",
    "#!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:24:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:24:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:24:23 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 16:24:23 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 16:24:23 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 16:24:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 16:24:23 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 16:24:23 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local706008687_0001\n",
      "16/01/25 16:24:24 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 16:24:24 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 16:24:24 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 16:24:24 INFO mapreduce.Job: Running job: job_local706008687_0001\n",
      "16/01/25 16:24:24 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:24:24 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 16:24:24 INFO mapred.LocalJobRunner: Starting task: attempt_local706008687_0001_m_000000_0\n",
      "16/01/25 16:24:24 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:24:24 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 16:24:24 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 16:24:24 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/25 16:24:24 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 16:24:24 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 16:24:24 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 16:24:24 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 16:24:24 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 16:24:24 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 16:24:24 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 16:24:24 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 16:24:24 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 16:24:24 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:24:24 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:24:25 INFO mapreduce.Job: Job job_local706008687_0001 running in uber mode : false\n",
      "16/01/25 16:24:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/25 16:24:25 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:100=100/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: \n",
      "16/01/25 16:24:28 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 16:24:28 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 16:24:28 INFO mapred.MapTask: bufstart = 0; bufend = 4284; bufvoid = 104857600\n",
      "16/01/25 16:24:28 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "16/01/25 16:24:28 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 16:24:28 INFO mapred.Task: Task:attempt_local706008687_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "16/01/25 16:24:28 INFO mapred.Task: Task 'attempt_local706008687_0001_m_000000_0' done.\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: Finishing task: attempt_local706008687_0001_m_000000_0\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: Starting task: attempt_local706008687_0001_r_000000_0\n",
      "16/01/25 16:24:28 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:24:28 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 16:24:28 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 16:24:28 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1b5683c\n",
      "16/01/25 16:24:28 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 16:24:28 INFO reduce.EventFetcher: attempt_local706008687_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 16:24:28 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local706008687_0001_m_000000_0 decomp: 4486 len: 4490 to MEMORY\n",
      "16/01/25 16:24:28 INFO reduce.InMemoryMapOutput: Read 4486 bytes from map-output for attempt_local706008687_0001_m_000000_0\n",
      "16/01/25 16:24:28 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4486, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4486\n",
      "16/01/25 16:24:28 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:24:28 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 16:24:28 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 16:24:28 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4461 bytes\n",
      "16/01/25 16:24:28 INFO reduce.MergeManagerImpl: Merged 1 segments, 4486 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 16:24:28 INFO reduce.MergeManagerImpl: Merging 1 files, 4490 bytes from disk\n",
      "16/01/25 16:24:28 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 16:24:28 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 16:24:28 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4461 bytes\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/25 16:24:28 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 16:24:28 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 16:24:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 16:24:28 INFO mapred.Task: Task:attempt_local706008687_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:24:28 INFO mapred.Task: Task attempt_local706008687_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 16:24:28 INFO output.FileOutputCommitter: Saved output of task 'attempt_local706008687_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailClassLaplace/_temporary/0/task_local706008687_0001_r_000000\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "16/01/25 16:24:28 INFO mapred.Task: Task 'attempt_local706008687_0001_r_000000_0' done.\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: Finishing task: attempt_local706008687_0001_r_000000_0\n",
      "16/01/25 16:24:28 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 16:24:29 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 16:24:29 INFO mapreduce.Job: Job job_local706008687_0001 completed successfully\n",
      "16/01/25 16:24:29 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=221096\n",
      "\t\tFILE: Number of bytes written=812302\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=4543\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=4284\n",
      "\t\tMap output materialized bytes=4490\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=4490\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=201\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=620756992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4543\n",
      "16/01/25 16:24:29 INFO streaming.StreamJob: Output directory: enroneEmailClassLaplace\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailClassLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check output\n",
    "#!hdfs dfs -cat /user/dunmireg/enroneEmailClassLaplace/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:24:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:24:34 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/25 16:24:34 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailClassLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 16:24:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:24:36 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 16:24:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:24:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailClassLaplace\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 16:24:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 16:25:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailClassLaplace\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified: 0 which means this has an accuracy of 100%\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display outputs from results file\n",
    "import os\n",
    "\n",
    "with open(os.path.join('./enroneEmailClassLaplace', 'part-00000'), 'r') as myfile:\n",
    "    lines = myfile.readlines() #read file\n",
    "    lines = lines[-1:] #get last 3 lines\n",
    "    for line in lines:\n",
    "        print line #print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEXCAYAAABI/TQXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10665b9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEXCAYAAABI/TQXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJkjRZ/T+gktUaGqVigQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1066c13d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#Make histogram\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spam_probs = [] #list of spam probabilities\n",
    "ham_probs = [] #list of ham probabilities\n",
    "with open(os.path.join('./enroneEmailClassLaplace', 'part-00000'), 'r') as myfile: #read file\n",
    "    lines = myfile.readlines() \n",
    "    for line in lines[:-1]: #exclude last 3 lines which have results\n",
    "        components = line.split('\\t')\n",
    "        if len(line) != 2: #I believe I have too many text components and a newline character is getting introduced and some whitespace\n",
    "            #This is an outstanding error I was not able to resolve but I can parse using the logic above\n",
    "            spam_probs.append(float(components[3]))\n",
    "            ham_probs.append(float(components[4]))\n",
    "\n",
    "s = plt.figure(1)\n",
    "plt.hist(spam_probs, bins = 100)\n",
    "plt.xlabel = \"Probability\"\n",
    "plt.ylabel = \"Frequency\"\n",
    "plt.title(\"Spam Exp Probabilities w/ Laplace Histogram\")\n",
    "\n",
    "h = plt.figure(2)\n",
    "plt.hist(ham_probs, bins = 100)\n",
    "plt.xlabel = \"Probability\"\n",
    "plt.ylabel = \"Frequency\"\n",
    "plt.title(\"Ham Exp Probabilties w/ Laplace Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.5. \n",
    "\n",
    "Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #regex for word classification\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t') #split inpput\n",
    "    text = \" \".join(components[-2:]).strip() #combine subject and content into text field\n",
    "    words = re.findall(WORD_RE, text)\n",
    "    for word in words:\n",
    "        print components[0] + '\\t' + word + '\\t' + components[1] #print ID, word, spam flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "emails = set() #hold email IDs\n",
    "words = {} #hold words and associated counts\n",
    "spam_emails = 0 #how many emails are marked as spam\n",
    "spam_word_count = 0 #how many words in spam\n",
    "ham_word_count = 0 #how many words in ham\n",
    "vocab = set() #unique words in all text\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t') #split input\n",
    "    \n",
    "    ID = components[0] #put input into appropriate variables\n",
    "    word = components[1]\n",
    "    spam = int(components[2])\n",
    "    \n",
    "    if word not in words.keys(): #add word to words dictionary, give it spam and ham counts\n",
    "        words[word] = {'spam_count': 0, 'ham_count': 0}\n",
    "        vocab.add(word)\n",
    "    if ID not in emails: \n",
    "        emails.add(ID)\n",
    "        if spam == 1: #increment spam counter\n",
    "            spam_emails += 1\n",
    "        \n",
    "\n",
    "    if spam == 1: #if email is spam, increment the spam counter, otherwise increment ham counter\n",
    "        words[word]['spam_count'] += 1\n",
    "        spam_word_count += 1\n",
    "    else:\n",
    "        words[word]['ham_count'] += 1\n",
    "        ham_word_count += 1\n",
    "\n",
    "\n",
    "prior_spam = float(spam_emails)/len(emails) #get prior probabilities\n",
    "prior_ham = 1-prior_spam\n",
    "\n",
    "for word in words.keys(): #remove words that have less than 3 counts from dictionary\n",
    "    if words[word]['spam_count'] + words[word]['ham_count'] < 3:\n",
    "        del words[word]\n",
    "\n",
    "for i, word in words.iteritems(): #use laplace smoother to get conditional probabilities\n",
    "    word['spam_like'] = float(word['spam_count'] + 1)/(spam_word_count + len(vocab))\n",
    "    word['ham_like'] = float(word['ham_count'] + 1)/(ham_word_count + len(vocab))\n",
    "    \n",
    "\n",
    "print prior_spam #output ham and spam priors\n",
    "print prior_ham\n",
    "for word in words.keys():\n",
    "    #Word \"\\t\" spam likelihood '\\t' ham likelihood written to file\n",
    "    print word + '\\t' + str(words[word]['spam_like']) + '\\t' + str(words[word]['ham_like']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/25 16:41:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/25 16:42:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:42:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:42:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:42:15 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 16:42:15 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 16:42:15 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 16:42:15 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 16:42:15 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 16:42:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local393621611_0001\n",
      "16/01/25 16:42:15 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 16:42:15 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 16:42:15 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 16:42:15 INFO mapreduce.Job: Running job: job_local393621611_0001\n",
      "16/01/25 16:42:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:42:15 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 16:42:15 INFO mapred.LocalJobRunner: Starting task: attempt_local393621611_0001_m_000000_0\n",
      "16/01/25 16:42:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:42:15 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 16:42:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 16:42:15 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/25 16:42:15 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: R/W/S=100/5284/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 16:42:16 INFO mapred.LocalJobRunner: \n",
      "16/01/25 16:42:16 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: bufstart = 0; bufend = 1032108; bufvoid = 104857600\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26082748(104330992); length = 131649/6553600\n",
      "16/01/25 16:42:16 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 16:42:16 INFO mapred.Task: Task:attempt_local393621611_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 16:42:16 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/25 16:42:16 INFO mapred.Task: Task 'attempt_local393621611_0001_m_000000_0' done.\n",
      "16/01/25 16:42:16 INFO mapred.LocalJobRunner: Finishing task: attempt_local393621611_0001_m_000000_0\n",
      "16/01/25 16:42:16 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 16:42:16 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 16:42:16 INFO mapred.LocalJobRunner: Starting task: attempt_local393621611_0001_r_000000_0\n",
      "16/01/25 16:42:16 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:42:16 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 16:42:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 16:42:16 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4a12a357\n",
      "16/01/25 16:42:16 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 16:42:16 INFO reduce.EventFetcher: attempt_local393621611_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 16:42:16 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local393621611_0001_m_000000_0 decomp: 1097936 len: 1097940 to MEMORY\n",
      "16/01/25 16:42:16 INFO reduce.InMemoryMapOutput: Read 1097936 bytes from map-output for attempt_local393621611_0001_m_000000_0\n",
      "16/01/25 16:42:16 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1097936, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1097936\n",
      "16/01/25 16:42:16 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 16:42:16 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:42:16 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 16:42:16 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 16:42:16 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/25 16:42:16 INFO reduce.MergeManagerImpl: Merged 1 segments, 1097936 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 16:42:16 INFO reduce.MergeManagerImpl: Merging 1 files, 1097940 bytes from disk\n",
      "16/01/25 16:42:16 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 16:42:16 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 16:42:16 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/25 16:42:16 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 16:42:16 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:42:16 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:42:16 INFO mapreduce.Job: Job job_local393621611_0001 running in uber mode : false\n",
      "16/01/25 16:42:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/25 16:42:18 INFO streaming.PipeMapRed: Records R/W=32913/1\n",
      "16/01/25 16:42:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 16:42:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 16:42:19 INFO mapred.Task: Task:attempt_local393621611_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 16:42:19 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:42:19 INFO mapred.Task: Task attempt_local393621611_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 16:42:19 INFO output.FileOutputCommitter: Saved output of task 'attempt_local393621611_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailCondProb3/_temporary/0/task_local393621611_0001_r_000000\n",
      "16/01/25 16:42:19 INFO mapred.LocalJobRunner: Records R/W=32913/1 > reduce\n",
      "16/01/25 16:42:19 INFO mapred.Task: Task 'attempt_local393621611_0001_r_000000_0' done.\n",
      "16/01/25 16:42:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local393621611_0001_r_000000_0\n",
      "16/01/25 16:42:19 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 16:42:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 16:42:19 INFO mapreduce.Job: Job job_local393621611_0001 completed successfully\n",
      "16/01/25 16:42:19 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2407996\n",
      "\t\tFILE: Number of bytes written=4092640\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=79886\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32913\n",
      "\t\tMap output bytes=1032108\n",
      "\t\tMap output materialized bytes=1097940\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=1097940\n",
      "\t\tReduce input records=32913\n",
      "\t\tReduce output records=1883\n",
      "\t\tSpilled Records=65826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=510656512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=79886\n",
      "16/01/25 16:42:19 INFO streaming.StreamJob: Output directory: enroneEmailCondProb3\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailCondProb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:42:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:42:25 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/25 16:42:25 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailCondProb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 16:42:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:42:28 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 16:42:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:42:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailCondProb3\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 16:42:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 16:43:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailCondProb3\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#I have placed the mapper here but have not modified it in any way from the previous mapper. It will still\n",
    "#produce ID + \\t + word + \\t + true spam flag to send to the reducer. \n",
    "import sys\n",
    "import re\n",
    "import os \n",
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "priorSpam = 0 #priors\n",
    "priorHam = 0\n",
    "words = {}\n",
    "\n",
    "with open(os.path.join('./enroneEmailCondProbLaplace', 'part-00000'), 'r') as myfile: #read input file\n",
    "    lines = myfile.readlines() #parse lines\n",
    "    priorSpam = float(lines[0]) #get priors\n",
    "    priorHam = float(lines[1])\n",
    "    for line in lines[2:]:\n",
    "        components = line.split('\\t') #split remaining lines and add word and likelihoods to dictionary\n",
    "        words[components[0]] = {'spam_like': float(components[1]), 'ham_like': float(components[2])}\n",
    "        \n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in sys.stdin: #read input\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip() #get subject and content together as text\n",
    "    text = re.findall(WORD_RE, text)\n",
    "    \n",
    "    spamScore = log(priorSpam) #get priors\n",
    "    hamScore = log(priorHam)\n",
    "    for word in text:\n",
    "        if word in words.keys(): #increment scores based on word conditional probabilities\n",
    "            spamScore += log(float(words[word]['spam_like']))\n",
    "            hamScore += log(float(words[word]['ham_like']))\n",
    "        pred = 0 #predicted class\n",
    "    if spamScore > hamScore:\n",
    "        pred = 1\n",
    "    #output ID, spam flag, predicted class, and exponentiated conditional probabilities for document\n",
    "    print components[0] + '\\t' + components[1] + '\\t' + str(pred) + '\\t' + str(exp(spamScore)) + '\\t' + str(exp(hamScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "misclassified = 0 #keep track of how many are misclassified\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    if int(components[1]) != int(components[2]):\n",
    "            misclassified += 1 #if predicted and true flag disagree increment counter\n",
    "    print line\n",
    "print \"Misclassified: \" + str(misclassified) + \" which means this has an accuracy of \" + str(100-misclassified) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check code\n",
    "#!cat enronemail_1h.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/25 16:49:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/25 16:50:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:50:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:50:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:50:23 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/25 16:50:23 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/25 16:50:23 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/25 16:50:23 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/25 16:50:24 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/25 16:50:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local41400190_0001\n",
      "16/01/25 16:50:24 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/25 16:50:24 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/25 16:50:24 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/25 16:50:24 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:50:24 INFO mapreduce.Job: Running job: job_local41400190_0001\n",
      "16/01/25 16:50:24 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/25 16:50:24 INFO mapred.LocalJobRunner: Starting task: attempt_local41400190_0001_m_000000_0\n",
      "16/01/25 16:50:24 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:50:24 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 16:50:24 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 16:50:24 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/25 16:50:24 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/25 16:50:24 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/25 16:50:24 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/25 16:50:24 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/25 16:50:24 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/25 16:50:24 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/25 16:50:24 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/25 16:50:24 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/25 16:50:24 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/25 16:50:24 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:50:24 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:50:25 INFO mapreduce.Job: Job job_local41400190_0001 running in uber mode : false\n",
      "16/01/25 16:50:25 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/25 16:50:25 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:100=100/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/25 16:50:28 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 16:50:28 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/25 16:50:28 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 16:50:28 INFO mapred.LocalJobRunner: \n",
      "16/01/25 16:50:28 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/25 16:50:28 INFO mapred.MapTask: Spilling map output\n",
      "16/01/25 16:50:28 INFO mapred.MapTask: bufstart = 0; bufend = 4284; bufvoid = 104857600\n",
      "16/01/25 16:50:28 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "16/01/25 16:50:28 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/25 16:50:29 INFO mapred.Task: Task:attempt_local41400190_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "16/01/25 16:50:29 INFO mapred.Task: Task 'attempt_local41400190_0001_m_000000_0' done.\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local41400190_0001_m_000000_0\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: Starting task: attempt_local41400190_0001_r_000000_0\n",
      "16/01/25 16:50:29 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/25 16:50:29 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/25 16:50:29 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/25 16:50:29 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2551bf57\n",
      "16/01/25 16:50:29 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/25 16:50:29 INFO reduce.EventFetcher: attempt_local41400190_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/25 16:50:29 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local41400190_0001_m_000000_0 decomp: 4486 len: 4490 to MEMORY\n",
      "16/01/25 16:50:29 INFO reduce.InMemoryMapOutput: Read 4486 bytes from map-output for attempt_local41400190_0001_m_000000_0\n",
      "16/01/25 16:50:29 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4486, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4486\n",
      "16/01/25 16:50:29 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:50:29 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/25 16:50:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 16:50:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4461 bytes\n",
      "16/01/25 16:50:29 INFO reduce.MergeManagerImpl: Merged 1 segments, 4486 bytes to disk to satisfy reduce memory limit\n",
      "16/01/25 16:50:29 INFO reduce.MergeManagerImpl: Merging 1 files, 4490 bytes from disk\n",
      "16/01/25 16:50:29 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/25 16:50:29 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/25 16:50:29 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4461 bytes\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:50:29 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/25 16:50:29 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/25 16:50:29 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/25 16:50:29 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:50:29 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:50:29 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/25 16:50:29 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/25 16:50:29 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/25 16:50:29 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/25 16:50:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/25 16:50:29 INFO mapred.Task: Task:attempt_local41400190_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/25 16:50:29 INFO mapred.Task: Task attempt_local41400190_0001_r_000000_0 is allowed to commit now\n",
      "16/01/25 16:50:29 INFO output.FileOutputCommitter: Saved output of task 'attempt_local41400190_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailClass3/_temporary/0/task_local41400190_0001_r_000000\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "16/01/25 16:50:29 INFO mapred.Task: Task 'attempt_local41400190_0001_r_000000_0' done.\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: Finishing task: attempt_local41400190_0001_r_000000_0\n",
      "16/01/25 16:50:29 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/25 16:50:30 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/25 16:50:30 INFO mapreduce.Job: Job job_local41400190_0001 completed successfully\n",
      "16/01/25 16:50:30 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=221096\n",
      "\t\tFILE: Number of bytes written=809270\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=4543\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=4284\n",
      "\t\tMap output materialized bytes=4490\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=4490\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=201\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=9\n",
      "\t\tTotal committed heap usage (bytes)=510132224\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4543\n",
      "16/01/25 16:50:30 INFO streaming.StreamJob: Output directory: enroneEmailClass3\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailClass3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 16:50:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:50:36 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/25 16:50:36 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailClass3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 16:50:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:50:38 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/25 16:50:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 16:50:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailClass3\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/25 16:50:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/25 16:51:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailClass3\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified: 0 which means this has an accuracy of 100%\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display outputs from results file\n",
    "import os\n",
    "\n",
    "with open(os.path.join('./enroneEmailClass3', 'part-00000'), 'r') as myfile:\n",
    "    lines = myfile.readlines() #read file\n",
    "    lines = lines[-1:] #get last 3 lines\n",
    "    for line in lines:\n",
    "        print line #print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEXCAYAAABI/TQXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJUmSJEmSJEmSJEmSJEmSJEmSJOlp5P8DU5vNRcgcujIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10db6f710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEXCAYAAABI/TQXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIeVcUncAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10db7a750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#Make histogram\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spam_probs = [] #list of spam probabilities\n",
    "ham_probs = [] #list of ham probabilities\n",
    "with open(os.path.join('./enroneEmailClass3', 'part-00000'), 'r') as myfile: #read file\n",
    "    lines = myfile.readlines() \n",
    "    for line in lines[:-1]: #exclude last line which has results\n",
    "        components = line.split('\\t')\n",
    "        if len(line) != 2: #I believe I have too many text components and a newline character is getting introduced and some whitespace\n",
    "            #This is an outstanding error I was not able to resolve but I can parse using the logic above\n",
    "            spam_probs.append(float(components[3]))\n",
    "            ham_probs.append(float(components[4]))\n",
    "\n",
    "s = plt.figure(1)\n",
    "plt.hist(spam_probs, bins = 100)\n",
    "plt.xlabel = \"Probability\"\n",
    "plt.ylabel = \"Frequency\"\n",
    "plt.title(\"Spam Exp Probabilities w/ Laplace Exclude >3 Histogram\")\n",
    "\n",
    "h = plt.figure(2)\n",
    "plt.hist(ham_probs, bins = 100)\n",
    "plt.xlabel = \"Probability\"\n",
    "plt.ylabel = \"Frequency\"\n",
    "plt.title(\"Ham Exp Probabilties w/ Laplace Exclude >3 Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
