{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Homework 261 Spring 2016 - Glenn Dunmire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.0:\n",
    "\n",
    "What is a race condition in the context of parallel computation? Give an example.\n",
    "What is MapReduce?\n",
    "How does it differ from Hadoop?\n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: \n",
    "\n",
    "Given as input: Records of the form '<'integer, “NA”>, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form <'integer, “NA”> in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form <'integer, “NA”>. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Write a text file of form <integer, \"NA\">. \n",
    "#use the random package to get random numbers\n",
    "import random\n",
    "\n",
    "N = 1000 #set size of list of numbers\n",
    "\n",
    "#I chose to include a larger range than my total list size so that I would not simply have a list from 1 to 10000\n",
    "#However, this is easily adjusted in the range parameter\n",
    "numbers = random.sample(range(0, 1000), N) #list of numbers at random\n",
    "output = [] #store output\n",
    "for number in numbers:\n",
    "    output.append('<' + str(number) + ', ' + 'NA>') #properly format strings\n",
    "\n",
    "with open('integer.txt', 'w') as myfile: #write output to a text file\n",
    "    myfile.write(\"\\n\".join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper_sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    line = line[1:] #remove beginning '<' \n",
    "    num = line.split()[0] #split on whitespace and only keep number\n",
    "    num = num[:-1] #remove trailing comma\n",
    "    print '%s\\t%s' % (num, 'NA') #print result to STDOUT for input to reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer_sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    num, na = line.split('\\t')\n",
    "    \n",
    "    print '<' + num + ', NA>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test mapper and reducer\n",
    "#!cat integer.txt | python mapper.py | sort -n | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/23 13:23:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/23 13:23:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:24:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#make directory\n",
    "!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:24:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put integer.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 13:24:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:24:10 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 13:24:10 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 13:24:10 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 13:24:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 13:24:10 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 13:24:10 INFO Configuration.deprecation: mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "16/01/23 13:24:10 INFO Configuration.deprecation: mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "16/01/23 13:24:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1561200986_0001\n",
      "16/01/23 13:24:11 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 13:24:11 INFO mapreduce.Job: Running job: job_local1561200986_0001\n",
      "16/01/23 13:24:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: Starting task: attempt_local1561200986_0001_m_000000_0\n",
      "16/01/23 13:24:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 13:24:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 13:24:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/integer.txt:0+9889\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: Records R/W=1000/1\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: \n",
      "16/01/23 13:24:11 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: bufstart = 0; bufend = 6890; bufvoid = 104857600\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26210400(104841600); length = 3997/6553600\n",
      "16/01/23 13:24:11 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 13:24:11 INFO mapred.Task: Task:attempt_local1561200986_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: Records R/W=1000/1\n",
      "16/01/23 13:24:11 INFO mapred.Task: Task 'attempt_local1561200986_0001_m_000000_0' done.\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local1561200986_0001_m_000000_0\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: Starting task: attempt_local1561200986_0001_r_000000_0\n",
      "16/01/23 13:24:11 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 13:24:11 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 13:24:11 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 13:24:11 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@51fa3a0a\n",
      "16/01/23 13:24:11 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 13:24:11 INFO reduce.EventFetcher: attempt_local1561200986_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 13:24:11 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1561200986_0001_m_000000_0 decomp: 8892 len: 8896 to MEMORY\n",
      "16/01/23 13:24:11 INFO reduce.InMemoryMapOutput: Read 8892 bytes from map-output for attempt_local1561200986_0001_m_000000_0\n",
      "16/01/23 13:24:11 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 8892, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8892\n",
      "16/01/23 13:24:11 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 13:24:11 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 13:24:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 13:24:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8888 bytes\n",
      "16/01/23 13:24:11 INFO reduce.MergeManagerImpl: Merged 1 segments, 8892 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 13:24:11 INFO reduce.MergeManagerImpl: Merging 1 files, 8896 bytes from disk\n",
      "16/01/23 13:24:11 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 13:24:11 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 13:24:11 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 8888 bytes\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 13:24:11 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: Records R/W=1000/1\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 13:24:11 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 13:24:11 INFO mapred.Task: Task:attempt_local1561200986_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 13:24:11 INFO mapred.Task: Task attempt_local1561200986_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 13:24:11 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1561200986_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/integerOutput/_temporary/0/task_local1561200986_0001_r_000000\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: Records R/W=1000/1 > reduce\n",
      "16/01/23 13:24:11 INFO mapred.Task: Task 'attempt_local1561200986_0001_r_000000_0' done.\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: Finishing task: attempt_local1561200986_0001_r_000000_0\n",
      "16/01/23 13:24:11 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 13:24:12 INFO mapreduce.Job: Job job_local1561200986_0001 running in uber mode : false\n",
      "16/01/23 13:24:12 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 13:24:12 INFO mapreduce.Job: Job job_local1561200986_0001 completed successfully\n",
      "16/01/23 13:24:12 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=229894\n",
      "\t\tFILE: Number of bytes written=830582\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=19778\n",
      "\t\tHDFS: Number of bytes written=10890\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1000\n",
      "\t\tMap output records=1000\n",
      "\t\tMap output bytes=6890\n",
      "\t\tMap output materialized bytes=8896\n",
      "\t\tInput split bytes=99\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1000\n",
      "\t\tReduce shuffle bytes=8896\n",
      "\t\tReduce input records=1000\n",
      "\t\tReduce output records=1000\n",
      "\t\tSpilled Records=2000\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=510656512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=9889\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=10890\n",
      "16/01/23 13:24:12 INFO streaming.StreamJob: Output directory: integerOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D  mapred.text.key.comparator.options=-n \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input integer.txt \\\n",
    "-output integerOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/integerOutput/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/23 13:24:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:24:32 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/integer.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/23 13:24:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 13:24:34 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/integerOutput\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/23 13:24:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/23 13:25:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/integer.txt #check\n",
    "!hadoop fs -rmr /user/dunmireg/integerOutput\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 2.2\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results.\n",
    "\n",
    " \n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l    \n",
    "       8    \n",
    "       #NOTE  \"assistance\" occurs on 8 lines but how many times does the token occur? 10 times! This is the number we are looking for!\n",
    "\n",
    "__HW2.2.1__  Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#Credit to the test notebook provided by Professor Shanahan for structure of mapper and reducer\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #regex for matching to word\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    #split line into subcomponents by tab, ID + spam flag + subject + content\n",
    "    components = line.split('\\t')\n",
    "    text = components[2] + components[3] #combine subject and content\n",
    "    # increase counters\n",
    "    for word in WORD_RE.findall(text):\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        print '%s\\t%s' % (word, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_sort.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    word, count = line.split('\\t') #split into components of word, count\n",
    "    print '%s\\t%s' % (count, word) #print result to STDOUT for input to reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#Credit to Professor Shanahan's example code for inspiration for this reducer\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_word, current_count)\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print '%s\\t%s' % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_sort.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    count, word = line.split('\\t')\n",
    "    \n",
    "    print word + ', ' + count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test mapper and reducer\n",
    "#!cat enronemail_1h.txt | python mapper.py | sort -k1,1 | python reducer.py | python mapper_sort.py | sort -n -k1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/23 15:18:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/23 15:18:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:18:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 15:19:35 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 15:19:35 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/23 15:19:35 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/23 15:19:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/23 15:19:36 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/23 15:19:36 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/23 15:19:36 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1865327339_0001\n",
      "16/01/23 15:19:36 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/23 15:19:36 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/23 15:19:36 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/23 15:19:36 INFO mapreduce.Job: Running job: job_local1865327339_0001\n",
      "16/01/23 15:19:36 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 15:19:36 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/23 15:19:36 INFO mapred.LocalJobRunner: Starting task: attempt_local1865327339_0001_m_000000_0\n",
      "16/01/23 15:19:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 15:19:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 15:19:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: R/W/S=100/3338/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 15:19:37 INFO mapred.LocalJobRunner: \n",
      "16/01/23 15:19:37 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: Spilling map output\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: bufstart = 0; bufend = 252205; bufvoid = 104857600\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26082756(104331024); length = 131641/6553600\n",
      "16/01/23 15:19:37 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/23 15:19:37 INFO mapred.Task: Task:attempt_local1865327339_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/23 15:19:37 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/23 15:19:37 INFO mapred.Task: Task 'attempt_local1865327339_0001_m_000000_0' done.\n",
      "16/01/23 15:19:37 INFO mapred.LocalJobRunner: Finishing task: attempt_local1865327339_0001_m_000000_0\n",
      "16/01/23 15:19:37 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/23 15:19:37 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/23 15:19:37 INFO mapred.LocalJobRunner: Starting task: attempt_local1865327339_0001_r_000000_0\n",
      "16/01/23 15:19:37 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/23 15:19:37 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/23 15:19:37 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/23 15:19:37 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@d7d5f7\n",
      "16/01/23 15:19:37 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/23 15:19:37 INFO reduce.EventFetcher: attempt_local1865327339_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/23 15:19:37 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1865327339_0001_m_000000_0 decomp: 318029 len: 318033 to MEMORY\n",
      "16/01/23 15:19:37 INFO reduce.InMemoryMapOutput: Read 318029 bytes from map-output for attempt_local1865327339_0001_m_000000_0\n",
      "16/01/23 15:19:37 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 318029, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->318029\n",
      "16/01/23 15:19:37 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/23 15:19:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 15:19:37 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/23 15:19:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 15:19:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318025 bytes\n",
      "16/01/23 15:19:37 INFO reduce.MergeManagerImpl: Merged 1 segments, 318029 bytes to disk to satisfy reduce memory limit\n",
      "16/01/23 15:19:37 INFO reduce.MergeManagerImpl: Merging 1 files, 318033 bytes from disk\n",
      "16/01/23 15:19:37 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/23 15:19:37 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/23 15:19:37 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 318025 bytes\n",
      "16/01/23 15:19:37 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/23 15:19:37 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: Records R/W=20019/1\n",
      "16/01/23 15:19:37 INFO mapreduce.Job: Job job_local1865327339_0001 running in uber mode : false\n",
      "16/01/23 15:19:37 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/23 15:19:37 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/23 15:19:38 INFO mapred.Task: Task:attempt_local1865327339_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/23 15:19:38 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/23 15:19:38 INFO mapred.Task: Task attempt_local1865327339_0001_r_000000_0 is allowed to commit now\n",
      "16/01/23 15:19:38 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1865327339_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailWordCountOutput/_temporary/0/task_local1865327339_0001_r_000000\n",
      "16/01/23 15:19:38 INFO mapred.LocalJobRunner: Records R/W=20019/1 > reduce\n",
      "16/01/23 15:19:38 INFO mapred.Task: Task 'attempt_local1865327339_0001_r_000000_0' done.\n",
      "16/01/23 15:19:38 INFO mapred.LocalJobRunner: Finishing task: attempt_local1865327339_0001_r_000000_0\n",
      "16/01/23 15:19:38 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/23 15:19:38 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/23 15:19:38 INFO mapreduce.Job: Job job_local1865327339_0001 completed successfully\n",
      "16/01/23 15:19:38 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=848182\n",
      "\t\tFILE: Number of bytes written=1755951\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=53511\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32911\n",
      "\t\tMap output bytes=252205\n",
      "\t\tMap output materialized bytes=318033\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=5493\n",
      "\t\tReduce shuffle bytes=318033\n",
      "\t\tReduce input records=32911\n",
      "\t\tReduce output records=5493\n",
      "\t\tSpilled Records=65822\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=511180800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=53511\n",
      "16/01/23 15:19:38 INFO streaming.StreamJob: Output directory: enroneEmailWordCountOutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailWordCountOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/enroneEmailWordCountOutput/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/23 15:25:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/23 15:25:22 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailWordCountOutput\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/23 15:25:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/23 15:25:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt #check\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailWordCountOutput\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see. \n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip()\n",
    "    words = re.findall(WORD_RE, text)\n",
    "    for word in words:\n",
    "        print components[0] + '\\t' + word + '\\t' + components[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "emails = set() #hold email IDs\n",
    "words = {} #hold words and associated counts\n",
    "spam_emails = 0 #how many emails are marked as spam\n",
    "spam_word_count = 0\n",
    "ham_word_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    \n",
    "    ID = components[0]\n",
    "    word = components[1]\n",
    "    spam = int(components[2])\n",
    "    \n",
    "    if word not in words.keys():\n",
    "        words[word] = {'spam_count': 0, 'ham_count': 0}\n",
    "    if ID not in emails:\n",
    "        emails.add(ID)\n",
    "        if spam == 1:\n",
    "            spam_emails += 1\n",
    "        \n",
    "\n",
    "    if spam == 1:\n",
    "        words[word]['spam_count'] += 1\n",
    "        spam_word_count += 1\n",
    "    else:\n",
    "        words[word]['ham_count'] += 1\n",
    "        ham_word_count += 1\n",
    "\n",
    "\n",
    "prior_spam = float(spam_emails)/len(emails)\n",
    "prior_ham = 1-prior_spam\n",
    "\n",
    "for i, word in words.iteritems():\n",
    "    word['spam_like'] = float(word['spam_count'])/(spam_word_count)\n",
    "    word['ham_like'] = float(word['ham_count'])/(ham_word_count)\n",
    "    \n",
    "\n",
    "print prior_spam\n",
    "print prior_ham\n",
    "for word in words.keys():\n",
    "    #Word \"\\t\" spam likelihood '\\t' ham likelihood written to file\n",
    "    print word + '\\t' + str(words[word]['spam_like']) + '\\t' + str(words[word]['ham_like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Examine output\n",
    "#!cat enronemail_1h.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/24 21:21:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/24 21:21:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 21:21:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#make directory\n",
    "!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 21:21:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `/user/dunmireg/enronemail_1h.txt': File exists\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 21:21:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 21:21:42 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/24 21:21:42 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/24 21:21:42 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/24 21:21:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 21:21:42 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/24 21:21:42 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1023877301_0001\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/24 21:21:43 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/24 21:21:43 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 21:21:43 INFO mapreduce.Job: Running job: job_local1023877301_0001\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: Starting task: attempt_local1023877301_0001_m_000000_0\n",
      "16/01/24 21:21:43 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 21:21:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 21:21:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: R/W/S=100/8672/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: \n",
      "16/01/24 21:21:43 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: Spilling map output\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: bufstart = 0; bufend = 1032108; bufvoid = 104857600\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26082748(104330992); length = 131649/6553600\n",
      "16/01/24 21:21:43 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/24 21:21:43 INFO mapred.Task: Task:attempt_local1023877301_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/24 21:21:43 INFO mapred.Task: Task 'attempt_local1023877301_0001_m_000000_0' done.\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: Finishing task: attempt_local1023877301_0001_m_000000_0\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: Starting task: attempt_local1023877301_0001_r_000000_0\n",
      "16/01/24 21:21:43 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 21:21:43 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 21:21:43 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 21:21:43 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@d7d5f7\n",
      "16/01/24 21:21:43 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/24 21:21:43 INFO reduce.EventFetcher: attempt_local1023877301_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/24 21:21:43 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1023877301_0001_m_000000_0 decomp: 1097936 len: 1097940 to MEMORY\n",
      "16/01/24 21:21:43 INFO reduce.InMemoryMapOutput: Read 1097936 bytes from map-output for attempt_local1023877301_0001_m_000000_0\n",
      "16/01/24 21:21:43 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1097936, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1097936\n",
      "16/01/24 21:21:43 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 21:21:43 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/24 21:21:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 21:21:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/24 21:21:43 INFO reduce.MergeManagerImpl: Merged 1 segments, 1097936 bytes to disk to satisfy reduce memory limit\n",
      "16/01/24 21:21:43 INFO reduce.MergeManagerImpl: Merging 1 files, 1097940 bytes from disk\n",
      "16/01/24 21:21:43 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/24 21:21:43 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 21:21:43 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/24 21:21:43 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/24 21:21:43 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:21:43 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:21:44 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:21:44 INFO mapreduce.Job: Job job_local1023877301_0001 running in uber mode : false\n",
      "16/01/24 21:21:44 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 21:21:46 INFO streaming.PipeMapRed: Records R/W=32913/1\n",
      "16/01/24 21:21:46 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 21:21:46 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 21:21:46 INFO mapred.Task: Task:attempt_local1023877301_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/24 21:21:46 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 21:21:46 INFO mapred.Task: Task attempt_local1023877301_0001_r_000000_0 is allowed to commit now\n",
      "16/01/24 21:21:46 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1023877301_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailCondProbs/_temporary/0/task_local1023877301_0001_r_000000\n",
      "16/01/24 21:21:46 INFO mapred.LocalJobRunner: Records R/W=32913/1 > reduce\n",
      "16/01/24 21:21:46 INFO mapred.Task: Task 'attempt_local1023877301_0001_r_000000_0' done.\n",
      "16/01/24 21:21:46 INFO mapred.LocalJobRunner: Finishing task: attempt_local1023877301_0001_r_000000_0\n",
      "16/01/24 21:21:46 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/24 21:21:47 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 21:21:47 INFO mapreduce.Job: Job job_local1023877301_0001 completed successfully\n",
      "16/01/24 21:21:47 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2407996\n",
      "\t\tFILE: Number of bytes written=4095648\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=172513\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32913\n",
      "\t\tMap output bytes=1032108\n",
      "\t\tMap output materialized bytes=1097940\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=1097940\n",
      "\t\tReduce input records=32913\n",
      "\t\tReduce output records=5493\n",
      "\t\tSpilled Records=65826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=5\n",
      "\t\tTotal committed heap usage (bytes)=511180800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=172513\n",
      "16/01/24 21:21:47 INFO streaming.StreamJob: Output directory: enroneEmailCondProbs\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailCondProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show results\n",
    "#!hdfs dfs -cat /user/dunmireg/enroneEmailCondProbs/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 21:21:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 21:21:59 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/24 21:21:59 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory\n",
    "#bin/hadoop fs -copyToLocal /hdfs/source/path /localfs/destination/path\n",
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailCondProbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 21:22:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 21:22:02 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 21:22:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 21:22:04 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailCondProbs\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/24 21:22:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/24 21:22:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt #check\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailCondProbs\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#I have placed the mapper here but have not modified it in any way from the previous mapper. It will still\n",
    "#produce ID + \\t + word + \\t + true spam flag to send to the reducer. \n",
    "import sys\n",
    "import re\n",
    "import os \n",
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "priorSpam = 0\n",
    "priorHam = 0\n",
    "words = {}\n",
    "\n",
    "with open(os.path.join('./enroneEmailCondProbs', 'part-00000'), 'r') as myfile:\n",
    "    lines = myfile.readlines()\n",
    "    priorSpam = float(lines[0])\n",
    "    priorHam = float(lines[1])\n",
    "    for line in lines[2:]:\n",
    "        components = line.split('\\t')\n",
    "        words[components[0]] = {'spam_like': float(components[1]), 'ham_like': float(components[2])}\n",
    "        \n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "spamSkip = 0\n",
    "hamSkip = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip()\n",
    "    text = re.findall(WORD_RE, text)\n",
    "    \n",
    "    spamScore = log(priorSpam)\n",
    "    hamScore = log(priorHam)\n",
    "    for word in text:\n",
    "        if word in words.keys():\n",
    "            if float(words[word]['spam_like']) != 0:\n",
    "                spamScore += log(float(words[word]['spam_like']))\n",
    "            else:\n",
    "                spamScore += -300\n",
    "                spamSkip += 1\n",
    "            if float(words[word]['ham_like']) != 0:\n",
    "                hamScore += log(float(words[word]['ham_like']))\n",
    "            else:\n",
    "                hamScore += -300\n",
    "                hamSkip += 1\n",
    "        pred = 0\n",
    "    if spamScore > hamScore:\n",
    "        pred = 1\n",
    "    print components[0] + '\\t' + components[1] + '\\t' + str(pred) + '\\t' + str(exp(spamScore)) + '\\t' + str(exp(hamScore))\n",
    "print spamSkip\n",
    "print hamSkip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "misclassified = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    if len(components) != 1:\n",
    "        if int(components[1]) != int(components[2]):\n",
    "                misclassified += 1\n",
    "        print line\n",
    "    else:\n",
    "        print line\n",
    "print \"Misclassified: \" + str(misclassified) + \" which means this has an accuracy of \" + str(100-misclassified) + \"%\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check results\n",
    "#!cat enronemail_1h.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/24 21:23:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/24 21:23:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make directory\n",
    "!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 21:23:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 21:23:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 21:23:29 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/24 21:23:29 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/24 21:23:29 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/24 21:23:30 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 21:23:30 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/24 21:23:30 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1498359296_0001\n",
      "16/01/24 21:23:30 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/24 21:23:30 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/24 21:23:30 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/24 21:23:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 21:23:30 INFO mapreduce.Job: Running job: job_local1498359296_0001\n",
      "16/01/24 21:23:30 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/24 21:23:30 INFO mapred.LocalJobRunner: Starting task: attempt_local1498359296_0001_m_000000_0\n",
      "16/01/24 21:23:30 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 21:23:30 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 21:23:30 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 21:23:30 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/24 21:23:30 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/24 21:23:30 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/24 21:23:30 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/24 21:23:30 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/24 21:23:30 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/24 21:23:30 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/24 21:23:30 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/24 21:23:30 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/24 21:23:30 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/24 21:23:30 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:23:30 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:23:31 INFO mapreduce.Job: Job job_local1498359296_0001 running in uber mode : false\n",
      "16/01/24 21:23:31 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 21:23:31 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:100=100/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 21:23:35 INFO mapred.LocalJobRunner: \n",
      "16/01/24 21:23:35 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/24 21:23:35 INFO mapred.MapTask: Spilling map output\n",
      "16/01/24 21:23:35 INFO mapred.MapTask: bufstart = 0; bufend = 3950; bufvoid = 104857600\n",
      "16/01/24 21:23:35 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26213992(104855968); length = 405/6553600\n",
      "16/01/24 21:23:35 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/24 21:23:35 INFO mapred.Task: Task:attempt_local1498359296_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/24 21:23:35 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "16/01/24 21:23:35 INFO mapred.Task: Task 'attempt_local1498359296_0001_m_000000_0' done.\n",
      "16/01/24 21:23:35 INFO mapred.LocalJobRunner: Finishing task: attempt_local1498359296_0001_m_000000_0\n",
      "16/01/24 21:23:35 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/24 21:23:35 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/24 21:23:35 INFO mapred.LocalJobRunner: Starting task: attempt_local1498359296_0001_r_000000_0\n",
      "16/01/24 21:23:35 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 21:23:35 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 21:23:35 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 21:23:35 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5dafef4d\n",
      "16/01/24 21:23:35 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/24 21:23:35 INFO reduce.EventFetcher: attempt_local1498359296_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/24 21:23:35 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1498359296_0001_m_000000_0 decomp: 4156 len: 4160 to MEMORY\n",
      "16/01/24 21:23:35 INFO reduce.InMemoryMapOutput: Read 4156 bytes from map-output for attempt_local1498359296_0001_m_000000_0\n",
      "16/01/24 21:23:35 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4156, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4156\n",
      "16/01/24 21:23:35 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/24 21:23:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 21:23:35 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/24 21:23:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 21:23:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4131 bytes\n",
      "16/01/24 21:23:35 INFO reduce.MergeManagerImpl: Merged 1 segments, 4156 bytes to disk to satisfy reduce memory limit\n",
      "16/01/24 21:23:35 INFO reduce.MergeManagerImpl: Merging 1 files, 4160 bytes from disk\n",
      "16/01/24 21:23:35 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/24 21:23:35 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 21:23:35 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4131 bytes\n",
      "16/01/24 21:23:35 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/24 21:23:35 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/24 21:23:35 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dunmireg/Documents/261HW/HW2/./reducer.py\", line 9, in <module>\n",
      "    if int(components[1]) != int(components[2]):\n",
      "ValueError: invalid literal for int() with base 10: ''\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: Records R/W=102/1\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: PipeMapRed failed!\n",
      "java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:453)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/01/24 21:23:35 INFO streaming.PipeMapRed: PipeMapRed failed!\n",
      "java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)\n",
      "\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/01/24 21:23:35 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/24 21:23:35 WARN mapred.LocalJobRunner: job_local1498359296_0001\n",
      "java.lang.Exception: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\n",
      "Caused by: java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:322)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:535)\n",
      "\tat org.apache.hadoop.streaming.PipeReducer.close(PipeReducer.java:134)\n",
      "\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\n",
      "\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n",
      "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
      "\tat java.lang.Thread.run(Thread.java:745)\n",
      "16/01/24 21:23:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 21:23:35 INFO mapreduce.Job: Job job_local1498359296_0001 failed with state FAILED due to: NA\n",
      "16/01/24 21:23:35 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=106042\n",
      "\t\tFILE: Number of bytes written=405106\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=204658\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=1\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=102\n",
      "\t\tMap output bytes=3950\n",
      "\t\tMap output materialized bytes=4160\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=0\n",
      "\t\tReduce shuffle bytes=4160\n",
      "\t\tReduce input records=0\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=102\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=310902784\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "16/01/24 21:23:35 ERROR streaming.StreamJob: Job not successful!\n",
      "Streaming Command Failed!\n",
      "16/01/24 21:23:35 WARN hdfs.DFSClient: DataStreamer Exception\n",
      "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dunmireg/enroneEmailClassificationNoSmoothing/_temporary/0/_temporary/attempt_local1498359296_0001_r_000000_0/part-00000 (inode 16429): File does not exist. Holder DFSClient_NONMAPREDUCE_382350742_1 does not have any open files.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3431)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3236)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3074)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3034)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1407)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy9.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy10.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1430)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1226)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:449)\n",
      "16/01/24 21:23:35 ERROR hdfs.DFSClient: Failed to close inode 16429\n",
      "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): No lease on /user/dunmireg/enroneEmailClassificationNoSmoothing/_temporary/0/_temporary/attempt_local1498359296_0001_r_000000_0/part-00000 (inode 16429): File does not exist. Holder DFSClient_NONMAPREDUCE_382350742_1 does not have any open files.\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3431)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState(FSNamesystem.java:3236)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getNewBlockTargets(FSNamesystem.java:3074)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3034)\n",
      "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:723)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:492)\n",
      "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)\n",
      "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2049)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2045)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n",
      "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2043)\n",
      "\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1476)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1407)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)\n",
      "\tat com.sun.proxy.$Proxy9.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:418)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:497)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)\n",
      "\tat com.sun.proxy.$Proxy10.addBlock(Unknown Source)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1430)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1226)\n",
      "\tat org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:449)\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailClassificationNoSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 20:14:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "cat: `/user/dunmireg/enroneEmailClassificationNoSmoothing/part-00000': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/dunmireg/enroneEmailClassificationNoSmoothing/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 20:15:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#move output to local directory - makes easier to process for analysis\n",
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailClassificationNoSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 21:23:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 21:23:47 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 21:23:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 21:23:48 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailClassificationNoSmoothing\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/24 21:24:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/24 21:24:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt #check\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailClassificationNoSmoothing\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEXCAYAAAC9A7+nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwZ+7xSmJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103aea9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEXCAYAAABI/TQXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103af7a10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#Make histogram\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spam_probs = []\n",
    "ham_probs = []\n",
    "with open(os.path.join('./enroneEmailClassificationNoSmoothing', 'part-00000'), 'r') as myfile:\n",
    "    lines = myfile.readlines()\n",
    "    misclassified = 0\n",
    "    for line in lines[:-2]:\n",
    "        components = line.split('\\t')\n",
    "        spam_probs.append(float(components[3]))\n",
    "        ham_probs.append(float(components[4]))\n",
    "\n",
    "s = plt.figure(1)\n",
    "plt.hist(spam_probs, bins = 100)\n",
    "plt.xlabel = \"Probability\"\n",
    "plt.ylabel = \"Frequency\"\n",
    "plt.title(\"Spam Probabilities Histogram\")\n",
    "\n",
    "h = plt.figure(2)\n",
    "plt.hist(ham_probs, bins = 100)\n",
    "plt.xlabel = \"Probability\"\n",
    "plt.ylabel = \"Frequency\"\n",
    "plt.title(\"Ham Probabilties Histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.4 \n",
    "\n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip()\n",
    "    words = re.findall(WORD_RE, text)\n",
    "    for word in words:\n",
    "        print components[0] + '\\t' + word + '\\t' + components[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "emails = set() #hold email IDs\n",
    "words = {} #hold words and associated counts\n",
    "spam_emails = 0 #how many emails are marked as spam\n",
    "spam_word_count = 0\n",
    "ham_word_count = 0\n",
    "vocab = set()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    \n",
    "    ID = components[0]\n",
    "    word = components[1]\n",
    "    spam = int(components[2])\n",
    "    \n",
    "    if word not in words.keys():\n",
    "        words[word] = {'spam_count': 0, 'ham_count': 0}\n",
    "        vocab.add(word)\n",
    "    if ID not in emails:\n",
    "        emails.add(ID)\n",
    "        if spam == 1:\n",
    "            spam_emails += 1\n",
    "        \n",
    "\n",
    "    if spam == 1:\n",
    "        words[word]['spam_count'] += 1\n",
    "        spam_word_count += 1\n",
    "    else:\n",
    "        words[word]['ham_count'] += 1\n",
    "        ham_word_count += 1\n",
    "\n",
    "\n",
    "prior_spam = float(spam_emails)/len(emails)\n",
    "prior_ham = 1-prior_spam\n",
    "\n",
    "for i, word in words.iteritems():\n",
    "    word['spam_like'] = float(word['spam_count'] + 1)/(spam_word_count + len(vocab))\n",
    "    word['ham_like'] = float(word['ham_count'] + 1)/(ham_word_count + len(vocab))\n",
    "    \n",
    "\n",
    "print prior_spam\n",
    "print prior_ham\n",
    "for word in words.keys():\n",
    "    #Word \"\\t\" spam likelihood '\\t' ham likelihood written to file\n",
    "    print word + '\\t' + str(words[word]['spam_like']) + '\\t' + str(words[word]['ham_like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "resourcemanager running as process 28941. Stop it first.\n",
      "localhost: nodemanager running as process 29016. Stop it first.\n",
      "16/01/24 22:46:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: namenode running as process 29131. Stop it first.\n",
      "localhost: datanode running as process 29200. Stop it first.\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: secondarynamenode running as process 29289. Stop it first.\n",
      "16/01/24 22:46:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:46:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#make directory\n",
    "!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:46:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:46:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 22:46:21 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/24 22:46:21 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/24 22:46:21 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/24 22:46:22 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 22:46:22 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/24 22:46:22 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1362718381_0001\n",
      "16/01/24 22:46:22 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/24 22:46:22 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/24 22:46:22 INFO mapreduce.Job: Running job: job_local1362718381_0001\n",
      "16/01/24 22:46:22 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/24 22:46:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 22:46:23 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/24 22:46:23 INFO mapred.LocalJobRunner: Starting task: attempt_local1362718381_0001_m_000000_0\n",
      "16/01/24 22:46:23 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 22:46:23 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 22:46:23 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/24 22:46:23 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/24 22:46:23 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/24 22:46:23 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:46:23 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:46:23 INFO streaming.PipeMapRed: Records R/W=96/1\n",
      "16/01/24 22:46:23 INFO streaming.PipeMapRed: R/W/S=100/116/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:46:23 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 22:46:23 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 22:46:23 INFO mapred.LocalJobRunner: \n",
      "16/01/24 22:46:23 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: Spilling map output\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: bufstart = 0; bufend = 1032108; bufvoid = 104857600\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26082748(104330992); length = 131649/6553600\n",
      "16/01/24 22:46:23 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/24 22:46:23 INFO mapred.Task: Task:attempt_local1362718381_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/24 22:46:23 INFO mapred.LocalJobRunner: Records R/W=96/1\n",
      "16/01/24 22:46:23 INFO mapred.Task: Task 'attempt_local1362718381_0001_m_000000_0' done.\n",
      "16/01/24 22:46:23 INFO mapred.LocalJobRunner: Finishing task: attempt_local1362718381_0001_m_000000_0\n",
      "16/01/24 22:46:23 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/24 22:46:23 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/24 22:46:23 INFO mapred.LocalJobRunner: Starting task: attempt_local1362718381_0001_r_000000_0\n",
      "16/01/24 22:46:23 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 22:46:23 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 22:46:23 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 22:46:23 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@609a718a\n",
      "16/01/24 22:46:23 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/24 22:46:23 INFO reduce.EventFetcher: attempt_local1362718381_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/24 22:46:23 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1362718381_0001_m_000000_0 decomp: 1097936 len: 1097940 to MEMORY\n",
      "16/01/24 22:46:23 INFO mapreduce.Job: Job job_local1362718381_0001 running in uber mode : false\n",
      "16/01/24 22:46:23 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 22:46:23 INFO reduce.InMemoryMapOutput: Read 1097936 bytes from map-output for attempt_local1362718381_0001_m_000000_0\n",
      "16/01/24 22:46:23 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1097936, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1097936\n",
      "16/01/24 22:46:23 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/24 22:46:23 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 22:46:23 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/24 22:46:23 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 22:46:23 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/24 22:46:24 INFO reduce.MergeManagerImpl: Merged 1 segments, 1097936 bytes to disk to satisfy reduce memory limit\n",
      "16/01/24 22:46:24 INFO reduce.MergeManagerImpl: Merging 1 files, 1097940 bytes from disk\n",
      "16/01/24 22:46:24 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/24 22:46:24 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 22:46:24 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/24 22:46:24 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 22:46:24 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/24 22:46:24 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/24 22:46:24 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/24 22:46:24 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:46:24 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:46:24 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:46:24 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:46:24 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:46:26 INFO streaming.PipeMapRed: Records R/W=32913/1\n",
      "16/01/24 22:46:26 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 22:46:26 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 22:46:26 INFO mapred.Task: Task:attempt_local1362718381_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/24 22:46:26 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 22:46:26 INFO mapred.Task: Task attempt_local1362718381_0001_r_000000_0 is allowed to commit now\n",
      "16/01/24 22:46:26 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1362718381_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailCondProbLaplace/_temporary/0/task_local1362718381_0001_r_000000\n",
      "16/01/24 22:46:26 INFO mapred.LocalJobRunner: Records R/W=32913/1 > reduce\n",
      "16/01/24 22:46:26 INFO mapred.Task: Task 'attempt_local1362718381_0001_r_000000_0' done.\n",
      "16/01/24 22:46:26 INFO mapred.LocalJobRunner: Finishing task: attempt_local1362718381_0001_r_000000_0\n",
      "16/01/24 22:46:26 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/24 22:46:26 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 22:46:26 INFO mapreduce.Job: Job job_local1362718381_0001 completed successfully\n",
      "16/01/24 22:46:26 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2407996\n",
      "\t\tFILE: Number of bytes written=4095684\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=237609\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32913\n",
      "\t\tMap output bytes=1032108\n",
      "\t\tMap output materialized bytes=1097940\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=1097940\n",
      "\t\tReduce input records=32913\n",
      "\t\tReduce output records=5493\n",
      "\t\tSpilled Records=65826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=7\n",
      "\t\tTotal committed heap usage (bytes)=510656512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=237609\n",
      "16/01/24 22:46:26 INFO streaming.StreamJob: Output directory: enroneEmailCondProbLaplace\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailCondProbLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check output\n",
    "#!hdfs dfs -cat /user/dunmireg/enroneEmailCondProbLaplace/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:46:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 22:46:37 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/24 22:46:37 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailCondProbLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 22:46:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 22:46:40 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 22:46:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 22:46:42 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailCondProbLaplace\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/24 22:46:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/24 22:47:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailCondProbLaplace\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#I have placed the mapper here but have not modified it in any way from the previous mapper. It will still\n",
    "#produce ID + \\t + word + \\t + true spam flag to send to the reducer. \n",
    "import sys\n",
    "import re\n",
    "import os \n",
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "priorSpam = 0\n",
    "priorHam = 0\n",
    "words = {}\n",
    "\n",
    "with open(os.path.join('./enroneEmailCondProbLaplace', 'part-00000'), 'r') as myfile:\n",
    "    lines = myfile.readlines()\n",
    "    priorSpam = float(lines[0])\n",
    "    priorHam = float(lines[1])\n",
    "    for line in lines[2:]:\n",
    "        components = line.split('\\t')\n",
    "        words[components[0]] = {'spam_like': float(components[1]), 'ham_like': float(components[2])}\n",
    "        \n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "spamSkip = 0\n",
    "hamSkip = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip()\n",
    "    text = re.findall(WORD_RE, text)\n",
    "    \n",
    "    spamScore = log(priorSpam)\n",
    "    hamScore = log(priorHam)\n",
    "    for word in text:\n",
    "        if word in words.keys():\n",
    "            if float(words[word]['spam_like']) != 0:\n",
    "                spamScore += log(float(words[word]['spam_like']))\n",
    "            else:\n",
    "                spamScore += -300\n",
    "                spamSkip += 1\n",
    "            if float(words[word]['ham_like']) != 0:\n",
    "                hamScore += log(float(words[word]['ham_like']))\n",
    "            else:\n",
    "                hamScore += -300\n",
    "                hamSkip += 1\n",
    "        pred = 0\n",
    "    if spamScore > hamScore:\n",
    "        pred = 1\n",
    "    print components[0] + '\\t' + components[1] + '\\t' + str(pred) + '\\t' + str(exp(spamScore)) + '\\t' + str(exp(hamScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "misclassified = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    if int(components[1]) != int(components[2]):\n",
    "            misclassified += 1\n",
    "    print line\n",
    "print \"Misclassified: \" + str(misclassified) + \" which means this has an accuracy of \" + str(100-misclassified) + \"%\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check code\n",
    "#!cat enronemail_1h.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/24 22:47:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/24 22:47:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:47:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#make directory\n",
    "!hdfs dfs -mkdir -p /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:48:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:48:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 22:48:04 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/24 22:48:04 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/24 22:48:04 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/24 22:48:04 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 22:48:05 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/24 22:48:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local101249988_0001\n",
      "16/01/24 22:48:05 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/24 22:48:05 INFO mapreduce.Job: Running job: job_local101249988_0001\n",
      "16/01/24 22:48:05 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/24 22:48:05 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/24 22:48:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 22:48:05 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/24 22:48:05 INFO mapred.LocalJobRunner: Starting task: attempt_local101249988_0001_m_000000_0\n",
      "16/01/24 22:48:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 22:48:05 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 22:48:05 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 22:48:05 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/24 22:48:05 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/24 22:48:05 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/24 22:48:05 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/24 22:48:05 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/24 22:48:05 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/24 22:48:05 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/24 22:48:05 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/24 22:48:05 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/24 22:48:05 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/24 22:48:05 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:48:05 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:48:06 INFO mapreduce.Job: Job job_local101249988_0001 running in uber mode : false\n",
      "16/01/24 22:48:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 22:48:06 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:100=100/1 [rec/s] out:0=0/1 [rec/s]\n",
      "16/01/24 22:48:09 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 22:48:09 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/24 22:48:09 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 22:48:09 INFO mapred.LocalJobRunner: \n",
      "16/01/24 22:48:09 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/24 22:48:09 INFO mapred.MapTask: Spilling map output\n",
      "16/01/24 22:48:09 INFO mapred.MapTask: bufstart = 0; bufend = 4284; bufvoid = 104857600\n",
      "16/01/24 22:48:09 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "16/01/24 22:48:09 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/24 22:48:09 INFO mapred.Task: Task:attempt_local101249988_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/24 22:48:09 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "16/01/24 22:48:09 INFO mapred.Task: Task 'attempt_local101249988_0001_m_000000_0' done.\n",
      "16/01/24 22:48:09 INFO mapred.LocalJobRunner: Finishing task: attempt_local101249988_0001_m_000000_0\n",
      "16/01/24 22:48:09 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/24 22:48:09 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/24 22:48:09 INFO mapred.LocalJobRunner: Starting task: attempt_local101249988_0001_r_000000_0\n",
      "16/01/24 22:48:09 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 22:48:09 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 22:48:09 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 22:48:09 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@71afb01\n",
      "16/01/24 22:48:09 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/24 22:48:09 INFO reduce.EventFetcher: attempt_local101249988_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/24 22:48:10 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local101249988_0001_m_000000_0 decomp: 4486 len: 4490 to MEMORY\n",
      "16/01/24 22:48:10 INFO reduce.InMemoryMapOutput: Read 4486 bytes from map-output for attempt_local101249988_0001_m_000000_0\n",
      "16/01/24 22:48:10 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4486, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4486\n",
      "16/01/24 22:48:10 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/24 22:48:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 22:48:10 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/24 22:48:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 22:48:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4461 bytes\n",
      "16/01/24 22:48:10 INFO reduce.MergeManagerImpl: Merged 1 segments, 4486 bytes to disk to satisfy reduce memory limit\n",
      "16/01/24 22:48:10 INFO reduce.MergeManagerImpl: Merging 1 files, 4490 bytes from disk\n",
      "16/01/24 22:48:10 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/24 22:48:10 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 22:48:10 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4461 bytes\n",
      "16/01/24 22:48:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 22:48:10 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/24 22:48:10 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/24 22:48:10 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/24 22:48:10 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:48:10 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:48:10 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 22:48:10 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/24 22:48:10 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 22:48:10 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 22:48:10 INFO mapred.Task: Task:attempt_local101249988_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/24 22:48:10 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 22:48:10 INFO mapred.Task: Task attempt_local101249988_0001_r_000000_0 is allowed to commit now\n",
      "16/01/24 22:48:10 INFO output.FileOutputCommitter: Saved output of task 'attempt_local101249988_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailClassLaplace/_temporary/0/task_local101249988_0001_r_000000\n",
      "16/01/24 22:48:10 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "16/01/24 22:48:10 INFO mapred.Task: Task 'attempt_local101249988_0001_r_000000_0' done.\n",
      "16/01/24 22:48:10 INFO mapred.LocalJobRunner: Finishing task: attempt_local101249988_0001_r_000000_0\n",
      "16/01/24 22:48:10 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/24 22:48:10 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 22:48:10 INFO mapreduce.Job: Job job_local101249988_0001 completed successfully\n",
      "16/01/24 22:48:10 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=221096\n",
      "\t\tFILE: Number of bytes written=812314\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=4543\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=4284\n",
      "\t\tMap output materialized bytes=4490\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=4490\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=201\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=618659840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4543\n",
      "16/01/24 22:48:10 INFO streaming.StreamJob: Output directory: enroneEmailClassLaplace\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailClassLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check output\n",
    "#!hdfs dfs -cat /user/dunmireg/enroneEmailClassLaplace/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 22:48:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 22:48:23 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/24 22:48:23 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailClassLaplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 22:48:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 22:48:26 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 22:48:27 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 22:48:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailClassLaplace\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "localhost: nodemanager did not stop gracefully after 5 seconds: killing with kill -9\n",
      "no proxyserver to stop\n",
      "16/01/24 22:48:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/24 22:48:57 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailClassLaplace\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### HW2.5. \n",
    "\n",
    "Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip()\n",
    "    words = re.findall(WORD_RE, text)\n",
    "    for word in words:\n",
    "        print components[0] + '\\t' + word + '\\t' + components[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "emails = set() #hold email IDs\n",
    "words = {} #hold words and associated counts\n",
    "spam_emails = 0 #how many emails are marked as spam\n",
    "spam_word_count = 0\n",
    "ham_word_count = 0\n",
    "vocab = set()\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    \n",
    "    ID = components[0]\n",
    "    word = components[1]\n",
    "    spam = int(components[2])\n",
    "    \n",
    "    if word not in words.keys():\n",
    "        words[word] = {'spam_count': 0, 'ham_count': 0}\n",
    "        vocab.add(word)\n",
    "    if ID not in emails:\n",
    "        emails.add(ID)\n",
    "        if spam == 1:\n",
    "            spam_emails += 1\n",
    "        \n",
    "\n",
    "    if spam == 1:\n",
    "        words[word]['spam_count'] += 1\n",
    "        spam_word_count += 1\n",
    "    else:\n",
    "        words[word]['ham_count'] += 1\n",
    "        ham_word_count += 1\n",
    "\n",
    "\n",
    "prior_spam = float(spam_emails)/len(emails)\n",
    "prior_ham = 1-prior_spam\n",
    "\n",
    "for word in words.keys():\n",
    "    if words[word]['spam_count'] + words[word]['ham_count'] < 3:\n",
    "        del words[word]\n",
    "\n",
    "for i, word in words.iteritems():\n",
    "    word['spam_like'] = float(word['spam_count'] + 1)/(spam_word_count + len(vocab))\n",
    "    word['ham_like'] = float(word['ham_count'] + 1)/(ham_word_count + len(vocab))\n",
    "    \n",
    "\n",
    "print prior_spam\n",
    "print prior_ham\n",
    "for word in words.keys():\n",
    "    #Word \"\\t\" spam likelihood '\\t' ham likelihood written to file\n",
    "    print word + '\\t' + str(words[word]['spam_like']) + '\\t' + str(words[word]['ham_like'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/24 23:08:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/24 23:08:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 23:08:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 23:09:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 23:09:14 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/24 23:09:14 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/24 23:09:14 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/24 23:09:14 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 23:09:14 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/24 23:09:15 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local108502301_0001\n",
      "16/01/24 23:09:15 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/24 23:09:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 23:09:15 INFO mapreduce.Job: Running job: job_local108502301_0001\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: Starting task: attempt_local108502301_0001_m_000000_0\n",
      "16/01/24 23:09:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 23:09:15 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 23:09:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/24 23:09:15 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/24 23:09:15 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/24 23:09:15 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:09:15 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:09:15 INFO streaming.PipeMapRed: Records R/W=72/1\n",
      "16/01/24 23:09:15 INFO streaming.PipeMapRed: R/W/S=100/9645/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:09:15 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 23:09:15 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: \n",
      "16/01/24 23:09:15 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: Spilling map output\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: bufstart = 0; bufend = 1032108; bufvoid = 104857600\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26082748(104330992); length = 131649/6553600\n",
      "16/01/24 23:09:15 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/24 23:09:15 INFO mapred.Task: Task:attempt_local108502301_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: Records R/W=72/1\n",
      "16/01/24 23:09:15 INFO mapred.Task: Task 'attempt_local108502301_0001_m_000000_0' done.\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: Finishing task: attempt_local108502301_0001_m_000000_0\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/24 23:09:15 INFO mapred.LocalJobRunner: Starting task: attempt_local108502301_0001_r_000000_0\n",
      "16/01/24 23:09:15 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 23:09:15 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 23:09:15 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 23:09:15 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@45538a6\n",
      "16/01/24 23:09:15 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/24 23:09:15 INFO reduce.EventFetcher: attempt_local108502301_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/24 23:09:16 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local108502301_0001_m_000000_0 decomp: 1097936 len: 1097940 to MEMORY\n",
      "16/01/24 23:09:16 INFO reduce.InMemoryMapOutput: Read 1097936 bytes from map-output for attempt_local108502301_0001_m_000000_0\n",
      "16/01/24 23:09:16 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 1097936, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1097936\n",
      "16/01/24 23:09:16 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/24 23:09:16 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 23:09:16 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/24 23:09:16 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 23:09:16 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/24 23:09:16 INFO reduce.MergeManagerImpl: Merged 1 segments, 1097936 bytes to disk to satisfy reduce memory limit\n",
      "16/01/24 23:09:16 INFO reduce.MergeManagerImpl: Merging 1 files, 1097940 bytes from disk\n",
      "16/01/24 23:09:16 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/24 23:09:16 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 23:09:16 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 1097911 bytes\n",
      "16/01/24 23:09:16 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 23:09:16 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/24 23:09:16 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/24 23:09:16 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/24 23:09:16 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:09:16 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:09:16 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:09:16 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:09:16 INFO mapreduce.Job: Job job_local108502301_0001 running in uber mode : false\n",
      "16/01/24 23:09:16 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 23:09:16 INFO streaming.PipeMapRed: R/W/S=10000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:09:18 INFO streaming.PipeMapRed: Records R/W=32913/1\n",
      "16/01/24 23:09:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 23:09:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 23:09:18 INFO mapred.Task: Task:attempt_local108502301_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/24 23:09:18 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 23:09:18 INFO mapred.Task: Task attempt_local108502301_0001_r_000000_0 is allowed to commit now\n",
      "16/01/24 23:09:18 INFO output.FileOutputCommitter: Saved output of task 'attempt_local108502301_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailCondProb3/_temporary/0/task_local108502301_0001_r_000000\n",
      "16/01/24 23:09:18 INFO mapred.LocalJobRunner: Records R/W=32913/1 > reduce\n",
      "16/01/24 23:09:18 INFO mapred.Task: Task 'attempt_local108502301_0001_r_000000_0' done.\n",
      "16/01/24 23:09:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local108502301_0001_r_000000_0\n",
      "16/01/24 23:09:18 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/24 23:09:19 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 23:09:19 INFO mapreduce.Job: Job job_local108502301_0001 completed successfully\n",
      "16/01/24 23:09:19 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2407996\n",
      "\t\tFILE: Number of bytes written=4092652\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=79886\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=32913\n",
      "\t\tMap output bytes=1032108\n",
      "\t\tMap output materialized bytes=1097940\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=1097940\n",
      "\t\tReduce input records=32913\n",
      "\t\tReduce output records=1883\n",
      "\t\tSpilled Records=65826\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=511180800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=79886\n",
      "16/01/24 23:09:19 INFO streaming.StreamJob: Output directory: enroneEmailCondProb3\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailCondProb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 23:10:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 23:10:59 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/24 23:10:59 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailCondProb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 23:11:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 23:11:16 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 23:11:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 23:11:17 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailCondProb3\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/24 23:11:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/24 23:11:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailCondProb3\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#I have placed the mapper here but have not modified it in any way from the previous mapper. It will still\n",
    "#produce ID + \\t + word + \\t + true spam flag to send to the reducer. \n",
    "import sys\n",
    "import re\n",
    "import os \n",
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "priorSpam = 0\n",
    "priorHam = 0\n",
    "words = {}\n",
    "\n",
    "with open(os.path.join('./enroneEmailCondProbLaplace', 'part-00000'), 'r') as myfile:\n",
    "    lines = myfile.readlines()\n",
    "    priorSpam = float(lines[0])\n",
    "    priorHam = float(lines[1])\n",
    "    for line in lines[2:]:\n",
    "        components = line.split('\\t')\n",
    "        words[components[0]] = {'spam_like': float(components[1]), 'ham_like': float(components[2])}\n",
    "        \n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "spamSkip = 0\n",
    "hamSkip = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    text = \" \".join(components[-2:]).strip()\n",
    "    text = re.findall(WORD_RE, text)\n",
    "    \n",
    "    spamScore = log(priorSpam)\n",
    "    hamScore = log(priorHam)\n",
    "    for word in text:\n",
    "        if word in words.keys():\n",
    "            if float(words[word]['spam_like']) != 0:\n",
    "                spamScore += log(float(words[word]['spam_like']))\n",
    "            else:\n",
    "                spamScore += -300\n",
    "                spamSkip += 1\n",
    "            if float(words[word]['ham_like']) != 0:\n",
    "                hamScore += log(float(words[word]['ham_like']))\n",
    "            else:\n",
    "                hamScore += -300\n",
    "                hamSkip += 1\n",
    "        pred = 0\n",
    "    if spamScore > hamScore:\n",
    "        pred = 1\n",
    "    print components[0] + '\\t' + components[1] + '\\t' + str(pred) + '\\t' + str(exp(spamScore)) + '\\t' + str(exp(hamScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "misclassified = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    components = line.split('\\t')\n",
    "    if int(components[1]) != int(components[2]):\n",
    "            misclassified += 1\n",
    "    print line\n",
    "print \"Misclassified: \" + str(misclassified) + \" which means this has an accuracy of \" + str(100-misclassified) + \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Check code\n",
    "#!cat enronemail_1h.txt | python mapper.py | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-resourcemanager-Glenns-Air.home.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/yarn-dunmireg-nodemanager-Glenns-Air.home.out\n",
      "16/01/24 23:14:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-namenode-Glenns-Air.home.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-datanode-Glenns-Air.home.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.7.1/libexec/logs/hadoop-dunmireg-secondarynamenode-Glenns-Air.home.out\n",
      "16/01/24 23:14:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Start hadoop yarn\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 23:15:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "#add input to hdfs\n",
    "!hdfs dfs -put enronemail_1h.txt /user/dunmireg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 23:15:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 23:15:16 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "16/01/24 23:15:16 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "16/01/24 23:15:16 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "16/01/24 23:15:16 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/01/24 23:15:16 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/01/24 23:15:17 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1577829084_0001\n",
      "16/01/24 23:15:17 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "16/01/24 23:15:17 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "16/01/24 23:15:17 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "16/01/24 23:15:17 INFO mapreduce.Job: Running job: job_local1577829084_0001\n",
      "16/01/24 23:15:17 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 23:15:17 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "16/01/24 23:15:17 INFO mapred.LocalJobRunner: Starting task: attempt_local1577829084_0001_m_000000_0\n",
      "16/01/24 23:15:17 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 23:15:17 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 23:15:17 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 23:15:17 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/dunmireg/enronemail_1h.txt:0+204658\n",
      "16/01/24 23:15:17 INFO mapred.MapTask: numReduceTasks: 1\n",
      "16/01/24 23:15:17 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "16/01/24 23:15:17 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "16/01/24 23:15:17 INFO mapred.MapTask: soft limit at 83886080\n",
      "16/01/24 23:15:17 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "16/01/24 23:15:17 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "16/01/24 23:15:17 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "16/01/24 23:15:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./mapper.py]\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "16/01/24 23:15:17 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "16/01/24 23:15:17 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:15:17 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:15:17 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:15:18 INFO mapreduce.Job: Job job_local1577829084_0001 running in uber mode : false\n",
      "16/01/24 23:15:18 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: \n",
      "16/01/24 23:15:22 INFO mapred.MapTask: Starting flush of map output\n",
      "16/01/24 23:15:22 INFO mapred.MapTask: Spilling map output\n",
      "16/01/24 23:15:22 INFO mapred.MapTask: bufstart = 0; bufend = 4284; bufvoid = 104857600\n",
      "16/01/24 23:15:22 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214000(104856000); length = 397/6553600\n",
      "16/01/24 23:15:22 INFO mapred.MapTask: Finished spill 0\n",
      "16/01/24 23:15:22 INFO mapred.Task: Task:attempt_local1577829084_0001_m_000000_0 is done. And is in the process of committing\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: Records R/W=100/1\n",
      "16/01/24 23:15:22 INFO mapred.Task: Task 'attempt_local1577829084_0001_m_000000_0' done.\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local1577829084_0001_m_000000_0\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: Starting task: attempt_local1577829084_0001_r_000000_0\n",
      "16/01/24 23:15:22 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "16/01/24 23:15:22 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "16/01/24 23:15:22 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "16/01/24 23:15:22 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@161de46\n",
      "16/01/24 23:15:22 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=334338464, maxSingleShuffleLimit=83584616, mergeThreshold=220663392, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "16/01/24 23:15:22 INFO reduce.EventFetcher: attempt_local1577829084_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "16/01/24 23:15:22 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/01/24 23:15:22 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1577829084_0001_m_000000_0 decomp: 4486 len: 4490 to MEMORY\n",
      "16/01/24 23:15:22 INFO reduce.InMemoryMapOutput: Read 4486 bytes from map-output for attempt_local1577829084_0001_m_000000_0\n",
      "16/01/24 23:15:22 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 4486, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->4486\n",
      "16/01/24 23:15:22 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 23:15:22 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "16/01/24 23:15:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 23:15:22 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4461 bytes\n",
      "16/01/24 23:15:22 INFO reduce.MergeManagerImpl: Merged 1 segments, 4486 bytes to disk to satisfy reduce memory limit\n",
      "16/01/24 23:15:22 INFO reduce.MergeManagerImpl: Merging 1 files, 4490 bytes from disk\n",
      "16/01/24 23:15:22 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "16/01/24 23:15:22 INFO mapred.Merger: Merging 1 sorted segments\n",
      "16/01/24 23:15:22 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 4461 bytes\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/dunmireg/Documents/261HW/HW2/./reducer.py]\n",
      "16/01/24 23:15:22 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "16/01/24 23:15:22 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: Records R/W=100/1\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "16/01/24 23:15:22 INFO streaming.PipeMapRed: mapRedFinished\n",
      "16/01/24 23:15:22 INFO mapred.Task: Task:attempt_local1577829084_0001_r_000000_0 is done. And is in the process of committing\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "16/01/24 23:15:22 INFO mapred.Task: Task attempt_local1577829084_0001_r_000000_0 is allowed to commit now\n",
      "16/01/24 23:15:22 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1577829084_0001_r_000000_0' to hdfs://localhost:9000/user/dunmireg/enroneEmailClass3/_temporary/0/task_local1577829084_0001_r_000000\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: Records R/W=100/1 > reduce\n",
      "16/01/24 23:15:22 INFO mapred.Task: Task 'attempt_local1577829084_0001_r_000000_0' done.\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: Finishing task: attempt_local1577829084_0001_r_000000_0\n",
      "16/01/24 23:15:22 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "16/01/24 23:15:23 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/01/24 23:15:23 INFO mapreduce.Job: Job job_local1577829084_0001 completed successfully\n",
      "16/01/24 23:15:23 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=221096\n",
      "\t\tFILE: Number of bytes written=815298\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=409316\n",
      "\t\tHDFS: Number of bytes written=4543\n",
      "\t\tHDFS: Number of read operations=13\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=100\n",
      "\t\tMap output records=100\n",
      "\t\tMap output bytes=4284\n",
      "\t\tMap output materialized bytes=4490\n",
      "\t\tInput split bytes=105\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=100\n",
      "\t\tReduce shuffle bytes=4490\n",
      "\t\tReduce input records=100\n",
      "\t\tReduce output records=201\n",
      "\t\tSpilled Records=200\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=6\n",
      "\t\tTotal committed heap usage (bytes)=508559360\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=204658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4543\n",
      "16/01/24 23:15:23 INFO streaming.StreamJob: Output directory: enroneEmailClass3\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \\\n",
    "-mapper mapper.py \\\n",
    "-reducer reducer.py \\\n",
    "-input enronemail_1h.txt \\\n",
    "-output enroneEmailClass3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/24 23:16:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 23:16:07 WARN hdfs.DFSClient: DFSInputStream has been closed already\n",
      "16/01/24 23:16:08 WARN hdfs.DFSClient: DFSInputStream has been closed already\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -copyToLocal /user/dunmireg/enroneEmailClass3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 23:16:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 23:16:21 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enronemail_1h.txt\n",
      "rmr: DEPRECATED: Please use 'rm -r' instead.\n",
      "16/01/24 23:16:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/24 23:16:23 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/dunmireg/enroneEmailClass3\n",
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "16/01/24 23:16:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "16/01/24 23:16:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Remove output directory and stop yarn and hdfs\n",
    "!hadoop fs -rmr /user/dunmireg/enronemail_1h.txt\n",
    "!hadoop fs -rmr /user/dunmireg/enroneEmailClass3\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
