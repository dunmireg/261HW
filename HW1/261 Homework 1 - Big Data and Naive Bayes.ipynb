{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 261 Homework 1 - Big Data and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise. \n",
    "\n",
    "HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__: Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NB__ Certain lines appeared to be incorrect with some text based errors. The ids are listed below. I have edited the text and stored the original, unedited text as a file \"enronemail_1h copy.txt\"\n",
    "\n",
    "0011.2001-06-28.SA_and_HP -> incorrect return made two lines\n",
    "\n",
    "0001.2000-06-06.lokay -> separated text to appropriate place and made subject NA\n",
    "\n",
    "0009.2001-06-26.SA_and_HP -> moved text to content section and made subject NA\n",
    "\n",
    "This was manually checked using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        stuff = line.split('\\t')\n",
    "        if len(stuff) != 4:\n",
    "            print stuff[0]\n",
    "            print len(stuff)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__: Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "   To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word.\n",
    "\n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l\n",
    "      \n",
    "      8 (should really be 10 if counting occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        words = re.findall(WORD_RE, line)\n",
    "        occ = [i for i,x in enumerate(words) if x.lower() == findword.lower()]\n",
    "        count += len(occ)\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Overwrite mapper.py\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "files = sys.argv[1:]\n",
    "sum = 0\n",
    "for filename in files:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            sum = sum + int(line)\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#overwrite reducer.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "#\\rm $data.chunk.*\n",
    "\\rm enronemail_1h.txt.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times word appears: \n",
      "10\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\"\n",
    "! echo \"Number of times word appears: \"\n",
    "! head enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        flag = components[1]\n",
    "        text = components[2] + components[3]\n",
    "        word_count = {}\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        for word, count in word_count.iteritems():\n",
    "            found_word = 0\n",
    "            if word.lower() == findword.lower():\n",
    "                found_word = 1\n",
    "            print ID + '\\t' + str(flag) + '\\t' + word + '\\t' + str(count) + '\\t' + str(found_word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Overwrite mapper.py\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "emailID = set()\n",
    "spam = set()\n",
    "vocab = set()\n",
    "spamCount = 0\n",
    "spamWordCount = 0\n",
    "hamCount= 0\n",
    "hamWordCount = 0\n",
    "foundWord = set()\n",
    "\n",
    "files = sys.argv[1:]\n",
    "for filename in files:\n",
    "    with open(filename, 'r') as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            components = line.split('\\t')\n",
    "            ID = components[0]\n",
    "            flag = int(components[1])\n",
    "            word = components[2]\n",
    "            count = int(components[3])\n",
    "            findword = int(components[4])\n",
    "            if findword == 1:\n",
    "                foundWord.add(word)\n",
    "            emailID.add(ID)\n",
    "            if flag == 1:\n",
    "                spam.add(ID)\n",
    "                spamCount += count\n",
    "                if findword == 1:\n",
    "                    spamWordCount += count\n",
    "            else:\n",
    "                hamCount += count\n",
    "                if findword == 1:\n",
    "                    hamWordCount += count\n",
    "\n",
    "            \n",
    "priorSpam = float(len(spam))/len(emailID)\n",
    "priorHam = float(len(emailID) - len(spam))/len(emailID)\n",
    "\n",
    "\n",
    "condProbSpam = float(spamWordCount)/spamCount\n",
    "condProbHam = float(hamWordCount)/hamCount\n",
    "\n",
    "#classify new emails\n",
    "#in this case I decided to read the original file, rather than reassembling the emails from the reducer step.\n",
    "#This increases modularity because in practice we would report our results on a validation or test set as opposed\n",
    "#to the training set. \n",
    "WORD_RE = re.compile(r\"[\\w']+\")    \n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        trueLabel = components[1]\n",
    "        text = components[2] + ' ' + components[3]\n",
    "        spamScore = log(priorSpam)\n",
    "        hamScore = log(priorHam)\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in foundWord and condProbSpam != 0:\n",
    "                spamScore += log(condProbSpam)\n",
    "            if word in foundWord and condProbHam != 0:\n",
    "                hamScore += log(condProbHam)\n",
    "        predicted = 0\n",
    "        if spamScore > hamScore:\n",
    "            predicted = 1\n",
    "        print ID + '\\t' + str(trueLabel) + '\\t' + str(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#overwrite reducer.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "with open('enronemail_1h.txt.output', 'r') as myfile:\n",
    "    count = 0\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        if int(components[1]) != int(components[2]):\n",
    "            count += 1\n",
    "    print count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 4\n",
    "\n",
    "HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py \n",
    "\n",
    "   performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        flag = components[1]\n",
    "        text = components[2] + components[3]\n",
    "        word_count = {}\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        for word, count in word_count.iteritems():\n",
    "            found_word = 0\n",
    "            if word.lower() in findwords:\n",
    "                found_word = 1\n",
    "            print ID + '\\t' + str(flag) + '\\t' + word + '\\t' + str(count) + '\\t' + str(found_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Overwrite mapper.py\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "emailID = set()\n",
    "spam = set()\n",
    "vocab = set()\n",
    "spamCount = 0\n",
    "spamWordCount = {}\n",
    "hamCount= 0\n",
    "hamWordCount = {}\n",
    "foundWord = set()\n",
    "\n",
    "files = sys.argv[1:]\n",
    "for filename in files:\n",
    "    with open(filename, 'r') as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            components = line.split('\\t')\n",
    "            ID = components[0]\n",
    "            flag = int(components[1])\n",
    "            word = components[2]\n",
    "            count = int(components[3])\n",
    "            findword = int(components[4])\n",
    "            if findword == 1:\n",
    "                foundWord.add(word)\n",
    "            emailID.add(ID)\n",
    "            if flag == 1:\n",
    "                spam.add(ID)\n",
    "                spamCount += count\n",
    "                if findword == 1:\n",
    "                    if word not in spamWordCount:\n",
    "                        spamWordCount[word] = count\n",
    "                    else:\n",
    "                        spamWordCount[word] += count\n",
    "            else:\n",
    "                hamCount += count\n",
    "                if findword == 1:\n",
    "                    if word not in hamWordCount:\n",
    "                        hamWordCount[word] = count\n",
    "                    else:\n",
    "                        hamWordCount[word] += count\n",
    "\n",
    "            \n",
    "priorSpam = float(len(spam))/len(emailID)\n",
    "priorHam = float(len(emailID) - len(spam))/len(emailID)\n",
    "\n",
    "\n",
    "condProbSpam = {}\n",
    "condProbHam = {}\n",
    "\n",
    "for word in foundWord:\n",
    "    if word in spamWordCount:\n",
    "        condProbSpam[word] = float(spamWordCount[word])/spamCount\n",
    "    if word in hamWordCount:\n",
    "        condProbHam[word] = float(hamWordCount[word])/hamCount\n",
    "\n",
    "        \n",
    "#classify new emails\n",
    "#in this case I decided to read the original file, rather than reassembling the emails from the reducer step.\n",
    "#This increases modularity because in practice we would report our results on a validation or test set as opposed\n",
    "#to the training set. \n",
    "WORD_RE = re.compile(r\"[\\w']+\")    \n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        trueLabel = components[1]\n",
    "        text = components[2] + ' ' + components[3]\n",
    "        spamScore = log(priorSpam)\n",
    "        hamScore = log(priorHam)\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in foundWord and word in condProbSpam:\n",
    "                spamScore += log(condProbSpam[word])\n",
    "            if word in foundWord and word in condProbHam:\n",
    "                hamScore += log(condProbHam[word])\n",
    "        predicted = 0\n",
    "        if spamScore > hamScore:\n",
    "            predicted = 1\n",
    "        print ID + '\\t' + str(trueLabel) + '\\t' + str(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#overwrite reducer.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 'assistance valium enlargementWithATypo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "with open('enronemail_1h.txt.output', 'r') as myfile:\n",
    "    count = 0\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        if int(components[1]) != int(components[2]):\n",
    "            count += 1\n",
    "    print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.82098055207\n",
      "-0.579818495253\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 5\n",
    "\n",
    "HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present.\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of all words, and\n",
    "   - reducer.py performs a word-distribution-wide Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        flag = components[1]\n",
    "        text = components[2] + components[3]\n",
    "        word_count = {}\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        for word, count in word_count.iteritems():\n",
    "            print ID + '\\t' + str(flag) + '\\t' + word + '\\t' + str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change permissions on mapper\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "emails = set()\n",
    "spams = set()\n",
    "words_in_corpus = set()\n",
    "spam_counts = {}\n",
    "ham_counts = {}\n",
    "\n",
    "files = sys.argv[1:]\n",
    "for filename in files:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            components = line.split('\\t')\n",
    "            ID = components[0]\n",
    "            flag = int(components[1])\n",
    "            word = components[2]\n",
    "            count = int(components[3])\n",
    "            emails.add(ID)\n",
    "            words_in_corpus.add(word)\n",
    "            if flag == 1:\n",
    "                spams.add(ID)\n",
    "                if word not in spam_counts:\n",
    "                    spam_counts[word] = count\n",
    "                else:\n",
    "                    spam_counts[word] += count\n",
    "            else:\n",
    "                if word not in ham_counts:\n",
    "                    ham_counts[word] = count\n",
    "                else:\n",
    "                    ham_counts[word] += count\n",
    "            \n",
    "priorSpam = float(len(spams))/len(emails)\n",
    "priorHam = float(len(emails) - len(spams))/len(emails)\n",
    "\n",
    "condProbSpam = {}\n",
    "condProbHam = {}\n",
    "\n",
    "for word in words_in_corpus:\n",
    "    if word in spam_counts:\n",
    "        spam_tokens = spam_counts[word] + 1\n",
    "    else:\n",
    "        spam_tokens = 1\n",
    "    condProbSpam[word] = float(spam_tokens)/(sum(spam_counts.values()) + len(words_in_corpus))\n",
    "    if word in ham_counts:\n",
    "        ham_tokens = ham_counts[word] + 1\n",
    "    else:\n",
    "        ham_tokens = 1\n",
    "    condProbHam[word] = float(ham_tokens)/(sum(ham_counts.values()) + len(words_in_corpus))\n",
    "\n",
    "#classify new emails\n",
    "#in this case I decided to read the original file, rather than reassembling the emails from the reducer step.\n",
    "#This increases modularity because in practice we would report our results on a validation or test set as opposed\n",
    "#to the training set. \n",
    "WORD_RE = re.compile(r\"[\\w']+\")    \n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        trueLabel = components[1]\n",
    "        text = components[2] + ' ' + components[3]\n",
    "        spamScore = log(priorSpam)\n",
    "        hamScore = log(priorHam)\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in condProbSpam:\n",
    "                spamScore += log(condProbSpam[word])\n",
    "            if word in condProbHam:\n",
    "                hamScore += log(condProbHam[word])\n",
    "        predicted = 0\n",
    "        if spamScore > hamScore:\n",
    "            predicted = 1\n",
    "        print ID + '\\t' + str(trueLabel) + '\\t' + str(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change permissions on reducer\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('enronemail_1h.txt.output', 'r') as myfile:\n",
    "    count = 0\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        true = int(components[1])\n",
    "        pred = int(components[2])\n",
    "        count += (true - pred)\n",
    "    print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        flag = components[1]\n",
    "        text = components[2] + components[3]\n",
    "        word_count = {}\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        for word, count in word_count.iteritems():\n",
    "            print ID + '\\t' + str(flag) + '\\t' + word + '\\t' + str(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
