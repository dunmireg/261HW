{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 261 Homework 1 - Big Data and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Glenn (Ted) Dunmire\n",
    "\n",
    "Email: glenn.dunmire.iv@gmail.com\n",
    "\n",
    "Class: W261-4\n",
    "\n",
    "Week: 1\n",
    "\n",
    "Date: 1/19/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__HW1.0.0.__ Define big data. Provide an example of a big data problem in your domain of expertise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, \"Big data\" is a term for a data set that is so large or so complex that traditional data processing techniques are inadequate. For example, the Twitter firehose provides an enormous volume of data but it is also being generated extremely quickly, capturing the velocity aspect of big data. \n",
    "My domain of expertise is in survey research. Although we do not currently deal with many big data tasks, it is easy to imagine scaling up several of our web surveys. Especially when we use pre-selected internet panels, it would be pretty understandable to see something like the Twitter firehose with large volumes of data responses coming in at a very fast rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__HW1.0.1.__ In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2, 3, 4, 5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In plain English, we are establishing how \"good\" our model is. \n",
    "The mean squared error of the model is usually what we are looking at and can be decomposed into three parts. The MSE is calculated as follows: (NB: predicted means using the model to predict the value, eg f_hat(x))\n",
    "\n",
    "E[(true_value(x) - predicted(x))^2] = bias^2 + variance + noise\n",
    "\n",
    "The bias is a measure of how much the model prediction would deviate from the true value. We do not know the true function f(x) but we do have the true values for the test dataset. The bias is calculated as:\n",
    "\n",
    "(E[predicted(x)] - true_function(x))^2\n",
    "\n",
    "However, we do not have the true function. Instead we will approximate it with the true value of the test set so the bias is:\n",
    "\n",
    "(E[predicted(x)] - test_value(x))^2\n",
    "\n",
    "The variance is a measure of how much the prediction of one training set differs from expected predicted values over different training sets. Essentially this is measuring how consistent predictions are from each other. It is calculated as follows:\n",
    "\n",
    "E[(predicted(x) - E[predicted(x)])^2]\n",
    "\n",
    "Finally, the noise is the irreducible error. This is error that fundamentally cannot be captured in the model, as a result of inherent uncertainty. Essentially, any system will naturally vary and cannot be captured perfectly in a model. \n",
    "\n",
    "Noise = MSE - (bias^2 + variance)\n",
    "\n",
    "In order to calculate these, I would set up functions that enable me to pass a test set and a model which will then produce the bias^2 term and variance. Then I would use a loop to check the different polynomial degrees and get the various errors. \n",
    "\n",
    "Ex:\n",
    "\n",
    "for degree in [1, 2, 3, 4, 5]:\n",
    "    model = model_function(degree)\n",
    "    bias = bias_squared(test, model)\n",
    "    variance = variance(test, model)\n",
    "    noise = MSE(model) - (bias + variance)\n",
    "\n",
    "Then for model selection, I would try to select the model that minimizes the combined error of bias squared and variance. However, there is a tradeoff. Usually, as a model becomes more complex, the bias^2 decreases but the variance increases. So I would find the lowest order polynomial with the lowest combined value of bias^2 and variance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 1__\n",
    "\n",
    "Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NB__ Certain lines appeared to be incorrect with some text based errors. The ids are listed below. I have edited the text and stored the original, unedited text as a file \"enronemail_1h copy.txt\"\n",
    "\n",
    "0011.2001-06-28.SA_and_HP -> incorrect return made two lines\n",
    "\n",
    "0001.2000-06-06.lokay -> separated text to appropriate place and made subject NA\n",
    "\n",
    "0009.2001-06-26.SA_and_HP -> moved text to content section and made subject NA\n",
    "\n",
    "This was manually checked using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        stuff = line.split('\\t')\n",
    "        if len(stuff) != 4:\n",
    "            print stuff[0]\n",
    "            print len(stuff)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__\n",
    "\n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "   To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word.\n",
    "\n",
    "CROSSCHECK: >grep assistance enronemail_1h.txt|cut -d$'\\t' -f4| grep assistance|wc -l\n",
    "      \n",
    "      8 (should really be 10 if counting occurrences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0 #keep track of count of found word\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #regex for real words\n",
    "filename = sys.argv[1] #parse file name\n",
    "findword = sys.argv[2] #parse findword, the word we're looking for\n",
    "with open (filename, \"r\") as myfile: #read file\n",
    "    for line in myfile.readlines():\n",
    "        words = re.findall(WORD_RE, line) #grab all words from the text\n",
    "        occ = [i for i,x in enumerate(words) if x.lower() == findword.lower()] #create list based on logical match\n",
    "        count += len(occ) #increment count\n",
    "print count #print count to temporary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Change permissions on mapper\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "\n",
    "files = sys.argv[1:] #read file\n",
    "sum = 0 #total sum\n",
    "for filename in files:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            sum = sum + int(line) #increment sum by occurrence of count\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change permissions on reducer\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below provided in assignment document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "#\\rm $data.chunk.*\n",
    "\\rm enronemail_1h.txt.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times word appears: \n",
      "10\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\"\n",
    "! echo \"Number of times word appears: \"\n",
    "! head enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verified that checking for assistance correctly prints out the 10 times the word assistance occurs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "__Question 3__\n",
    "\n",
    "HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "\n",
    "   that performs a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   NOTE if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeld as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimated of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #regex for matching to word\n",
    "filename = sys.argv[1] #grab filename from command line input\n",
    "findword = sys.argv[2] #get findword in question from command line input\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines(): #read all lines in file and repeat mapping\n",
    "        components = line.split('\\t') #separate line into ID + spam flag + subject + text\n",
    "        ID = components[0]\n",
    "        flag = components[1]\n",
    "        text = components[2] + components[3] #combine subject and content into one string\n",
    "        word_count = {} #dictionary to hold the number of times each word is found. The word is the key, count is value\n",
    "        for word in WORD_RE.findall(text): #check all words in the text\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1 #increment value by 1\n",
    "            else:\n",
    "                word_count[word] = 1 #set word to key in dictionary and give value of 1\n",
    "        for word, count in word_count.iteritems(): #scan through dictionary for all words and counts\n",
    "            found_word = 0 #flag for if word is the findword\n",
    "            if word.lower() == findword.lower(): #check and set flag if the word is the findword\n",
    "                found_word = 1\n",
    "            #map the following string to the output files\n",
    "            #ID + spam flag + word + count of word + logical flag if word is foundword (1 = True, 0 False)\n",
    "            print ID + '\\t' + str(flag) + '\\t' + word + '\\t' + str(count) + '\\t' + str(found_word) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change permission for mapper\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "#in this script I will be using a lot of sets. The point here is to prevent duplicates. Ex. the email \n",
    "#IDs occur multiple times because we are parsing by the individual word (see mapper above). However, we are only\n",
    "#interested in the ID once, therefore we will use a set to hold it. \n",
    "emailID = set() #set to hold unique email IDs\n",
    "spam = set() #set to hold unique email IDs if spam\n",
    "vocab = set() #set to hold all unique words in text\n",
    "spamCount = 0 #how many words are in the spam documents in total\n",
    "spamWordCount = 0 #how many times the findword occurs in the spam text\n",
    "hamCount= 0 #how many words appear in all ham documents\n",
    "hamWordCount = 0 #how many times does the findword appear in the ham text\n",
    "foundWord = set() #set of unique findwords (adapted from 1.4)\n",
    "\n",
    "files = sys.argv[1:]\n",
    "for filename in files:\n",
    "    with open(filename, 'r') as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            components = line.split('\\t') #split file into ID + spam flag + word + word count + find word logical flag\n",
    "            ID = components[0]\n",
    "            flag = int(components[1])\n",
    "            word = components[2]\n",
    "            count = int(components[3])\n",
    "            findword = int(components[4])\n",
    "            if findword == 1: #if the findword flag is present, add the word to the set of foundwords\n",
    "                foundWord.add(word)\n",
    "            emailID.add(ID) #add the email's ID to the set of IDs. \n",
    "            if flag == 1: #if the mapped word came from a spam email\n",
    "                spam.add(ID) #add ID to spam\n",
    "                spamCount += count #increase spam word count by count of this word\n",
    "                if findword == 1:\n",
    "                    spamWordCount += count #increase count of findword if necessary\n",
    "            else: #repeat above procedure for ham\n",
    "                hamCount += count\n",
    "                if findword == 1:\n",
    "                    hamWordCount += count\n",
    "\n",
    "\n",
    "#Calculate prior probability for spam and ham\n",
    "priorSpam = float(len(spam))/len(emailID) \n",
    "priorHam = float(len(emailID) - len(spam))/len(emailID)\n",
    "\n",
    "#conditional probability in this case, as given by instructions in the homework text\n",
    "#Conditional probability of a document being a class given a word is the \n",
    "#(count of the word in the class)/(total words in class)\n",
    "condProbSpam = float(spamWordCount)/spamCount\n",
    "condProbHam = float(hamWordCount)/hamCount\n",
    "\n",
    "#display priors and conditional probabilities\n",
    "print \"Prior Spam: \" + str(priorSpam)\n",
    "print \"Cond prob of spam \" + str(condProbSpam)\n",
    "print \"Prior Ham \" + str(priorHam) \n",
    "print \"Cond prob of Ham \" + str(condProbHam)\n",
    "\n",
    "#classify new emails\n",
    "#in this case I decided to read the original file, rather than reassembling the emails from the reducer step.\n",
    "#This increases modularity because in practice we would report our results on a validation or test set as opposed\n",
    "#to the training set. \n",
    "#Note also I am setting a condition that if a word does not appear in the text of a class, ex a word does not appear\n",
    "#in ham at all, then the probability does not get updated. Normally a smoother would take care of this but the \n",
    "#instructions say not use a smoother for this problem. \n",
    "WORD_RE = re.compile(r\"[\\w']+\")    \n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        trueLabel = components[1]\n",
    "        text = components[2] + ' ' + components[3] #combine email subject and content\n",
    "        spamScore = log(priorSpam) #take log\n",
    "        hamScore = log(priorHam)\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in foundWord and condProbSpam != 0: #check if findword and if word has appeared in spam\n",
    "                spamScore += log(condProbSpam) #increment by log\n",
    "            if word in foundWord and condProbHam != 0: #ditto for ham\n",
    "                hamScore += log(condProbHam)\n",
    "        predicted = 0\n",
    "        if spamScore > hamScore: #classify as spam only if spamscore is higher than hamscore\n",
    "            predicted = 1\n",
    "        print ID + '\\t' + str(trueLabel) + '\\t' + str(predicted) #print ID + true flag + predicted flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Change permissions on reducer\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Spam: 0.44\n",
      "\n",
      "Cond prob of spam 0.000428036383093\n",
      "\n",
      "Prior Ham 0.56\n",
      "\n",
      "Cond prob of Ham 0.000140637086\n",
      "\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "\n",
      "0011.2003-12-18.GP\t1\t0\n",
      "\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "\n",
      "Misclassified 40\n"
     ]
    }
   ],
   "source": [
    "with open('enronemail_1h.txt.output', 'r') as myfile:\n",
    "    count = 0 #count of how many emails are misclassified\n",
    "    for line in myfile.readlines():\n",
    "        print line\n",
    "        components = line.split('\\t') #display contents ID + spam flag + predicted flag (0 = ham, 1 = spam)\n",
    "        if len(components) > 2:\n",
    "            if int(components[1]) != int(components[2]):\n",
    "                count += 1\n",
    "    print \"Misclassified \" + str(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're getting 60% accuracy here. That is, 40 emails are misclassified while 60 are classified correctly. A lot of the emails are actually spam and are being classified as ham. This is due to the fact that the prior probability of an email being ham is quite a bit larger than the prior probability of an email being spam. So despite the conditional probability of \"assistance\" being higher for the class spam, in a lot of cases there aren't enough occurrences to classify as spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Question 4\n",
    "\n",
    "HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py \n",
    "\n",
    "   performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") #regex for word\n",
    "filename = sys.argv[1] #get file name\n",
    "findwords = re.split(\" \",sys.argv[2].lower()) #parse input string of words into list\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t') #split line into components ID + flag + text\n",
    "        ID = components[0]\n",
    "        flag = components[1]\n",
    "        text = components[2] + components[3] #combine subject and content into one text list \n",
    "        word_count = {} #keep track of all words and their counts\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        for word, count in word_count.iteritems(): \n",
    "            found_word = 0 \n",
    "            if word.lower() in findwords: #check if each word is in the findword list\n",
    "                found_word = 1\n",
    "            #print a string with ID + spam flag + word + count + if word is a findword\n",
    "            print ID + '\\t' + str(flag) + '\\t' + word + '\\t' + str(count) + '\\t' + str(found_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Overwrite mapper.py\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "emailID = set() #set to hold email IDs\n",
    "spam = set() #set to hold spam IDs\n",
    "vocab = set() #set to hold vocab of unique words in corpus of all text\n",
    "spamCount = 0 #count of how many words in spam documents\n",
    "spamWordCount = {} #dictionary for findwords in spam documents\n",
    "hamCount= 0 #count of how many words in ham documents\n",
    "hamWordCount = {} #dictionary for findwords in ham documents\n",
    "foundWord = set() #set for findwords\n",
    "\n",
    "files = sys.argv[1:] #read file name\n",
    "for filename in files:\n",
    "    with open(filename, 'r') as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            components = line.split('\\t') #split line by tab and assign components\n",
    "            ID = components[0]\n",
    "            flag = int(components[1])\n",
    "            word = components[2]\n",
    "            count = int(components[3])\n",
    "            findword = int(components[4])\n",
    "            if findword == 1:\n",
    "                foundWord.add(word) #add findword to set of existing foundwords\n",
    "            emailID.add(ID)\n",
    "            if flag == 1: #if email is spam\n",
    "                spam.add(ID) #add ID to set\n",
    "                spamCount += count #increment counter\n",
    "                if findword == 1: #if findword present add to dictionary or increment counter\n",
    "                    if word not in spamWordCount:\n",
    "                        spamWordCount[word] = count\n",
    "                    else:\n",
    "                        spamWordCount[word] += count\n",
    "            else: #repeat above steps for ham\n",
    "                hamCount += count\n",
    "                if findword == 1:\n",
    "                    if word not in hamWordCount:\n",
    "                        hamWordCount[word] = count\n",
    "                    else:\n",
    "                        hamWordCount[word] += count\n",
    "\n",
    "\n",
    "#Calculate priors for spam and ham\n",
    "priorSpam = float(len(spam))/len(emailID)\n",
    "priorHam = float(len(emailID) - len(spam))/len(emailID)\n",
    "\n",
    "#dictionary to hold conditional probabilities for spam and ham words\n",
    "condProbSpam = {}\n",
    "condProbHam = {}\n",
    "\n",
    "#assign conditional probabilities for each word in the dictionaries for spam and ham respectively\n",
    "#probability is (count of word in class)/(total words in class)\n",
    "for word in foundWord:\n",
    "    if word in spamWordCount:\n",
    "        condProbSpam[word] = float(spamWordCount[word])/spamCount\n",
    "    if word in hamWordCount:\n",
    "        condProbHam[word] = float(hamWordCount[word])/hamCount\n",
    "\n",
    "#print priors of spam and ham and conditional probabilities\n",
    "print \"Prior Spam: \" + str(priorSpam)\n",
    "print \"Cond prob of spam \" + str(condProbSpam)\n",
    "print \"Prior Ham \" + str(priorHam) \n",
    "print \"Cond prob of Ham \" + str(condProbHam)\n",
    "      \n",
    "#classify new emails\n",
    "#in this case I decided to read the original file, rather than reassembling the emails from the reducer step.\n",
    "#This increases modularity because in practice we would report our results on a validation or test set as opposed\n",
    "#to the training set. \n",
    "#Note also I am setting a condition that if a word does not appear in the text of a class, ex a word does not appear\n",
    "#in ham at all, then the probability does not get updated. Normally a smoother would take care of this but the \n",
    "#instructions say not use a smoother for this problem. \n",
    "WORD_RE = re.compile(r\"[\\w']+\")    \n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t') #split lines\n",
    "        ID = components[0] #assign components\n",
    "        trueLabel = components[1]\n",
    "        text = components[2] + ' ' + components[3] #combine subject and content into one text blob\n",
    "        spamScore = log(priorSpam)\n",
    "        hamScore = log(priorHam)\n",
    "        for word in WORD_RE.findall(text): #increase score for each word found that is present in a class\n",
    "            if word in foundWord and word in condProbSpam:\n",
    "                spamScore += log(condProbSpam[word])\n",
    "            if word in foundWord and word in condProbHam:\n",
    "                hamScore += log(condProbHam[word])\n",
    "        predicted = 0 #assign class based on score\n",
    "        if spamScore > hamScore:\n",
    "            predicted = 1\n",
    "        print ID + '\\t' + str(trueLabel) + '\\t' + str(predicted) #print word + spam flag + predicted class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#overwrite reducer.py\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4 'assistance valium enlargementWithATypo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Spam: 0.44\n",
      "\n",
      "Cond prob of spam {'assistance': 0.00042803638309256285, 'valium': 0.00016051364365971107}\n",
      "\n",
      "Prior Ham 0.56\n",
      "\n",
      "Cond prob of Ham {'assistance': 0.00014063708599957808}\n",
      "\n",
      "0001.1999-12-10.farmer\t0\t0\n",
      "\n",
      "0001.1999-12-10.kaminski\t0\t0\n",
      "\n",
      "0001.2000-01-17.beck\t0\t0\n",
      "\n",
      "0001.2000-06-06.lokay\t0\t0\n",
      "\n",
      "0001.2001-02-07.kitchen\t0\t0\n",
      "\n",
      "0001.2001-04-02.williams\t0\t0\n",
      "\n",
      "0002.1999-12-13.farmer\t0\t0\n",
      "\n",
      "0002.2001-02-07.kitchen\t0\t0\n",
      "\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\n",
      "\n",
      "0002.2003-12-18.GP\t1\t0\n",
      "\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "\n",
      "0003.1999-12-10.kaminski\t0\t0\n",
      "\n",
      "0003.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0003.2000-01-17.beck\t0\t0\n",
      "\n",
      "0003.2001-02-08.kitchen\t0\t0\n",
      "\n",
      "0003.2003-12-18.GP\t1\t0\n",
      "\n",
      "0003.2004-08-01.BG\t1\t0\n",
      "\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "\n",
      "0004.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0004.2001-04-02.williams\t0\t0\n",
      "\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\n",
      "\n",
      "0004.2004-08-01.BG\t1\t0\n",
      "\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "\n",
      "0005.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0005.2000-06-06.lokay\t0\t0\n",
      "\n",
      "0005.2001-02-08.kitchen\t0\t0\n",
      "\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\n",
      "\n",
      "0005.2003-12-18.GP\t1\t0\n",
      "\n",
      "0006.1999-12-13.kaminski\t0\t0\n",
      "\n",
      "0006.2001-02-08.kitchen\t0\t0\n",
      "\n",
      "0006.2001-04-03.williams\t0\t0\n",
      "\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\n",
      "\n",
      "0006.2003-12-18.GP\t1\t0\n",
      "\n",
      "0006.2004-08-01.BG\t1\t0\n",
      "\n",
      "0007.1999-12-13.kaminski\t0\t0\n",
      "\n",
      "0007.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0007.2000-01-17.beck\t0\t0\n",
      "\n",
      "0007.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0007.2003-12-18.GP\t1\t0\n",
      "\n",
      "0007.2004-08-01.BG\t1\t0\n",
      "\n",
      "0008.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\n",
      "\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\n",
      "\n",
      "0008.2003-12-18.GP\t1\t0\n",
      "\n",
      "0008.2004-08-01.BG\t1\t0\n",
      "\n",
      "0009.1999-12-13.kaminski\t0\t0\n",
      "\n",
      "0009.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0009.2000-06-07.lokay\t0\t0\n",
      "\n",
      "0009.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\n",
      "\n",
      "0009.2003-12-18.GP\t1\t0\n",
      "\n",
      "0010.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0010.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0010.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "\n",
      "0010.2003-12-18.GP\t1\t0\n",
      "\n",
      "0010.2004-08-01.BG\t1\t0\n",
      "\n",
      "0011.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\n",
      "\n",
      "0011.2003-12-18.GP\t1\t0\n",
      "\n",
      "0011.2004-08-01.BG\t1\t0\n",
      "\n",
      "0012.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0012.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0012.2000-01-17.beck\t0\t0\n",
      "\n",
      "0012.2000-06-08.lokay\t0\t0\n",
      "\n",
      "0012.2001-02-09.kitchen\t0\t0\n",
      "\n",
      "0012.2003-12-19.GP\t1\t0\n",
      "\n",
      "0013.1999-12-14.farmer\t0\t0\n",
      "\n",
      "0013.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0013.2001-04-03.williams\t0\t0\n",
      "\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\n",
      "\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "\n",
      "0014.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0014.1999-12-15.farmer\t0\t0\n",
      "\n",
      "0014.2001-02-12.kitchen\t0\t0\n",
      "\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\n",
      "\n",
      "0014.2003-12-19.GP\t1\t0\n",
      "\n",
      "0014.2004-08-01.BG\t1\t0\n",
      "\n",
      "0015.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0015.1999-12-15.farmer\t0\t0\n",
      "\n",
      "0015.2000-06-09.lokay\t0\t0\n",
      "\n",
      "0015.2001-02-12.kitchen\t0\t0\n",
      "\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\n",
      "\n",
      "0015.2003-12-19.GP\t1\t0\n",
      "\n",
      "0016.1999-12-15.farmer\t0\t0\n",
      "\n",
      "0016.2001-02-12.kitchen\t0\t0\n",
      "\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\n",
      "\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\n",
      "\n",
      "0016.2003-12-19.GP\t1\t0\n",
      "\n",
      "0016.2004-08-01.BG\t1\t0\n",
      "\n",
      "0017.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0017.2000-01-17.beck\t0\t0\n",
      "\n",
      "0017.2001-04-03.williams\t0\t0\n",
      "\n",
      "0017.2003-12-18.GP\t1\t0\n",
      "\n",
      "0017.2004-08-01.BG\t1\t0\n",
      "\n",
      "0017.2004-08-02.BG\t1\t0\n",
      "\n",
      "0018.1999-12-14.kaminski\t0\t0\n",
      "\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "\n",
      "Misclassified 40\n"
     ]
    }
   ],
   "source": [
    "#examine results\n",
    "with open('enronemail_1h.txt.output', 'r') as myfile:\n",
    "    count = 0 #count of how many emails are misclassified\n",
    "    for line in myfile.readlines():\n",
    "        print line\n",
    "        components = line.split('\\t') #display contents ID + spam flag + predicted flag (0 = ham, 1 = spam)\n",
    "        if len(components) > 2:\n",
    "            if int(components[1]) != int(components[2]): #increment counter if true and predicted class don't match\n",
    "                count += 1\n",
    "    print \"Misclassified \" + str(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice here we have the same thing as above. An accuracy of 60%, meaning we misclassified 40 emails. This is probably due to the same reasons above, the large prior probability of ham and the relatively small conditional probabilities. Also valium only appears in the spam case so it will only add to spam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 5 - Had previously done before revised homework\n",
    "\n",
    "HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present.\n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of all words, and\n",
    "   - reducer.py performs a word-distribution-wide Naive Bayes classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        flag = components[1]\n",
    "        text = components[2] + components[3]\n",
    "        word_count = {}\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        for word, count in word_count.iteritems():\n",
    "            print ID + '\\t' + str(flag) + '\\t' + word + '\\t' + str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change permissions on mapper\n",
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "from math import log\n",
    "\n",
    "emails = set()\n",
    "spams = set()\n",
    "words_in_corpus = set()\n",
    "spam_counts = {}\n",
    "ham_counts = {}\n",
    "\n",
    "files = sys.argv[1:]\n",
    "for filename in files:\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            components = line.split('\\t')\n",
    "            ID = components[0]\n",
    "            flag = int(components[1])\n",
    "            word = components[2]\n",
    "            count = int(components[3])\n",
    "            emails.add(ID)\n",
    "            words_in_corpus.add(word)\n",
    "            if flag == 1:\n",
    "                spams.add(ID)\n",
    "                if word not in spam_counts:\n",
    "                    spam_counts[word] = count\n",
    "                else:\n",
    "                    spam_counts[word] += count\n",
    "            else:\n",
    "                if word not in ham_counts:\n",
    "                    ham_counts[word] = count\n",
    "                else:\n",
    "                    ham_counts[word] += count\n",
    "            \n",
    "priorSpam = float(len(spams))/len(emails)\n",
    "priorHam = float(len(emails) - len(spams))/len(emails)\n",
    "\n",
    "condProbSpam = {}\n",
    "condProbHam = {}\n",
    "\n",
    "for word in words_in_corpus:\n",
    "    if word in spam_counts:\n",
    "        spam_tokens = spam_counts[word] + 1\n",
    "    else:\n",
    "        spam_tokens = 1\n",
    "    condProbSpam[word] = float(spam_tokens)/(sum(spam_counts.values()) + len(words_in_corpus))\n",
    "    if word in ham_counts:\n",
    "        ham_tokens = ham_counts[word] + 1\n",
    "    else:\n",
    "        ham_tokens = 1\n",
    "    condProbHam[word] = float(ham_tokens)/(sum(ham_counts.values()) + len(words_in_corpus))\n",
    "\n",
    "#classify new emails\n",
    "#in this case I decided to read the original file, rather than reassembling the emails from the reducer step.\n",
    "#This increases modularity because in practice we would report our results on a validation or test set as opposed\n",
    "#to the training set. \n",
    "WORD_RE = re.compile(r\"[\\w']+\")    \n",
    "with open('enronemail_1h.txt', 'r') as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        trueLabel = components[1]\n",
    "        text = components[2] + ' ' + components[3]\n",
    "        spamScore = log(priorSpam)\n",
    "        hamScore = log(priorHam)\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in condProbSpam:\n",
    "                spamScore += log(condProbSpam[word])\n",
    "            if word in condProbHam:\n",
    "                hamScore += log(condProbHam[word])\n",
    "        predicted = 0\n",
    "        if spamScore > hamScore:\n",
    "            predicted = 1\n",
    "        print ID + '\\t' + str(trueLabel) + '\\t' + str(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change permissions on reducer\n",
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "with open('enronemail_1h.txt.output', 'r') as myfile:\n",
    "    count = 0\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        true = int(components[1])\n",
    "        pred = int(components[2])\n",
    "        count += (true - pred)\n",
    "    print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        components = line.split('\\t')\n",
    "        ID = components[0]\n",
    "        flag = components[1]\n",
    "        text = components[2] + components[3]\n",
    "        word_count = {}\n",
    "        for word in WORD_RE.findall(text):\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        for word, count in word_count.iteritems():\n",
    "            print ID + '\\t' + str(flag) + '\\t' + word + '\\t' + str(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 - Had not done when received revised homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
